{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import re\n",
    "import codecs\n",
    "import shutil\n",
    "from IPython.display import Audio\n",
    "import io\n",
    "import tempfile\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sentencepiece as sp\n",
    "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
    "import fairseq\n",
    "from fairseq.data.encoders.gpt2_bpe import GPT2BPE, GPT2BPEConfig\n",
    "from fairseq.tasks import TASK_REGISTRY\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from fairseq.binarizer import VocabularyDatasetBinarizer, FileBinarizer, AlignmentDatasetBinarizer, BinarizeSummary\n",
    "from fairseq.data import data_utils\n",
    "from fairseq.data import Dictionary\n",
    "from fairseq.data import StripTokenDataset, AppendTokenDataset, TruncateDataset, RandomCropDataset, AppendTokenDataset, PrependTokenDataset, ConcatDataset, PadDataset, TokenBlockDataset, \\\n",
    "    MonolingualDataset, LanguagePairDataset, MaskTokensDataset, NumelDataset, ConcatSentencesDataset, NestedDictionaryDataset, RawLabelDataset\n",
    "\n",
    "from asr_dataset import ASRDataset\n",
    "plt.style.use(\"seaborn-v0_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_path = \"/mnt/dl/NLP/LibriSpeech/train-clean-100/LibriSpeech/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPEAKERS.TXT',\n",
       " 'train-clean-100',\n",
       " 'BOOKS.TXT',\n",
       " 'CHAPTERS.TXT',\n",
       " 'README.TXT',\n",
       " 'LICENSE.TXT']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(librispeech_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_train_path = \"/mnt/dl/NLP/LibriSpeech/train-dataset\"\n",
    "os.makedirs(librispeech_train_path, exist_ok=True)\n",
    "librispeech_train_text_path = os.path.join(librispeech_train_path, \"text\")\n",
    "librispeech_train_audio_path = os.path.join(librispeech_train_path, \"audio\")\n",
    "os.makedirs(librispeech_train_audio_path, exist_ok=True)\n",
    "os.makedirs(librispeech_train_text_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set()\n",
    "idx = 0\n",
    "with (open(os.path.join(librispeech_train_text_path, \"text.txt\"), \"w\") as train_f, \n",
    "      open(os.path.join(librispeech_train_text_path, \"metadata.txt\"), \"w\") as meta_f, \n",
    "      open(os.path.join(librispeech_train_text_path, \"audio.txt\"), \"w\") as audio_f, \n",
    "      ):\n",
    "    for reader_id in sorted(os.listdir(os.path.join(librispeech_path, \"train-clean-100\")), key=lambda x: int(x)):\n",
    "        chapters = os.listdir(os.path.join(librispeech_path, \"train-clean-100\", reader_id))\n",
    "        for chap in chapters:\n",
    "            transcript = os.path.join(librispeech_path, \"train-clean-100\", reader_id, chap, f\"{reader_id}-{chap}.trans.txt\")\n",
    "            with open(transcript, \"r\") as tf:\n",
    "                for line in tf:\n",
    "                    line = line.strip()\n",
    "                    if not line.strip():\n",
    "                        continue\n",
    "                    audio_id =  line.split(\" \")[0]\n",
    "                    shutil.copyfile(os.path.join(librispeech_path, \"train-clean-100\", reader_id, chap, audio_id + \".flac\"), \n",
    "                                    os.path.join(librispeech_train_audio_path, f\"{idx:05}.flac\")\n",
    "                                    )\n",
    "                    train_f.write(\" \".join(line.split(\" \")[1:]))\n",
    "                    train_f.write(\"\\n\")\n",
    "                    \n",
    "                    audio_f.write(os.path.join(librispeech_train_audio_path, f\"{idx:05}.flac\"))\n",
    "                    audio_f.write(\"\\n\")\n",
    "                    \n",
    "                    meta_f.write(os.path.join(librispeech_path, \"train-clean-100\", reader_id, chap, audio_id + \".flac\") + \"\\n\")\n",
    "                    \n",
    "                    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28539"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_train_pieces_path = os.path.join(librispeech_train_path, \"sentencepiece\")\n",
    "os.makedirs(librispeech_train_pieces_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mnt/dl/NLP/LibriSpeech/train-dataset/text/text.txt\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 100000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: -1\n",
      "  eos_id: 2\n",
      "  pad_id: 1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: /mnt/dl/NLP/LibriSpeech/train-dataset/text/text.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 28539 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=5298357\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=28\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 28539 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2708305\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 81579 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 28539\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 33798\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 33798 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=28479 obj=9.30792 num_tokens=58684 num_tokens/piece=2.06061\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=21986 obj=7.5261 num_tokens=59016 num_tokens/piece=2.68425\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=16486 obj=7.47486 num_tokens=63690 num_tokens/piece=3.86328\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=16476 obj=7.45698 num_tokens=63683 num_tokens/piece=3.8652\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=12357 obj=7.55644 num_tokens=71995 num_tokens/piece=5.82625\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=12357 obj=7.53121 num_tokens=71996 num_tokens/piece=5.82633\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=9267 obj=7.68798 num_tokens=81493 num_tokens/piece=8.79389\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=9267 obj=7.65394 num_tokens=81504 num_tokens/piece=8.79508\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6950 obj=7.859 num_tokens=90381 num_tokens/piece=13.0045\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6950 obj=7.81806 num_tokens=90380 num_tokens/piece=13.0043\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5500 obj=8.01826 num_tokens=97693 num_tokens/piece=17.7624\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5500 obj=7.98033 num_tokens=97700 num_tokens/piece=17.7636\n"
     ]
    }
   ],
   "source": [
    "# Train spm model\n",
    "bpemode = \"unigram\"\n",
    "nbpe = 5000\n",
    "librispeech_bpe_model_path = os.path.join(librispeech_train_pieces_path, \"bpe.m\")\n",
    "spm =  SentencePieceTrainer.train(input=os.path.join(librispeech_train_text_path, \"text.txt\"),\n",
    "                                    vocab_size=nbpe, model_writer=codecs.open(librispeech_bpe_model_path, \"wb\"), \n",
    "                                    model_type=bpemode, character_coverage=1., input_sentence_size=100000000,\n",
    "                                    unk_id=3, eos_id=2, pad_id=1, bos_id=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentencePieceProcessor(librispeech_bpe_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "labels = []\n",
    "with (open(os.path.join(librispeech_train_text_path, \"text.txt\")) as rf,\n",
    "      open(os.path.join(librispeech_train_pieces_path, \"text_pieces.txt\"), \"w\") as wf,\n",
    "      ):\n",
    "    for line in rf:\n",
    "        line = line.strip()\n",
    "        line_piece = encoder.EncodeAsPieces(line)\n",
    "        for p in line_piece:\n",
    "          d[p] = d.get(p, 0) + 1\n",
    "        wf.write(\" \".join(line_piece))\n",
    "        labels.append({\"text\": line, \"piece\": \" \".join(line_piece)})\n",
    "        wf.write(\"\\n\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'NORTHANGER ABBEY', 'piece': '▁NORTH ANG ER ▁ABBE Y'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.piece_to_id(\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.id_to_piece(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for i in range(nbpe):\n",
    "    piece = encoder.id_to_piece(i)\n",
    "    vocab.append((piece, str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁THE', '0'),\n",
       " ('<pad>', '1'),\n",
       " ('</s>', '2'),\n",
       " ('<unk>', '3'),\n",
       " ('S', '4'),\n",
       " ('▁AND', '5'),\n",
       " ('▁OF', '6'),\n",
       " ('▁TO', '7'),\n",
       " ('▁A', '8'),\n",
       " ('ED', '9')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.DecodeIds(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ⁇ '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.DecodeIds(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁THE', '<pad>', '</s>', '<unk>', 'S', '▁AND', '▁OF', '▁TO', '▁A', 'ED']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [t for t, idx in vocab]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_dict = sorted(d.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁THE', 60522),\n",
       " ('S', 37967),\n",
       " ('▁AND', 33316),\n",
       " ('▁OF', 29894),\n",
       " ('▁TO', 27992),\n",
       " ('▁A', 24032),\n",
       " ('ED', 18292),\n",
       " ('▁IN', 18013),\n",
       " ('▁I', 14580),\n",
       " ('▁HE', 13703)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_dict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(librispeech_train_pieces_path, \"dict.txt\"), \"w\") as f:\n",
    "    for token, count in bpe_dict:\n",
    "        f.write(f\"{token} {count}\")\n",
    "        f.write(\"\\n\")\n",
    "with open(os.path.join(librispeech_train_pieces_path, \"tokens.txt\"), \"w\") as f:\n",
    "    for token, idx in vocab:\n",
    "        f.write(f\"{token}\")\n",
    "        f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary.load(os.path.join(librispeech_train_pieces_path, \"dict.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.bos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training data\n",
    "audio_files = [text.strip() for text in open(os.path.join(librispeech_train_text_path, \"audio.txt\"))]\n",
    "ls_train_data = {}\n",
    "for i, audio_fname in enumerate(audio_files):\n",
    "    input = {}\n",
    "    output = {}\n",
    "    sample_info = torchaudio.info(audio_fname)\n",
    "    input[\"length_ms\"]  = int( sample_info.num_frames / sample_info.sample_rate / 0.001)\n",
    "    assert sample_info.num_channels == 1\n",
    "    input[\"path\"] = audio_fname\n",
    "    output[\"text\"] = labels[i][\"text\"]\n",
    "    output[\"token\"] = labels[i][\"piece\"]\n",
    "    output[\"tokenid\"] = \", \".join(map(str, [t.tolist() for t in  dictionary.encode_line(labels[i][\"piece\"], append_eos=False)]))\n",
    "    ls_train_data[f\"{i:05}\"] = {\"input\": input, \"output\": output}\n",
    "    \n",
    "    # print(input[\"length_ms\"], sample_info.num_channels, sample_info.num_frames, sample_info.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/dl/NLP/LibriSpeech/train-dataset'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librispeech_train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_train_data_path = os.path.join(librispeech_train_path, \"data\")\n",
    "os.makedirs(librispeech_train_data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(ls_train_data, codecs.open(os.path.join(librispeech_train_data_path, \"train.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task speech recognition \n",
    "asr_dict = Dictionary.load(os.path.join(librispeech_train_pieces_path, \"dict.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_blanc = asr_dict.add_symbol(\"<ctc_blank>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = json.load(codecs.open(os.path.join(librispeech_train_data_path, \"train.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'length_ms': 1965,\n",
       "  'path': '/mnt/dl/NLP/LibriSpeech/train-dataset/audio/00000.flac'},\n",
       " 'output': {'text': 'NORTHANGER ABBEY',\n",
       "  'token': '▁NORTH ANG ER ▁ABBE Y',\n",
       "  'tokenid': '827, 1460, 54, 3749, 39'}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_samples['00000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_samples = sorted(data_samples.items(), key=lambda sample: sample[1][\"input\"][\"length_ms\"], \n",
    "                        reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('25725',\n",
       " {'input': {'length_ms': 24524,\n",
       "   'path': '/mnt/dl/NLP/LibriSpeech/train-dataset/audio/25725.flac'},\n",
       "  'output': {'text': 'THERE WAS OF COURSE NO LEGALITY IN THE ACT AND KARL THE GREAT WAS IN NO REAL SENSE THE SUCCESSOR OF HONORIUS AND ROMULUS AUGUSTULUS BUT HE RULED A GROUP OF KINGDOMS WHICH EMBRACED THE LARGER HALF OF THE OLD WESTERN EMPIRE AND FORMED A FAIR EQUIPOISE TO THE REALM NOW RULED BY IRENE FROM EIGHT HUNDRED THEN ONWARD WE HAVE ONCE MORE A WEST ROMAN EMPIRE IN EXISTENCE AS WELL AS THE EAST ROMAN',\n",
       "   'token': '▁THERE ▁WAS ▁OF ▁COURSE ▁NO ▁LEG AL ITY ▁IN ▁THE ▁ACT ▁AND ▁K AR L ▁THE ▁GREAT ▁WAS ▁IN ▁NO ▁REAL ▁SENSE ▁THE ▁SUCCESS OR ▁OF ▁HONOR IUS ▁AND ▁RO M UL US ▁AUGUST UL US ▁BUT ▁HE ▁RULE D ▁A ▁GROUP ▁OF ▁KINGDOM S ▁WHICH ▁EMBRAC ED ▁THE ▁LARGE R ▁HALF ▁OF ▁THE ▁OLD ▁WESTERN ▁EMPIRE ▁AND ▁FORMED ▁A ▁FAIR ▁E QUI P O ISE ▁TO ▁THE ▁REAL M ▁NOW ▁RULE D ▁BY ▁I RE NE ▁FROM ▁EIGHT ▁HUNDRED ▁THEN ▁ON WARD ▁WE ▁HAVE ▁ONCE ▁MORE ▁A ▁WEST ▁ROMAN ▁EMPIRE ▁IN ▁EXISTENCE ▁AS ▁WELL ▁AS ▁THE ▁EAST ▁ROMAN',\n",
       "   'tokenid': '55, 14, 7, 351, 56, 1086, 95, 170, 11, 4, 638, 6, 460, 177, 98, 4, 129, 14, 11, 56, 752, 731, 4, 1295, 109, 7, 1336, 1770, 6, 778, 70, 287, 99, 2054, 287, 99, 29, 13, 1205, 32, 9, 1144, 7, 2024, 5, 47, 2518, 10, 4, 410, 61, 339, 7, 4, 125, 2882, 3132, 6, 1748, 9, 627, 225, 723, 78, 85, 1010, 8, 4, 752, 70, 87, 1205, 32, 40, 12, 106, 333, 49, 833, 328, 74, 36, 618, 52, 41, 244, 81, 9, 1001, 1051, 3132, 11, 1678, 20, 104, 20, 4, 1329, 1051'}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('19601',\n",
       " {'input': {'length_ms': 19985,\n",
       "   'path': '/mnt/dl/NLP/LibriSpeech/train-dataset/audio/19601.flac'},\n",
       "  'output': {'text': 'IF WE TAKE IT IN THE WIDEST MEANING THIS WOULD EVIDENTLY INCLUDE EVERY POSSIBLE MEDICAL TASK FROM FILLING A PAINFUL TOOTH TO OPERATING ON A PAINFUL APPENDIX AS IN EVERY CASE WHERE PAIN RESULTS THE MENTAL EQUILIBRIUM IS DISTURBED BY IT AND THE NORMAL MENTAL LIFE OF THE PATIENT REDUCED IN ITS EFFICIENCY',\n",
       "   'token': '▁IF ▁WE ▁TAKE ▁IT ▁IN ▁THE ▁WIDE ST ▁MEANING ▁THIS ▁WOULD ▁EVIDENTLY ▁INCLUD E ▁EVERY ▁POSSIBLE ▁ME D ICAL ▁TASK ▁FROM ▁FILL ING ▁A ▁PAINFUL ▁TOO TH ▁TO ▁O PER ATING ▁ON ▁A ▁PAINFUL ▁A PP EN DI X ▁AS ▁IN ▁EVERY ▁CASE ▁WHERE ▁PAIN ▁RESULT S ▁THE ▁MENTAL ▁E QUI L I BR IUM ▁IS ▁DISTURB ED ▁BY ▁IT ▁AND ▁THE ▁NORMAL ▁MENTAL ▁LIFE ▁OF ▁THE ▁PATIENT ▁REDUC ED ▁IN ▁ITS ▁E F FICIENCY',\n",
       "   'tokenid': '66, 52, 213, 17, 11, 4, 894, 102, 1633, 45, 62, 1422, 2199, 35, 196, 653, 48, 32, 518, 1747, 49, 1826, 16, 9, 2797, 183, 202, 8, 179, 674, 944, 36, 9, 2797, 9, 388, 101, 946, 291, 20, 11, 196, 448, 142, 1215, 997, 5, 4, 1979, 225, 723, 98, 188, 789, 1546, 30, 2139, 10, 40, 17, 6, 4, 4041, 1979, 185, 7, 4, 1672, 3294, 10, 11, 126, 225, 128, 4272'}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28539"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_dataset = ASRDataset(fname=os.path.join(librispeech_train_data_path, \"train.json\"), asr_dict=asr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'audio': tensor([[-1.0151, -1.0751, -1.1164,  ..., -0.5994, -0.6832, -0.7371],\n",
       "         [-1.0151, -1.0751, -1.1164,  ..., -0.5664, -0.6700, -0.6668],\n",
       "         [-1.0151, -1.0751, -1.1164,  ..., -0.4569, -0.6798, -0.6443],\n",
       "         ...,\n",
       "         [-1.0151, -1.0751, -1.1164,  ..., -0.6817, -0.7759, -0.6673],\n",
       "         [-1.0151, -1.0751, -1.1164,  ..., -0.7449, -0.7478, -0.7143],\n",
       "         [-1.0151, -1.0751, -1.1164,  ..., -0.8040, -0.7029, -0.6784]]),\n",
       " 'text': [55,\n",
       "  14,\n",
       "  7,\n",
       "  351,\n",
       "  56,\n",
       "  1086,\n",
       "  95,\n",
       "  170,\n",
       "  11,\n",
       "  4,\n",
       "  638,\n",
       "  6,\n",
       "  460,\n",
       "  177,\n",
       "  98,\n",
       "  4,\n",
       "  129,\n",
       "  14,\n",
       "  11,\n",
       "  56,\n",
       "  752,\n",
       "  731,\n",
       "  4,\n",
       "  1295,\n",
       "  109,\n",
       "  7,\n",
       "  1336,\n",
       "  1770,\n",
       "  6,\n",
       "  778,\n",
       "  70,\n",
       "  287,\n",
       "  99,\n",
       "  2054,\n",
       "  287,\n",
       "  99,\n",
       "  29,\n",
       "  13,\n",
       "  1205,\n",
       "  32,\n",
       "  9,\n",
       "  1144,\n",
       "  7,\n",
       "  2024,\n",
       "  5,\n",
       "  47,\n",
       "  2518,\n",
       "  10,\n",
       "  4,\n",
       "  410,\n",
       "  61,\n",
       "  339,\n",
       "  7,\n",
       "  4,\n",
       "  125,\n",
       "  2882,\n",
       "  3132,\n",
       "  6,\n",
       "  1748,\n",
       "  9,\n",
       "  627,\n",
       "  225,\n",
       "  723,\n",
       "  78,\n",
       "  85,\n",
       "  1010,\n",
       "  8,\n",
       "  4,\n",
       "  752,\n",
       "  70,\n",
       "  87,\n",
       "  1205,\n",
       "  32,\n",
       "  40,\n",
       "  12,\n",
       "  106,\n",
       "  333,\n",
       "  49,\n",
       "  833,\n",
       "  328,\n",
       "  74,\n",
       "  36,\n",
       "  618,\n",
       "  52,\n",
       "  41,\n",
       "  244,\n",
       "  81,\n",
       "  9,\n",
       "  1001,\n",
       "  1051,\n",
       "  3132,\n",
       "  11,\n",
       "  1678,\n",
       "  20,\n",
       "  104,\n",
       "  20,\n",
       "  4,\n",
       "  1329,\n",
       "  1051,\n",
       "  2]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2451, 80])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_dataset[0][\"audio\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1722, 80])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_dataset[10][\"audio\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
