{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import io\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import codecs\n",
    "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bsd_ja_en\n",
    "ja_fname = os.path.join(\"/mnt/dl/NLP/bsd_ja_en/data/train.ja\")\n",
    "en_fname = os.path.join(\"/mnt/dl/NLP/bsd_ja_en/data/train.en\")\n",
    "savepath = os.path.join(\"/mnt/dl/NLP/bsd_ja_en/data/sentencepiece\")\n",
    "os.makedirs(savepath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".en-ja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpg7624l8y.en-ja'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mnt/dl/NLP/bsd_ja_en/data/train.en\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1024\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: /mnt/dl/NLP/bsd_ja_en/data/train.en\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 20000 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=990025\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9554% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999554\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 20000 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=527484\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 25119 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 20000\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 15096\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 15096 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8734 obj=10.2656 num_tokens=29842 num_tokens/piece=3.41676\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=7252 obj=8.09481 num_tokens=29997 num_tokens/piece=4.13638\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5439 obj=8.10092 num_tokens=32583 num_tokens/piece=5.99062\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5435 obj=8.07342 num_tokens=32591 num_tokens/piece=5.9965\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4076 obj=8.24162 num_tokens=36482 num_tokens/piece=8.95044\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4076 obj=8.2052 num_tokens=36477 num_tokens/piece=8.94921\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3057 obj=8.43358 num_tokens=41332 num_tokens/piece=13.5204\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3057 obj=8.38661 num_tokens=41330 num_tokens/piece=13.5198\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2292 obj=8.6813 num_tokens=46743 num_tokens/piece=20.394\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2292 obj=8.6317 num_tokens=46744 num_tokens/piece=20.3944\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1719 obj=8.99988 num_tokens=52727 num_tokens/piece=30.6731\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1719 obj=8.94289 num_tokens=52726 num_tokens/piece=30.6725\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1289 obj=9.38271 num_tokens=58536 num_tokens/piece=45.4119\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1289 obj=9.31594 num_tokens=58541 num_tokens/piece=45.4158\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=1126 obj=9.52682 num_tokens=61395 num_tokens/piece=54.5249\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=1126 obj=9.49609 num_tokens=61395 num_tokens/piece=54.5249\n"
     ]
    }
   ],
   "source": [
    "en_model_fname = os.path.join(savepath, \"train.en.m\")\n",
    "en_writer = codecs.open(en_model_fname, \"wb\")\n",
    "en_spm = SentencePieceTrainer.train(input=en_fname, \n",
    "                                    vocab_size=1024, \n",
    "                                    model_writer=en_writer\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sp = SentencePieceProcessor(model_file=en_model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7f64a8440f00> >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.bos_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.unk_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(en_fname, \"r\") as f:\n",
    "    sent = f.readline()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So let's pretend we have to export a product to Japan today.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.GetPieceSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.PieceToId(\"unk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.PieceToId(\"bos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.PieceToId(\"<s>\"), en_sp.PieceToId(\"</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.PieceToId(\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.IdToPiece(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁Oh'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.IdToPiece(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.PieceToId(\"▁go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict()\n",
    "for i in range(en_sp.get_piece_size()): \n",
    "    vocab[en_sp.IdToPiece(i)] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '0',\n",
       " '00',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '9',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '<unk>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'W',\n",
       " 'Y',\n",
       " 'a',\n",
       " 'able',\n",
       " 'ac',\n",
       " 'ack',\n",
       " 'age',\n",
       " 'ake',\n",
       " 'al',\n",
       " 'ally',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ance',\n",
       " 'ant',\n",
       " 'ar',\n",
       " 'ashi',\n",
       " 'ate',\n",
       " 'ation',\n",
       " 'ative',\n",
       " 'b',\n",
       " 'body',\n",
       " 'c',\n",
       " 'ccording',\n",
       " 'ce',\n",
       " 'cha',\n",
       " 'chi',\n",
       " 'clock',\n",
       " 'com',\n",
       " 'ctual',\n",
       " 'd',\n",
       " 'dditional',\n",
       " 'e',\n",
       " 'ec',\n",
       " 'ed',\n",
       " 'el',\n",
       " 'en',\n",
       " 'end',\n",
       " 'ent',\n",
       " 'er',\n",
       " 'ers',\n",
       " 'es',\n",
       " 'ever',\n",
       " 'f',\n",
       " 'fully',\n",
       " 'g',\n",
       " 'ge',\n",
       " 'giving',\n",
       " 'h',\n",
       " 'ha',\n",
       " 'house',\n",
       " 'i',\n",
       " 'ic',\n",
       " 'id',\n",
       " 'ies',\n",
       " 'if',\n",
       " 'ight',\n",
       " 'il',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'in',\n",
       " 'ing',\n",
       " 'ion',\n",
       " 'ir',\n",
       " 'is',\n",
       " 'ite',\n",
       " 'ity',\n",
       " 'ive',\n",
       " 'ize',\n",
       " 'j',\n",
       " 'k',\n",
       " 'ki',\n",
       " 'l',\n",
       " 'la',\n",
       " 'le',\n",
       " 'less',\n",
       " 'li',\n",
       " 'll',\n",
       " 'lo',\n",
       " 'ly',\n",
       " 'm',\n",
       " 'mail',\n",
       " 'ment',\n",
       " 'n',\n",
       " 'nce',\n",
       " 'ne',\n",
       " 'o',\n",
       " 'ock',\n",
       " 'ok',\n",
       " 'ol',\n",
       " 'on',\n",
       " 'op',\n",
       " 'or',\n",
       " 'ose',\n",
       " 'ough',\n",
       " 'ous',\n",
       " 'out',\n",
       " 'ow',\n",
       " 'p',\n",
       " 'per',\n",
       " 'q',\n",
       " 'r',\n",
       " 'ra',\n",
       " 're',\n",
       " 'related',\n",
       " 'round',\n",
       " 'ry',\n",
       " 's',\n",
       " 'san',\n",
       " 'sh',\n",
       " 'side',\n",
       " 'sign',\n",
       " 'source',\n",
       " 'special',\n",
       " 'spect',\n",
       " 'st',\n",
       " 'struction',\n",
       " 't',\n",
       " 'ta',\n",
       " 'tern',\n",
       " 'th',\n",
       " 'ther',\n",
       " 'time',\n",
       " 'ting',\n",
       " 'u',\n",
       " 'ul',\n",
       " 'um',\n",
       " 'un',\n",
       " 'unds',\n",
       " 'up',\n",
       " 'ur',\n",
       " 'ure',\n",
       " 'v',\n",
       " 've',\n",
       " 'very',\n",
       " 'w',\n",
       " 'wa',\n",
       " 'work',\n",
       " 'x',\n",
       " 'xcuse',\n",
       " 'y',\n",
       " 'z',\n",
       " '’',\n",
       " '▁',\n",
       " '▁\"',\n",
       " '▁1',\n",
       " '▁10',\n",
       " '▁15',\n",
       " '▁2',\n",
       " '▁20',\n",
       " '▁3',\n",
       " '▁5',\n",
       " '▁A',\n",
       " '▁Actually',\n",
       " '▁After',\n",
       " '▁Alright',\n",
       " '▁Also',\n",
       " '▁America',\n",
       " '▁And',\n",
       " '▁Any',\n",
       " '▁Anyway',\n",
       " '▁Are',\n",
       " '▁B',\n",
       " '▁But',\n",
       " '▁Bye',\n",
       " '▁C',\n",
       " '▁CEO',\n",
       " '▁Can',\n",
       " '▁Certainly',\n",
       " '▁China',\n",
       " '▁Company',\n",
       " '▁Corporation',\n",
       " '▁D',\n",
       " '▁Did',\n",
       " '▁Do',\n",
       " '▁E',\n",
       " '▁Elaine',\n",
       " '▁English',\n",
       " '▁Everyone',\n",
       " '▁Exactly',\n",
       " '▁F',\n",
       " '▁First',\n",
       " '▁For',\n",
       " '▁Friday',\n",
       " '▁G',\n",
       " '▁Good',\n",
       " '▁Got',\n",
       " '▁Great',\n",
       " '▁H',\n",
       " '▁HR',\n",
       " '▁Have',\n",
       " '▁He',\n",
       " '▁Hello',\n",
       " '▁Hey',\n",
       " '▁Hi',\n",
       " '▁Hmm',\n",
       " '▁How',\n",
       " '▁However',\n",
       " '▁I',\n",
       " '▁It',\n",
       " '▁Japan',\n",
       " '▁Japanese',\n",
       " '▁John',\n",
       " '▁Ju',\n",
       " '▁K',\n",
       " '▁L',\n",
       " '▁Let',\n",
       " '▁Like',\n",
       " '▁M',\n",
       " '▁May',\n",
       " '▁Maybe',\n",
       " '▁Monday',\n",
       " '▁Mr',\n",
       " '▁Naka',\n",
       " '▁New',\n",
       " '▁Next',\n",
       " '▁No',\n",
       " '▁Not',\n",
       " '▁Now',\n",
       " '▁O',\n",
       " '▁OK',\n",
       " '▁Of',\n",
       " '▁Oh',\n",
       " '▁Ok',\n",
       " '▁Okay',\n",
       " '▁On',\n",
       " '▁Our',\n",
       " '▁P',\n",
       " '▁Please',\n",
       " '▁Product',\n",
       " '▁R',\n",
       " '▁Really',\n",
       " '▁Right',\n",
       " '▁S',\n",
       " '▁Sam',\n",
       " '▁Sato',\n",
       " '▁She',\n",
       " '▁Should',\n",
       " '▁So',\n",
       " '▁Some',\n",
       " '▁Sorry',\n",
       " '▁Sure',\n",
       " '▁T',\n",
       " '▁Taka',\n",
       " '▁Tanaka',\n",
       " '▁Thank',\n",
       " '▁That',\n",
       " '▁The',\n",
       " '▁Then',\n",
       " '▁There',\n",
       " '▁They',\n",
       " '▁Thi',\n",
       " '▁Thursday',\n",
       " '▁Today',\n",
       " '▁Tokyo',\n",
       " '▁Tom',\n",
       " '▁Tuesday',\n",
       " '▁Unfortunately',\n",
       " '▁W',\n",
       " '▁We',\n",
       " '▁Wednesday',\n",
       " '▁Well',\n",
       " '▁What',\n",
       " '▁When',\n",
       " '▁Where',\n",
       " '▁Which',\n",
       " '▁Who',\n",
       " '▁Why',\n",
       " '▁Will',\n",
       " '▁Would',\n",
       " '▁Wow',\n",
       " '▁Yamamoto',\n",
       " '▁Yeah',\n",
       " '▁Yes',\n",
       " '▁You',\n",
       " '▁a',\n",
       " '▁about',\n",
       " '▁accept',\n",
       " '▁account',\n",
       " '▁actually',\n",
       " '▁add',\n",
       " '▁address',\n",
       " '▁advance',\n",
       " '▁advice',\n",
       " '▁affect',\n",
       " '▁after',\n",
       " '▁afternoon',\n",
       " '▁again',\n",
       " '▁ago',\n",
       " '▁agree',\n",
       " '▁ahead',\n",
       " '▁all',\n",
       " '▁almost',\n",
       " '▁along',\n",
       " '▁already',\n",
       " '▁also',\n",
       " '▁always',\n",
       " '▁am',\n",
       " '▁amount',\n",
       " '▁an',\n",
       " '▁and',\n",
       " '▁another',\n",
       " '▁answer',\n",
       " '▁any',\n",
       " '▁anyone',\n",
       " '▁anything',\n",
       " '▁apologize',\n",
       " '▁application',\n",
       " '▁apply',\n",
       " '▁appointment',\n",
       " '▁appreciate',\n",
       " '▁approv',\n",
       " '▁are',\n",
       " '▁around',\n",
       " '▁as',\n",
       " '▁ask',\n",
       " '▁assist',\n",
       " '▁at',\n",
       " '▁attend',\n",
       " '▁audit',\n",
       " '▁available',\n",
       " '▁away',\n",
       " '▁b',\n",
       " '▁back',\n",
       " '▁bad',\n",
       " '▁base',\n",
       " '▁be',\n",
       " '▁because',\n",
       " '▁been',\n",
       " '▁before',\n",
       " '▁believe',\n",
       " '▁best',\n",
       " '▁better',\n",
       " '▁between',\n",
       " '▁big',\n",
       " '▁bit',\n",
       " '▁board',\n",
       " '▁book',\n",
       " '▁boss',\n",
       " '▁both',\n",
       " '▁br',\n",
       " '▁break',\n",
       " '▁bring',\n",
       " '▁budget',\n",
       " '▁building',\n",
       " '▁business',\n",
       " '▁busy',\n",
       " '▁but',\n",
       " '▁buy',\n",
       " '▁by',\n",
       " '▁c',\n",
       " '▁call',\n",
       " '▁came',\n",
       " '▁can',\n",
       " '▁candidate',\n",
       " '▁car',\n",
       " '▁card',\n",
       " '▁careful',\n",
       " '▁case',\n",
       " '▁catch',\n",
       " '▁certain',\n",
       " '▁chance',\n",
       " '▁change',\n",
       " '▁charge',\n",
       " '▁cheap',\n",
       " '▁check',\n",
       " '▁child',\n",
       " '▁choose',\n",
       " '▁class',\n",
       " '▁clean',\n",
       " '▁click',\n",
       " '▁client',\n",
       " '▁close',\n",
       " '▁co',\n",
       " '▁coffee',\n",
       " '▁collect',\n",
       " '▁come',\n",
       " '▁coming',\n",
       " '▁commuting',\n",
       " '▁companies',\n",
       " '▁company',\n",
       " '▁compare',\n",
       " '▁complain',\n",
       " '▁complete',\n",
       " '▁computer',\n",
       " '▁con',\n",
       " '▁concern',\n",
       " '▁conduct',\n",
       " '▁conference',\n",
       " '▁confident',\n",
       " '▁confirm',\n",
       " '▁connect',\n",
       " '▁consider',\n",
       " '▁consult',\n",
       " '▁contact',\n",
       " '▁contain',\n",
       " '▁content',\n",
       " '▁continue',\n",
       " '▁contract',\n",
       " '▁conversation',\n",
       " '▁corporate',\n",
       " '▁correct',\n",
       " '▁cost',\n",
       " '▁cou',\n",
       " '▁could',\n",
       " '▁course',\n",
       " '▁cover',\n",
       " '▁credit',\n",
       " '▁current',\n",
       " '▁customer',\n",
       " '▁customers',\n",
       " '▁data',\n",
       " '▁database',\n",
       " '▁date',\n",
       " '▁day',\n",
       " '▁days',\n",
       " '▁de',\n",
       " '▁deadline',\n",
       " '▁deal',\n",
       " '▁decide',\n",
       " '▁decision',\n",
       " '▁definite',\n",
       " '▁delay',\n",
       " '▁deliver',\n",
       " '▁department',\n",
       " '▁depend',\n",
       " '▁design',\n",
       " '▁desk',\n",
       " '▁detail',\n",
       " '▁develop',\n",
       " '▁did',\n",
       " '▁didn',\n",
       " '▁difference',\n",
       " '▁different',\n",
       " '▁difficult',\n",
       " '▁dinner',\n",
       " '▁direct',\n",
       " '▁dis',\n",
       " '▁discount',\n",
       " '▁discuss',\n",
       " '▁distribut',\n",
       " '▁division',\n",
       " '▁do',\n",
       " '▁document',\n",
       " '▁does',\n",
       " '▁doesn',\n",
       " '▁doing',\n",
       " '▁dollars',\n",
       " '▁don',\n",
       " '▁double',\n",
       " '▁down',\n",
       " '▁draft',\n",
       " '▁drink',\n",
       " '▁due',\n",
       " '▁dur',\n",
       " '▁each',\n",
       " '▁earlier',\n",
       " '▁early',\n",
       " '▁easier',\n",
       " '▁easy',\n",
       " '▁eat',\n",
       " '▁either',\n",
       " '▁else',\n",
       " '▁email',\n",
       " '▁emergency',\n",
       " '▁employee',\n",
       " '▁employees',\n",
       " '▁end',\n",
       " '▁enjoy',\n",
       " '▁enough',\n",
       " '▁enter',\n",
       " '▁entire',\n",
       " '▁environment',\n",
       " '▁equipment',\n",
       " '▁estimate',\n",
       " '▁even',\n",
       " '▁every',\n",
       " '▁everyone',\n",
       " '▁everything',\n",
       " '▁ex',\n",
       " '▁exactly',\n",
       " '▁example',\n",
       " '▁expect',\n",
       " '▁expense',\n",
       " '▁expensive',\n",
       " '▁experience',\n",
       " '▁explain',\n",
       " '▁export',\n",
       " '▁f',\n",
       " '▁facility',\n",
       " '▁factory',\n",
       " '▁familiar',\n",
       " '▁family',\n",
       " '▁far',\n",
       " '▁favor',\n",
       " '▁feel',\n",
       " '▁few',\n",
       " '▁field',\n",
       " '▁figure',\n",
       " '▁file',\n",
       " '▁final',\n",
       " '▁find',\n",
       " '▁fine',\n",
       " '▁finish',\n",
       " '▁first',\n",
       " '▁five',\n",
       " '▁flight',\n",
       " '▁floor',\n",
       " '▁focus',\n",
       " '▁follow',\n",
       " '▁food',\n",
       " '▁for',\n",
       " '▁forward',\n",
       " '▁found',\n",
       " '▁four',\n",
       " '▁free',\n",
       " '▁friend',\n",
       " '▁from',\n",
       " '▁front',\n",
       " '▁fun',\n",
       " '▁future',\n",
       " '▁ga',\n",
       " '▁general',\n",
       " '▁get',\n",
       " '▁gift',\n",
       " '▁give',\n",
       " '▁glad',\n",
       " '▁go',\n",
       " '▁going',\n",
       " '▁good',\n",
       " '▁got',\n",
       " '▁gra',\n",
       " '▁great',\n",
       " '▁group',\n",
       " '▁grow',\n",
       " '▁guess',\n",
       " '▁guest',\n",
       " '▁guys',\n",
       " '▁had',\n",
       " '▁half',\n",
       " '▁hand',\n",
       " '▁happen',\n",
       " '▁happy',\n",
       " '▁hard',\n",
       " '▁has',\n",
       " '▁have',\n",
       " '▁having',\n",
       " '▁he',\n",
       " '▁head',\n",
       " '▁hear',\n",
       " '▁heard',\n",
       " '▁help',\n",
       " '▁helpful',\n",
       " '▁her',\n",
       " '▁here',\n",
       " '▁hi',\n",
       " '▁high',\n",
       " '▁him',\n",
       " '▁hire',\n",
       " '▁his',\n",
       " '▁hold',\n",
       " '▁holiday',\n",
       " '▁home',\n",
       " '▁honest',\n",
       " '▁hope',\n",
       " '▁hotel',\n",
       " '▁hour',\n",
       " '▁how',\n",
       " '▁hundred',\n",
       " '▁idea',\n",
       " '▁imagin',\n",
       " '▁import',\n",
       " '▁important',\n",
       " '▁improve',\n",
       " '▁in',\n",
       " '▁include',\n",
       " '▁increase',\n",
       " '▁industry',\n",
       " '▁information',\n",
       " '▁inspection',\n",
       " '▁install',\n",
       " '▁instead',\n",
       " '▁insurance',\n",
       " '▁inter',\n",
       " '▁interested',\n",
       " '▁internal',\n",
       " '▁interview',\n",
       " '▁into',\n",
       " '▁invest',\n",
       " '▁invoice',\n",
       " '▁involved',\n",
       " '▁is',\n",
       " '▁issue',\n",
       " '▁it',\n",
       " '▁job',\n",
       " '▁join',\n",
       " '▁just',\n",
       " '▁keep',\n",
       " '▁kind',\n",
       " '▁know',\n",
       " '▁knowledge',\n",
       " '▁lab',\n",
       " '▁language',\n",
       " '▁laptop',\n",
       " '▁large',\n",
       " '▁last',\n",
       " '▁late',\n",
       " '▁later',\n",
       " '▁lead',\n",
       " '▁learn',\n",
       " '▁least',\n",
       " '▁leave',\n",
       " '▁leaving',\n",
       " '▁left',\n",
       " '▁let',\n",
       " '▁level',\n",
       " '▁life',\n",
       " '▁like',\n",
       " '▁limit',\n",
       " '▁list',\n",
       " '▁little',\n",
       " '▁live',\n",
       " '▁local',\n",
       " '▁log',\n",
       " '▁logistic',\n",
       " '▁long',\n",
       " '▁look',\n",
       " '▁looking',\n",
       " '▁lot',\n",
       " '▁love',\n",
       " '▁low',\n",
       " '▁luck',\n",
       " '▁lunch',\n",
       " '▁ma',\n",
       " '▁made',\n",
       " '▁major',\n",
       " '▁mak',\n",
       " '▁make',\n",
       " '▁manage',\n",
       " '▁management',\n",
       " '▁manager',\n",
       " '▁manufacturing',\n",
       " '▁many',\n",
       " '▁market',\n",
       " '▁material',\n",
       " '▁matter',\n",
       " '▁may',\n",
       " '▁maybe',\n",
       " '▁me',\n",
       " '▁mean',\n",
       " '▁meet',\n",
       " '▁meeting',\n",
       " '▁middle',\n",
       " '▁might',\n",
       " '▁million',\n",
       " '▁mind',\n",
       " '▁minutes',\n",
       " '▁mistake',\n",
       " '▁mo',\n",
       " '▁moment',\n",
       " '▁money',\n",
       " '▁month',\n",
       " '▁more',\n",
       " '▁morning',\n",
       " '▁most',\n",
       " '▁move',\n",
       " '▁much',\n",
       " '▁must',\n",
       " '▁my',\n",
       " '▁myself',\n",
       " '▁name',\n",
       " '▁near',\n",
       " '▁need',\n",
       " '▁negotia',\n",
       " '▁never',\n",
       " '▁new',\n",
       " '▁next',\n",
       " '▁nice',\n",
       " '▁night',\n",
       " '▁no',\n",
       " '▁not',\n",
       " '▁notice',\n",
       " '▁now',\n",
       " '▁number',\n",
       " '▁of',\n",
       " '▁off',\n",
       " '▁offer',\n",
       " '▁office',\n",
       " '▁okay',\n",
       " '▁old',\n",
       " '▁on',\n",
       " '▁once',\n",
       " '▁one',\n",
       " '▁online',\n",
       " '▁only',\n",
       " '▁open',\n",
       " '▁operation',\n",
       " '▁opinion',\n",
       " '▁opportunity',\n",
       " '▁option',\n",
       " '▁or',\n",
       " '▁order',\n",
       " '▁organize',\n",
       " '▁original',\n",
       " '▁other',\n",
       " '▁our',\n",
       " '▁out',\n",
       " '▁over',\n",
       " '▁own',\n",
       " '▁p',\n",
       " '▁package',\n",
       " '▁paid',\n",
       " '▁paper',\n",
       " '▁park',\n",
       " '▁part',\n",
       " '▁particular',\n",
       " '▁pass',\n",
       " '▁past',\n",
       " '▁pay',\n",
       " '▁people',\n",
       " '▁per',\n",
       " '▁percent',\n",
       " '▁perfect',\n",
       " '▁performance',\n",
       " '▁period',\n",
       " '▁person',\n",
       " '▁phone',\n",
       " '▁pick',\n",
       " '▁piece',\n",
       " '▁place',\n",
       " '▁plan',\n",
       " '▁planning',\n",
       " '▁play',\n",
       " '▁please',\n",
       " '▁pleasure',\n",
       " '▁point',\n",
       " '▁position',\n",
       " '▁possible',\n",
       " '▁potential',\n",
       " '▁prefer',\n",
       " '▁prepar',\n",
       " '▁presentation',\n",
       " '▁president',\n",
       " '▁pretty',\n",
       " '▁prevent',\n",
       " '▁previous',\n",
       " '▁price',\n",
       " '▁print',\n",
       " '▁pro',\n",
       " '▁probably',\n",
       " '▁problem',\n",
       " '▁procedure',\n",
       " '▁proceed',\n",
       " '▁process',\n",
       " '▁product',\n",
       " '▁products',\n",
       " '▁program',\n",
       " '▁project',\n",
       " '▁promot',\n",
       " '▁proposal',\n",
       " '▁provide',\n",
       " '▁public',\n",
       " '▁purchase',\n",
       " '▁purchasing',\n",
       " '▁put',\n",
       " '▁quarter',\n",
       " '▁question',\n",
       " '▁questions',\n",
       " '▁quick',\n",
       " '▁quite',\n",
       " '▁quote',\n",
       " '▁re',\n",
       " '▁read',\n",
       " '▁really',\n",
       " '▁reason',\n",
       " '▁receive',\n",
       " '▁recently',\n",
       " '▁recommend',\n",
       " '▁record',\n",
       " '▁refund',\n",
       " '▁regard',\n",
       " '▁regular',\n",
       " '▁relationship',\n",
       " '▁release',\n",
       " '▁remember',\n",
       " '▁report',\n",
       " '▁request',\n",
       " '▁require',\n",
       " '▁research',\n",
       " '▁restaurant',\n",
       " '▁result',\n",
       " '▁return',\n",
       " '▁review',\n",
       " '▁right',\n",
       " '▁room',\n",
       " '▁run',\n",
       " '▁safe',\n",
       " '▁said',\n",
       " '▁sales',\n",
       " '▁same',\n",
       " '▁save',\n",
       " '▁saw',\n",
       " '▁say',\n",
       " '▁sc',\n",
       " '▁schedule',\n",
       " '▁school',\n",
       " '▁season',\n",
       " '▁seat',\n",
       " '▁second',\n",
       " '▁section',\n",
       " '▁security',\n",
       " '▁see',\n",
       " '▁seem',\n",
       " '▁select',\n",
       " '▁sell',\n",
       " '▁seminar',\n",
       " '▁send',\n",
       " '▁sense',\n",
       " '▁sent',\n",
       " '▁serious',\n",
       " '▁serve',\n",
       " '▁service',\n",
       " '▁set',\n",
       " '▁share',\n",
       " '▁she',\n",
       " '▁shipment',\n",
       " '▁shipping',\n",
       " '▁short',\n",
       " '▁should',\n",
       " '▁show',\n",
       " '▁similar',\n",
       " '▁since',\n",
       " '▁sit',\n",
       " '▁situation',\n",
       " '▁skills',\n",
       " '▁small',\n",
       " '▁so',\n",
       " '▁software',\n",
       " '▁some',\n",
       " '▁someone',\n",
       " '▁something',\n",
       " '▁somewhere',\n",
       " '▁soon',\n",
       " '▁sorry',\n",
       " '▁sound',\n",
       " '▁space',\n",
       " '▁speak',\n",
       " '▁specific',\n",
       " '▁spend',\n",
       " '▁staff',\n",
       " '▁stand',\n",
       " '▁start',\n",
       " '▁stay',\n",
       " '▁still',\n",
       " '▁stop',\n",
       " '▁store',\n",
       " '▁strong',\n",
       " '▁student',\n",
       " '▁study',\n",
       " '▁sub',\n",
       " '▁submit',\n",
       " '▁success',\n",
       " '▁such',\n",
       " '▁suggest',\n",
       " '▁suit',\n",
       " '▁summer',\n",
       " '▁support',\n",
       " '▁sure',\n",
       " '▁switch',\n",
       " '▁system',\n",
       " '▁take',\n",
       " '▁taking',\n",
       " '▁talk',\n",
       " '▁task',\n",
       " '▁teach',\n",
       " '▁team',\n",
       " '▁tech',\n",
       " '▁technology',\n",
       " '▁tell',\n",
       " '▁ten',\n",
       " '▁term',\n",
       " '▁than',\n",
       " '▁thank',\n",
       " '▁that',\n",
       " '▁the',\n",
       " '▁their',\n",
       " '▁them',\n",
       " '▁then',\n",
       " '▁there',\n",
       " '▁these',\n",
       " '▁they',\n",
       " '▁thing',\n",
       " '▁things',\n",
       " '▁think',\n",
       " '▁thirty',\n",
       " '▁this',\n",
       " '▁those',\n",
       " '▁though',\n",
       " '▁thought',\n",
       " '▁thousand',\n",
       " '▁three',\n",
       " '▁through',\n",
       " '▁time',\n",
       " '▁to',\n",
       " '▁today',\n",
       " '▁together',\n",
       " '▁told',\n",
       " '▁tomorrow',\n",
       " '▁too',\n",
       " '▁track',\n",
       " '▁trad',\n",
       " '▁train',\n",
       " '▁training',\n",
       " '▁trans',\n",
       " '▁transfer',\n",
       " '▁travel',\n",
       " '▁trip',\n",
       " '▁trouble',\n",
       " '▁truck',\n",
       " '▁true',\n",
       " '▁try',\n",
       " '▁turn',\n",
       " '▁twenty',\n",
       " '▁two',\n",
       " '▁type',\n",
       " '▁under',\n",
       " '▁understand',\n",
       " '▁university',\n",
       " '▁unt',\n",
       " '▁up',\n",
       " '▁update',\n",
       " '▁us',\n",
       " '▁use',\n",
       " '▁used',\n",
       " '▁usual',\n",
       " '▁vacation',\n",
       " '▁value',\n",
       " '▁vari',\n",
       " '▁version',\n",
       " '▁vi',\n",
       " '▁visit',\n",
       " '▁volume',\n",
       " '▁wait',\n",
       " '▁walk',\n",
       " '▁want',\n",
       " '▁wanted',\n",
       " '▁was',\n",
       " '▁watch',\n",
       " '▁water',\n",
       " '▁way',\n",
       " '▁we',\n",
       " '▁website',\n",
       " '▁week',\n",
       " '▁well',\n",
       " '▁went',\n",
       " '▁were',\n",
       " '▁what',\n",
       " '▁when',\n",
       " '▁where',\n",
       " '▁which',\n",
       " '▁while',\n",
       " '▁who',\n",
       " '▁why',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So let's pretend we have to export a product to Japan today.\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98, 162, 11, 4, 101, 50, 9, 400, 35, 36, 10, 785, 13, 189, 10, 434, 173, 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.encode(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁So',\n",
       " '▁let',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁p',\n",
       " 're',\n",
       " 't',\n",
       " 'end',\n",
       " '▁we',\n",
       " '▁have',\n",
       " '▁to',\n",
       " '▁export',\n",
       " '▁a',\n",
       " '▁product',\n",
       " '▁to',\n",
       " '▁Japan',\n",
       " '▁today',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[en_sp.IdToPiece(i) for i in en_sp.encode(sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98, 162, 11, 4, 101, 50, 9, 400, 35, 36, 10, 785, 13, 189, 10, 434, 173, 3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.EncodeAsIds(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁So',\n",
       " '▁let',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁p',\n",
       " 're',\n",
       " 't',\n",
       " 'end',\n",
       " '▁we',\n",
       " '▁have',\n",
       " '▁to',\n",
       " '▁export',\n",
       " '▁a',\n",
       " '▁product',\n",
       " '▁to',\n",
       " '▁Japan',\n",
       " '▁today',\n",
       " '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.EncodeAsPieces(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So let's pretend we have to export a product to Japan today.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.DecodePieces(en_sp.EncodeAsPieces(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So let's pretend we have to export a product to Japan today.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.DecodeIds(en_sp.encode(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98, 162, 11, 4, 101, 50, 9, 400, 35, 36, 10, 785, 13, 189, 10, 434, 173, 3]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.SampleEncodeAsIds(sent, nbest_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁So',\n",
       " '▁let',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁',\n",
       " 'p',\n",
       " 're',\n",
       " 't',\n",
       " 'end',\n",
       " '▁we',\n",
       " '▁have',\n",
       " '▁to',\n",
       " '▁export',\n",
       " '▁a',\n",
       " '▁product',\n",
       " '▁to',\n",
       " '▁Japan',\n",
       " '▁today',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.SampleEncodeAsPieces(sent, nbest_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98,\n",
       " 162,\n",
       " 11,\n",
       " 4,\n",
       " 101,\n",
       " 30,\n",
       " 14,\n",
       " 9,\n",
       " 90,\n",
       " 18,\n",
       " 35,\n",
       " 36,\n",
       " 10,\n",
       " 785,\n",
       " 13,\n",
       " 189,\n",
       " 10,\n",
       " 434,\n",
       " 173,\n",
       " 3]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.SampleEncodeAsIds(sent, nbest_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁So',\n",
       " '▁let',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁',\n",
       " 'p',\n",
       " 're',\n",
       " 't',\n",
       " 'en',\n",
       " 'd',\n",
       " '▁we',\n",
       " '▁have',\n",
       " '▁to',\n",
       " '▁export',\n",
       " '▁',\n",
       " 'a',\n",
       " '▁product',\n",
       " '▁to',\n",
       " '▁Japan',\n",
       " '▁today',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sp.SampleEncodeAsPieces(sent, nbest_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /mnt/dl/NLP/bsd_ja_en/data/train.ja\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2048\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: /mnt/dl/NLP/bsd_ja_en/data/train.ja\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 20000 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=443656\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=1713\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 20000 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=158424\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 88910 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 20000\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 18142\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 18142 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=38006 obj=67.0044 num_tokens=139729 num_tokens/piece=3.6765\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=34113 obj=59.1155 num_tokens=140133 num_tokens/piece=4.10791\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=25463 obj=61.1356 num_tokens=147873 num_tokens/piece=5.80737\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=25318 obj=60.4057 num_tokens=147931 num_tokens/piece=5.84292\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=18948 obj=63.656 num_tokens=157790 num_tokens/piece=8.32753\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=18932 obj=62.9387 num_tokens=157920 num_tokens/piece=8.34143\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14193 obj=66.4044 num_tokens=169056 num_tokens/piece=11.9112\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=14189 obj=65.7148 num_tokens=169075 num_tokens/piece=11.9159\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=10640 obj=69.6261 num_tokens=180435 num_tokens/piece=16.9582\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=10639 obj=68.8478 num_tokens=180450 num_tokens/piece=16.9612\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=7979 obj=72.6928 num_tokens=192275 num_tokens/piece=24.0976\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=7979 obj=72.1167 num_tokens=192286 num_tokens/piece=24.099\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=5984 obj=76.6391 num_tokens=204854 num_tokens/piece=34.2336\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5983 obj=76.2847 num_tokens=204862 num_tokens/piece=34.2407\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4487 obj=80.6698 num_tokens=218603 num_tokens/piece=48.7192\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4487 obj=80.0506 num_tokens=218615 num_tokens/piece=48.7219\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3365 obj=85.2678 num_tokens=234617 num_tokens/piece=69.7227\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3365 obj=84.4932 num_tokens=234620 num_tokens/piece=69.7236\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2523 obj=90.7613 num_tokens=253883 num_tokens/piece=100.627\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2523 obj=89.7294 num_tokens=253885 num_tokens/piece=100.628\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2252 obj=92.9134 num_tokens=263403 num_tokens/piece=116.964\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2252 obj=92.4713 num_tokens=263402 num_tokens/piece=116.964\n"
     ]
    }
   ],
   "source": [
    "ja_model_fname = os.path.join(savepath, \"train.ja.m\")\n",
    "ja_writer = codecs.open(ja_model_fname, \"wb\")\n",
    "ja_spm = SentencePieceTrainer.train(input=ja_fname, model_writer=ja_writer,\n",
    "                                    vocab_size=1024 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_sp = SentencePieceProcessor()\n",
    "ja_sp.Load(ja_model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7f64a8440a80> >"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ja_fname) as f:\n",
    "    ja_sent = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'では、今日日本へ商品を輸出すると仮定しましょう。\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98, 5, 194, 205, 236, 145, 9, 1988, 53, 32, 14, 983, 203, 507, 4]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_sp.encode(ja_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[98, 5, 194, 205, 236, 145, 9, 1988, 53, 32, 14, 983, 203, 507, 4]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_sp.EncodeAsIds(ja_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁では',\n",
       " '、',\n",
       " '今日',\n",
       " '日本',\n",
       " 'へ',\n",
       " '商品',\n",
       " 'を',\n",
       " '輸',\n",
       " '出',\n",
       " 'する',\n",
       " 'と',\n",
       " '仮',\n",
       " '定',\n",
       " 'しましょう',\n",
       " '。']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_sp.EncodeAsPieces(ja_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'では、今日日本へ商品を輸出すると仮定しましょう。'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_sp.DecodeIds(ja_sp.encode(ja_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'では、今日日本へ商品を輸出すると仮定しましょう。'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_sp.DecodePieces(ja_sp.EncodeAsPieces(ja_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastBPE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
