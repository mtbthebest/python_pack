{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import zstandard as zstd\n",
    "import io\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fairseq\n",
    "from fairseq.data.encoders.gpt2_bpe import GPT2BPE, GPT2BPEConfig\n",
    "from fairseq import options\n",
    "from fairseq_cli.preprocess import main as preprocess\n",
    "from fairseq.data import Dictionary, TokenBlockDataset, MonolingualDataset, PrependTokenDataset, MaskTokensDataset, RightPadDataset,  IdDataset, NestedDictionaryDataset, NumSamplesDataset, \\\n",
    "    NestedDictionaryDataset, NumelDataset, SortDataset\n",
    "from fairseq.data.encoders import BPE_REGISTRY, register_bpe, build_bpe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dataset OpenWebText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/mnt/dl/fairseq/Masked_Language_Model/openwebtext/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(os.path.join(dataset_path, 'owt0.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23055709,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr['arr_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13749, 12409,   716, ...,  5239,    91,    29])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13749, 12409,   716, ...,  5239,    91,    29], dtype=uint16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr['arr_0'].astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/dl/fairseq/Masked_Language_Model/openwebtext/encoder.json'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(dataset_path, 'encoder.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_bpe_cfg = GPT2BPEConfig(gpt2_encoder_json=os.path.join(dataset_path, 'encoder.json'),\n",
    "                            gpt2_vocab_bpe=os.path.join(dataset_path, 'vocab.bpe')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_bpe = GPT2BPE(gpt2_bpe_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Historical amnesia is at once the most endearing and the most frustrating of American qualities. On the one hand, it means that -- F. Scott Fitzgerald notwithstanding -- there really are second acts in American lives. People can move somewhere else, reinvent themselves, start again.',\n",
       " '',\n",
       " \"On the other hand, our inability to remember what our policy was last week, never mind last decade, drives outsiders crazy. We forget that we supported the dictator before we decided to destroy him. Then we can't\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_bpe.bpe.decode(arr['arr_0'][:100]).strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_train_file(dataset_size = 1000):\n",
    "    rng = np.random.RandomState(seed=0)\n",
    "    filename = os.path.join(dataset_path, '2020-01.jsonl.zst')\n",
    "    data = []\n",
    "    with open(filename, 'rb') as fh, open(os.path.join(dataset_path, 'train.raw.txt'), 'w', encoding='utf-8') as oh:\n",
    "        dctx = zstd.ZstdDecompressor(max_window_size=2147483648)\n",
    "        stream_reader = dctx.stream_reader(fh)\n",
    "        text_stream = io.TextIOWrapper(stream_reader, encoding='utf-8')\n",
    "        for i, line in enumerate(text_stream):\n",
    "            text = json.loads(line)['text'].strip()\n",
    "            if not text.strip():\n",
    "                continue\n",
    "            if len(data) == dataset_size:\n",
    "                break\n",
    "            # if rng.uniform(0, 1, (1, ))[0] < 100000:\n",
    "            data.append(text + '\\n')\n",
    "            oh.write(data[-1])\n",
    "    \n",
    "    return data\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = write_train_file(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Advertising Read more\\n\\nParis (AFP)\\n\\nMore than 16,000 desalination plants scattered across the globe produce far more toxic sludge than fresh water, according to a first global assessment of the sector\\'s industrial waste, published Monday.\\n\\nFor every litre of fresh water extracted from the sea or brackish waterways, a litre-and-a-half of salty slurry, called brine, is dumped directly back into the ocean or the ground.\\n\\nThe super-salty substance is made even more toxic by the chemicals used in the desalination process, researchers reported in the journal Science of the Total Environment.\\n\\nCopper and chlorine, for example, are both commonly used.\\n\\nThe amount of brine produced worldwide every year -- more than 50 billion cubic metres -- is enough to cover the state of Florida, or England and Wales combined, in a 30-centimetre (one-foot) layer of salty slime, they calculated.\\n\\n\"The world produces less desalinated water than brine,\" co-author Manzoor Qadir, a scientist at the Institute for Water, Environment and Health at United Nations University in Ontario, Canada, told AFP.\\n\\n\"Almost all the brine goes back into the environment, mostly in the ocean.\"\\n\\nAll that extra salt raises the temperature of coastal waters, and decreases the level of oxygen, which can create \"dead zones\".\\n\\n\"It is difficult for aquatic organisms to breathe in these conditions -- they need O2 to survive,\" said Qadir.\\n\\nMore than half of the brine comes from only four countries: Saudi Arabia (22 percent), United Arab Emirates (20.2 percent), Kuwait (6 percent) and Qatar (5.8 percent).\\n\\nNorth Africa, the Middle East, and water-starved small island states in the Pacific and elsewhere also rely heavily on desalination to provide safe drinking water, which accounts for nearly two-thirds of consumption.\\n\\nThe rest is used in industry, as a coolant in energy production, and in agriculture.\\n\\nAround one in four people live in regions where water resources are insufficient during part of the year, and half-a-billion experience water scarcity year round, according to the United Nations.\\n\\n- Water scarcity -\\n\\nSince 2015, the World Economic Forum\\'s annual Global Risk Report has consistently ranked \"water crises\" as among the global threats -- above natural disasters, mass migration and cyber-attacks.\\n\\nWater scarcity is caused by many things, starting with a global population closing in on eight billion.\\n\\nMajor rivers no longer reach the sea, aquifers are being sucked dry, and pollution is tainting water above ground and below.\\n\\nWith climate change, the situation will get worse.\\n\\nFor each degree of global warming, about seven percent of the world\\'s population -- half-a-billion people -- will have 20 percent less freshwater, according to the Intergovernmental Panel on Climate Change (IPCC).\\n\\n\"Desalination technology has benefited a large number of people,\" said Qadir. \"But we cannot ignore the production of brine, which is going to become an even greater problem in the future.\"\\n\\nIndustrial-scale technology for removing salt from water has been around since the 1960s. By 1990, there were already 3,000 plants in operation around the globe.\\n\\nOn current trends, the sector will see a total of at least 17,500 plants by 2025, Qadir said, noting that one large plant can produce as much fresh water -- and brine -- as 200 or 300 small ones.\\n\\nMore than 90 percent of desalination plants are in wealthy economies. This reflects the fact that the technology remains expensive, especially in energy costs.\\n\\nBut it also means that rich nations have the capacity to develop ways to dispose of toxic brine that are less harmful to ocean and land environments, he added.\\n\\nSome pilot projects have even shown that modified brine can boost yields of certain fish species in aquaculture.\\n\\n? 2019 AFP\\n',\n",
       " 'Stainless Steel Tubing Materials\\n\\nStainless Steel Tubing Sizes\\n\\n3 inch stainless steel pipe 50mm stainless steel tube/pipe 1 stainless steel tubing 22mm stainless steel tube/pipe 1 inch stainless steel tubing 25mm stainless steel tube/pipe 2 inch stainless steel pipe 10mm stainless steel tube/pipe 4 inch stainless steel pipe 50mm stainless steel tube/pipe 1 inch stainless steel pipe 28mm stainless steel tube/pipe 4 stainless steel tubing 32mm stainless steel tube/pipe 2 inch stainless steel tubing 100mm stainless steel tube/pipe 1.5 inch stainless steel pipe 10mm stainless steel tube/pipe 1.5 stainless steel tubing 28mm od stainless steel tube/pipe 4 inch stainless steel tubing 100mm stainless steel tube/pipe 1.5 inch stainless steel tube 22mm stainless steel tube/pipe 6 inch stainless steel tubing 42mm stainless steel tube/pipe 8 inch stainless steel tubing 25mm stainless steel tube/pipe 5 inch stainless steel tubing 75mm stainless steel tube/pipe 2.5 inch stainless steel tubing 28mm stainless steel tube/pipe 2.5 304 stainless steel tubing 40mm stainless steel tube/pipe 38mm stainless steel tube/pipe 32mm stainless steel tube/pipe 50mm stainless steel tube/pipe 80mm stainless steel tube/pipe 12mm stainless steel tube/pipe\\n\\nFor different applications, we offer sevsral materials such as austenitic stainless steel, duplex & super duplex steel, nickel alloly steel etc. And these tubes consist of seamless tubing & welded tubing. Welded stainless steel tubing can be bent and cut to required shape and length. It is telescopic with adjacent sizes, and ideal for use in day-to-day pipe work, as well as for bushings, spacers, guide tubes, standoffs, and bearings.304 Stainless Steel Is the most versatile and extensively used of stainless metal grades, due to its combination of corrosion resistance, formability, and ductility. 316 Stainless Steel provides the first-class resistance to pitting and corrosion of any of the austenitic (300 series) stainless steels. It is extremely ductile, with remarkable electricity at improved temperatures. Seamless tubing can be produced with heavier wall thicknesses for mechanical applications. It is best for hydraulic and instrumentation tubing in chemical, textile, and pulp and paper industries, and in marine environments.Austenitic Stainless Steel:TP304, TP304L, TP304H, TP304LN, TP316/TP316L, TP316H, TP316Ti, TP316LN, TP321, TP321H, TP317, TP317L, TP347, TP347H, TP310S, TP310H, TP904L, 254Mo.S30432, S31042, N08367Duplex & Super Duplex Steel:S31500, S32101, S32003, S32304, S31803, S32205, S32750, S32760Nickel Alloy Steel:N06600, N06601, N06625, N07718, N08800, N08825, N10276, N04400, N05500, N02200, N02201, N08028, N08810, N08811, N08020, N10624, N10629, N10675Most sizes of stainless steel tubes can be cut to actual sizes. Non wellknown sizes can be made to order. Contact a sales associate nowadays for your specific stainless steel tubing requirement.OD 0.19mm--44.5mmT.W: 0.08mm--3.81mm\\n',\n",
       " 'Product description\\n\\nAerospace Tubing Description HUASHANG affords a broad range of grades and dimensions of stainlesss steel tubing for the aerospace industry. Grades encompass stainless steels for hydraulic purposes in particular, but additionally for structural components. HUASHANG stocks 304, 304L, 316, 316L, 321, alloys in drawn Stainless tubing in tempers of, annealed, 1/8, 1/4, half hard and full hard.\\n\\n\\n\\n304 and 304L are austenitic alloys. These alloys are the most familiar and frequently used alloys in the stainless steel family. The Lin 304L refers to a lower carbon content to permit for a higher corrosion protection when welding. Material produced to AMS-T-6845 is appropriate for use in excessive strain hydraulic systems.\\n\\n\\n\\nAerospace Tubing Sizes OD: 0.25mm--50.8mm\\n\\nW.T: 0.08mm--5.0mm\\n\\n\\n\\nAerospace Tubing Materials Austenitic Stainless Steel:\\n\\nTP304, TP304L, TP304H, TP316/TP316L, TP316H, TP316Ti, TP321, TP321H, TP317, TP317L, TP347, TP347H, TP310S, TP310H, TP904L, 254Mo.\\n\\n\\n\\nDuplex & Super Duplex Steel:\\n\\nS31803, S32205, S32750, S32760.\\n\\n\\n\\nNickel Alloy Steel:\\n\\nN06600, N06601, N06625, N07718, N08800, N08825, N010276, N04400, N05500, N08020, N08028\\n\\n\\n\\nAerospace Tubing Standard AMS 5566/5564, AMS 5645, AMS 5557/5570, AMS 5571/5556, AMS 5581, AMS 5590 etc\\n\\n\\n\\nApplication Areas\\n',\n",
       " 'Best Delivery Service in Ghana\\n\\nLooking for Best Delivery Service in Ghana? Find the list of Top Best Delivery Service in Ghana on our business directory. Best Delivery Service near me.\\n',\n",
       " 'Murray Stein, Ph.D. is an American Jungian analyst and author living and working in Zürich, Switzerland.\\n\\nDr. Stein holds a Master of Divinity from Yale University, a Diploma in Analytical Psychology (the degree of a Jungian analyst) from the C.G. Jung Institute Zürich, and a Ph.D. in religion and psychological studies from the University of Chicago.\\n\\nHe was the founding member and first president of the Chicago Society of Jungian Analysts, a co-founder of the Inter-Regional Society of Jungian Analysts, and is an honorary member of the C.G. Jung Institute of Chicago.\\n\\nDr. Stein served as president of the International Association for Analytical Psychology from 2001 to 2004 and the International School of Analytical Psychology Zürich from 2008 to 2012. For over thirty years, he worked as editor and publisher at Chiron Publications, and is now a training and supervising analyst at the International School of Analytical Psychology in Zürich.\\n\\nHe is the author of many books and articles, including The Principle of Individuation: Toward the Development of Human Consciousness, Minding the Self: Jungian Meditations on Contemporary Spirituality, and, along with Dr. Thomas Arzt, is the editor of the 3-volume series, Jung’s Red Book for Our Time: Searching for Soul Under Postmodern Conditions, published by Chiron.\\n\\nHis book, Jung’s Map of the Soul: An Introduction, first published in 1998, has become a #1 Best Seller due to its recognition by the K-Pop group BTS. Their new CD, titled Map of the Soul: Persona, was released on April 12th, and it is the subject of our talk today.\\n\\nThis interview was recorded on Sunday, April 14, 2019. It’s 55:55 long and 46.3 MB. You can listen to it right here in your browser or download it directly to your computer. This episode is also available on iTunes (Apple Podcasts), Stitcher, Google Play, TuneIn, and Spotify.\\n\\n*New: This episode is now available on YouTube. Please subscribe to our channel – it’s free.\\n',\n",
       " 'I wore it to a fall wedding and the material is thin enough that it could work for a spring event too!\\n\\nDM | Fit: True to size\\n',\n",
       " 'Download Pride and Prejudice by Charles Dickens For Free In PDF, EPUB, MOBI and AZW3\\n',\n",
       " '4x4 Car Rental Kigali\\n\\nAt Crystal Car Rentals , we take an extra hand to other services that might fall under vacationists. In this case, after you find a vehicle with us, you will need to know where to go. In this case, we have chauffeurs for those who would need to have one. This saves time and money because you will always get there on time with our well-trained driver guides.\\n\\nWe will find you the best place where you will spend an overnight depending on which Safari destination you are heading to. Accommodation means a lot to a vacationer because it determines how good a trip was. We have contracts with a number of hotels in Rwanda and this implies that we have improved deals of the rates. Therefore though we expect a greater discount.\\n\\nGuided safaris throughout Rwanda. Because you are on your first trip to Rwanda, we know you need not waste your precious time finding a way out. In this, we have guided tours especially to the major tourist destinations where you have to get in time. Our safaris also tend to cross borders to Uganda, Burundi, and Congo. All these places have different attractions from Rwanda and are good destinations which we recommend you to go with our guides.\\n',\n",
       " 'Rumsfeld Memos Won by NSArchive Play Key Role in “The Afghanistan Papers”: FRINFORMSUM 12/13/19\\n\\nRumsfeld Memos Play Key Role in “The Afghanistan Papers”\\n\\nDonald Rumsfeld’s “snowflakes” – memos that the former Secretary of Defense was as fond of sending subordinates as President Trump is of tweeting – play an important role in the Washington Post’s massive exposé on the Afghanistan war, The Afghanistan Papers. The series draws on both “lesson learned” interviews conducted by the Special Inspector General for Afghanistan Reconstruction, as well as Rumsfeld’s “snowflakes” that were obtained by the National Security Archive and provided to the Post (both the interviews and the snowflakes were obtained through FOIA lawsuits).\\n\\nSeveral of the snowflake highlights include:\\n\\nAn April 17, 2002 snowflake, Subject: Afghanistan, in which Rumsfeld states “We are never going to get the U.S. military out of Afghanistan unless we take care to see there is something going on that will provide the stability necessary for us to leave. Help!” (From “At War With the Truth”)\\n\\nAn October 21, 2002 snowflake, Subject: Meeting with President, that shows Afghanistan had become an after-thought as the George W. Bush administration plodded towards the invasion of Iraq. Rumsfeld asked the president if he wanted to meet with Army Lt. Gen. Dan McNeill, who had been the commander of US forces in Afghanistan for six months. “He said, ‘Who is General McNeill?’ I said he is the general in charge of Afghanistan. He said, ‘Well, I don’t need to meet with him.’” (From “Stranded Without a Strategy”)\\n\\nAn April 1, 2002 snowflake, Subject: Warlords, that would be a harbinger for US-sponsored corruption among Afghan warlords. Rumsfeld writes, “It seems to me the interagency group ought to have a plan for how we are going to deal with each of these warlords – who is going to get money from whom, on what basis, in exchange for what, what is the quid pro quo, etc.” On June 26, 2002 Rumsfeld followed up with the question, “Is the DoD giving any food, weapons or money to any of the warlords or to Karzai? Is the CIA doing that? Is State doing it? How are the donor funds coming in? We need to get a sense of the balance.” (From “Consumed by Corruption).\\n\\nNDAA Declassification Provisions\\n\\nTwo provisions of the House-Senate conference version of the FY2020 National Defense Authorization Act could be good news for researchers (thanks to Steve Aftergood for highlighting them in his Secrecy News blog). The first provision would require the Defense Department to plan how it will meet its declassification requirements, including for “legally mandated historical declassification, and reduce its backlog. (Language in the House bill that would have required similar reports from the State Department and the CIA were dropped from the final bill.) The well-meaning provision is not accompanied by any new funding for declassification or development of new technologies, however, and does not specify what happens if the DOD fails to meet its goals.\\n\\nThe second provision requires the DOD to produce an unclassified report on nuclear weapons programs in the US, China, and Russia. This is a welcome development after the DOD stopped releasing the current size of the United States’ nuclear stockpile, which it had been releasing annually since 2010.\\n\\nWhile the declassification provisions are welcome, other components of the NDAA are not. The Reporters Committee’s Melissa Wasser writes that the expansion of the Intelligence Identities Protection Act “could indefinitely criminalize the disclosure of the identity of ‘covert agents,’ regardless of whether the disclosure would present a risk of harm.” In July, the National Security Archive joined Reporters Committee and 27 other open government groups asking Congress to remove the provision criminalizing reasonable disclosure.\\n\\nThe House voted to pass the NDAA on December 12, sending it to President Trump for a likely signature.\\n\\nTrapped in the Archives: The U.S. Government’s System for Declassifying Historical Documents is in Crisis\\n\\nThe government’s processes for declassifying historical records are antiquated at best, and the entire system threatens to buckle under the weight of terabytes of incoming electronic records, this according to Archivist William Burr’s recent must-read Foreign Affairs article. Burr lays out a number of the systemic failures – from Congress not adequately funding key records management agencies, like the National Archives and Records Administration, to individual agencies compounding resource constraints with needless secrecy.\\n\\nThese problems, in addition to being a headache for requesters and FOIA processors, are bad for America’s self-governance. As Burr notes, “Declassification is vital to a thriving democracy. Not only does it help the public hold leaders accountable; it also allows for a more accurate and comprehensive accounting of the past… Only by unsealing its archives can the United States live up to its ideals as an open society and learn from its past.” And perhaps the best first step to unsealing the archives is for Congress to increase NARA’s budget; other suggestions include establishing advisory panels at key agencies like the Defense Department and CIA, requiring the DOD to create a centralized FOIA processing system, and forcing agencies to treat ISCAP declassification decisions as binding precedent.\\n\\nNew Digital National Security Archive Document Collection Covers US Policy toward Iran from 1978-2015\\n\\nAn extensive new Digital National Security Archive collection covering US policy towards Iran from the Carter through Obama years is now available! Most of the documents in the 1,761-document collection (produced with our partners at ProQuest) were obtained through FOIA and have never been published elsewhere.\\n\\nThe extensive breadth and depth of the set encompasses all major events of importance, such as Shah Mohammad Reza Pahlavi’s flight from Iran during the revolution which ultimately led to the 444-day hostage crisis, the Iran-Iraq war of 1980-1988 that continues to shape the narrative of Iran’s rulers, Iran’s explosive internal political scene during the 1990s, and the more recent post 9-11 landscape where terrorism and the nuclear issue have been the main drivers of global concern.\\n\\nNuclear Weapons and Ukraine: American, Ukrainian, and Russian Cooperation Eliminated World’s Third Largest Nuclear Force in 1990s\\n\\nThe global threats faced by the ICBMs, strategic bombers, and nuclear warheads that were left in Ukraine when the Soviet Union dissolved in 1991, were eliminated by cooperation between the US, Ukraine, and the Russian Federation – this according to declassified documents recently published by the Archive.\\n\\nThe documents detail the intensive trilateral diplomacy over Ukraine’s nuclear legacy beginning even before December 1991 and describe the vital role played by the Nunn-Lugar initiative. According to Tom Blanton and Svetlana Savranskaya, the posting also “directly addresses current narratives in all three countries that are historically misleading. In the U.S., the impeachment controversy features almost total amnesia about the extraordinary contribution to U.S. national security made by Ukraine’s decision to disarm, removing over 1,900 strategic weapons targeted on the U.S. In Russia, the new nationalist discourse dismisses the Nunn-Lugar cooperative threat reduction as forced disarmament, forgetting that the consolidation of the Soviet nuclear legacy in Russia directly served Russia’s security interests. In Ukraine, nostalgia for nuclear status is on the rise, fueled by the Russian annexation of Crimea and war in Donbas, while ignoring the enormous costs to Ukraine (diplomatic, financial, environmental, and more) had nuclear weapons been retained in the 1990s.”\\n\\nPeter Kornbluh Interviews Chile’s Mónica González\\n\\nThe Archive’s Chile Documentation Project director, Peter Kornbluh, recently interviewed Chilean journalist Mónica González about the ongoing protests in Chile for The Nation. Chileans are protesting economic inequality (the country is one of the 20 most unequal in the world despite its economic prosperity), corruption, and an array of government abuses. Government forces have killed more than 22 people, blinded more than 200 with rubber bullets, injured more than 2,000, and arrested more than 6,000 since the protests began. Read the wide-ranging interview here.\\n\\nISCAP Releases NSSE Order\\n\\nThis week’s last item comes from FRINFORMSUM reader Austin Nolen, who recently received a previously-classified 2007 George W. Bush administration order on reforming National Special Security Event authorities from the Interagency Security Classification Appeals Panel (ISCAP). (NSSEs are events – like the upcoming Republican and Democratic National Convention – that the Department of Homeland Security deems important enough to be a potential target for terrorist or criminal activity.) As Austin notes, “The document references back to Bush HSPD 7, unclassified, which appears to be the first time the DHS Secretary received NSSE designation authority, which had previously been given to the [Attorney General] and Treasury when NSSEs were first created by Clinton in NSC-62.” Thank you very much to Mr. Nolen for making these available to Unredacted and its readers!\\n\\nSign Up\\n\\nWant to stay on top of the latest FOIA news? Click here to sign up for our weekly FRINFORMSUM (Freedom of Information Summary) email newsletter.\\n',\n",
       " 'Link Copied\\n\\n\\n\\nOne of Europe’s loveliest urban journeys begins as you step aboard a trolley at the Montgomery Metro station in Brussels. Its tracks quickly emerge from underground to travel along a grand, tree-shaded boulevard lined with elegant mansions a century old or more, many of them now embassies. Then the route leaves the street traffic behind to run through a leafy forest of beech and oak, a former hunting ground for the dukes of Brabant that becomes a symphony of fluttering green light on a spring day. Finally the tracks end near a palatial stone edifice whose very existence embodies some of the unresolved tensions of our globalized world. To hear more feature stories, see our full list or get the Audm iPhone app. Welcome to the Royal Museum for Central Africa. Although one of the largest museums anywhere devoted exclusively to Africa, it is thousands of miles from the continent itself. The tall windows, pillared facade, rooftop balustrade, and 90-foot-high rotunda of the main building give it the look of a chateau. That impression is only enhanced by an inner courtyard and a surrounding park: formal French gardens, a reflecting pool and fountain, ponds with ducks and geese, wide lawns laced with hedges, and carefully groomed paths that sweep away to majestic trees in the distance.\\n\\nA visitor here is a long way from Africa, but not from the fruits of the continent’s colonization. For 23 years starting in 1885, Belgium’s King Leopold II was the “proprietor,” as he called himself, of the misnamed Congo Free State, the territory that today is the Democratic Republic of Congo. Exasperated by the declining power of European monarchs, Leopold wanted a place where he could reign supreme, unencumbered by voters or a parliament, and in the Congo he got it. He made a fortune from his privately owned colony—well over $1.1 billion in today’s dollars—chiefly by enslaving much of its male population as laborers to tap wild rubber vines. The king’s soldiers would march into village after village and hold the women hostage, in order to force the men to go deep into the rain forest for weeks at a time to gather wild rubber. Hunting, fishing, and the cultivation of crops were all disrupted, and the army seized much of what food was left. The birth rate plummeted and, weakened by hunger, people succumbed to diseases they might otherwise have survived. Demographers estimate that the Congo’s population may have been slashed by as much as half, or some 10 million people. Using testimony and photographs from missionaries and whistle-blowers, the British journalist Edmund Dene Morel turned Leopold’s slave-labor system into an international scandal. Luminaries from Booker T. Washington to Mark Twain to the archbishop of Canterbury took part in mass protest meetings. Rising outrage finally pressured the king to reluctantly sell the Congo to Belgium in 1908, a year before his death. The museum was filled with relics of colonial soldiers and idealized figures with inscriptions like “Belgium Brings Civilization to the Congo.” Until that point, Leopold, a master of public relations, had worked hard to portray himself as a philanthropist, motivated only by the desire to bring Christianity and civilization to the “Dark Continent.” In 1904, he had hired his favorite architect, the Frenchman Charles Girault, who designed the Petit Palais in Paris, to build this museum on the site of a royal PR coup seven years earlier. In 1897, when a world’s fair took place in Brussels, the king had orchestrated a special exhibit on the Congo here, just outside the city. Its centerpiece was human beings: 267 Congolese men, women, and children who for several months were on display in three specially constructed villages with thatched roofs. In the “river village” and the “forest village” they used drums, tools, and cooking pots brought from home, and paddled dugout canoes around a pond. In the “civilized village,” men dressed in the uniform of Leopold’s private Congo army played in a military band. More than 1 million visitors came to see them.\\n\\nIn 1910, soon after the king died and his personal colony became the Belgian Congo, the museum finally opened its doors. Part of it houses archives and sponsors natural-science research, but throughout the 20th century, its public exhibition halls continued to express a highly colonial view of the world. The human zoo was gone, but silence about the plunder remained. When I first visited the museum, in 1995, the exhibits of Congo flora included a cross section of rubber vine—but not a word about the millions of Congolese who died as a result of the slave-labor system established to harvest that rubber. It was as if a museum of Jewish life in Berlin made no reference to the Holocaust. After I mentioned my visit in King Leopold’s Ghost: A Story of Greed, Terror, and Heroism in Colonial Africa, published a few years later, a dissident staff member began emailing me about internal conflicts. The museum remained filled with relics of colonial soldiers and explorers and larger-than-life statues of heroic, idealized figures with inscriptions like “Belgium Brings Civilization to the Congo.” Belgians who cared about human rights were demanding changes; the country’s powerful “old colonial” lobby—people who had lived and worked in the Congo before it became independent, in 1960, and their descendants—was resisting them. The institution was paralyzed. Finally, in 2005, with much fanfare, a temporary exhibit purported to tell the truth about colonialism at last. It contained a few small photographs that showed the violence of colonial rule—but not a single display case explained the slave-labor system. The exhibit was so evasive that an activist group in Brussels published an online guide in the country’s two main languages, French and Dutch, that visitors could print out and take to the museum. It provided text and photographs—of women hostages in chains, for example, and enslaved laborers carrying baskets of wild rubber—to fill in the history that was not on display, room by room.\\n\\nA sign that year promised a new museum in 2010. But when 2010 came, only a small portion of display space had changed, given over to marking the 50th anniversary of Congolese independence. The exhibit did a considerably more honest job than the one five years earlier, but it, too, was temporary, gone after a few months. Finally, in 2013, the museum announced that it was closing down for a complete revamping, and would reopen in 2017. Behind the scenes, occasionally leaking into the press, tensions remained. That was hardly surprising, given that Europeans were spending a huge sum—the renovation bill would eventually total $83 million—to portray Africa to the world. Half a dozen scholars from Belgium’s African-diaspora community were recruited as an advisory committee, but they had to sign nondisclosure agreements, were given no authority, and came to feel that their advice was being ignored. Eventually the committee stopped meeting. One imaginative historian-anthropologist who worked at the museum for a time suggested that Africans should be invited to build a museum-within-the-museum portraying how they saw Belgium, but this idea was considered too radical. The year 2017 passed, and the museum remained closed. Unusual as the Royal Museum for Central Africa might be, the conflict over its contents mirrors similar arguments over museums, historic sites, and monuments everywhere from Scotland to Cape Town to Charlottesville, Virginia, where a protest and counterprotest over the removal of a statue of Confederate General Robert E. Lee turned deadly. Elsewhere in the United States, the Museum of Man, in San Diego, recently hired a Navajo educator as its “director of decolonization” and announced that it would no longer display human remains without tribal consent. In Monticello, Virginia, Thomas Jefferson’s home now has exhibit space devoted to Sally Hemings, the enslaved mother of some of his children. When the Fraternal Order of Retired Border Patrol Officers started the National Border Patrol Museum, in El Paso, Texas, several decades ago, little did they imagine that in 2019 the museum would close for several days after protesters pasted over its exhibits with photographs of children who had died in Border Patrol custody.\\n\\nMuseum professionals can now turn to a sudden plethora of books, symposia, workshops, and advice blogs about “creating conversation, not controversy,” “future-proofing” a museum, and handling protesters. The main problem, of course, is that so many monuments and museums were built a century or more ago by people who took colonialism, racial hierarchy, and slavery (or at least a benign Gone With the Wind view of the American South) for granted. You “can easily rewrite a textbook,” Lonnie Bunch, the founding director of the National Museum of African American History and Culture (and now the secretary of the Smithsonian Institution), has said, “but you can’t rewrite a museum.” Read: How a museum reckons with black pain Sometimes, though, you have to try. Of course, new museums can be built from scratch, and the African American museum, which opened in 2016, is the country’s most impressive in decades. With nearly 2 million visitors a year, it is arguably more influential than any textbook. But what if your existing museum already has even more visitors, sits on hundreds of millions of dollars’ worth of real estate, and owns more than 100 years’ worth of collections? Should you tear the place down? And what should you do with the stuff in it, especially when some of that stuff was booty gathered from conquered peoples at gunpoint? More than 90 percent of sub-Saharan African items housed in museums, for example, are held outside that continent. This is the Elgin Marbles controversy writ large. Should art or cultural objects taken from somewhere else be returned to the territories they came from? Even if that makes moral sense, it doesn’t always work out. The Royal Museum for Central Africa, in fact, gave a small portion of its magnificent African art collection to a museum in the Democratic Republic of Congo some 40 years ago. But the country’s long-term dictator at that time, Mobutu Sese Seko, was famously kleptocratic, and within a few years many of those same objects began appearing for sale in Europe, some in the shops of Brussels antique dealers.\\n\\nNowhere in the United States is a museum controversy so heated as at New York City’s venerable American Museum of Natural History. Its 5 million annual visitors have included, for four years now, hundreds of demonstrators who have trooped through the museum on an Anti–Columbus Day Tour. They chant, drum, dance, and unfurl banners: rename the day. respect the ancestors. decolonize! reclaim! imagine! They deliver speeches demanding changes, a few of which the museum is slowly making. Their prime target is the way exhibits still inherently reflect the assumptions of the museum’s 19th-century founders: that Native Americans, Africans, Eskimos, and stuffed rhinos and tigers are all, in some manner, equally exotic and museum-worthy—while that which comes from Europe or white America, being civilized rather than “natural,” does not merit being displayed. In one TV-news report, Marz Saffore, a young black woman from Decolonize This Place, the group that organizes the Columbus Day protests, stands in front of the sign for the museum’s Hall of African Peoples and points out, “There is no Hall of European Peoples. There’s no Hall of European Mammals. Because that’s called history; that’s called science.” Why, asks a leaflet from the group, “do Indigenous, Asian, Latin American, and African cultural artifacts reside in the AMNH, while their Greek and Roman counterparts are housed in the Metropolitan Museum of Art across the park?” Why are the rings on the cross section of an ancient California sequoia labeled with dates from “Eurocentric” history (Columbus “discovers” the Orinoco River, Yale is founded, Napoleon takes power) and not from the history of the peoples who lived in its shadow? Many of the signs on exhibits are now apologetic. Colonialism “remains a very controversial period,” one says gingerly. The protesters are also demanding the removal of the statue of Theodore Roosevelt on horseback (flanked by subservient African and Native American figures on foot) that stands in front of the museum. Yes, Roosevelt gave us many national parks, they say, but much of the land for those parks was cleansed of Native American inhabitants. And let’s not forget his enthusiasm for eugenics and his drumbeating for the Spanish-American and Philippine Wars and other imperial conquests. Two years ago, the base of the statue was splashed with red paint. Online, a group called the Monument Removal Brigade claimed credit: “Now the statue is bleeding. We did not make it bleed. It is bloody at its very foundation.” The museum acknowledged the protesters in July by including some of their voices in an exhibit and website called “Addressing the Statue.” But the statue still stands.\\n\\nA red-paint bath has also been the fate of several of the dozen-plus statues of King Leopold II scattered across Belgium. A bust of the king was recently stolen from a Brussels park and replaced with one of Nelson Mandela. The battle over monuments, like that over museums, is global—and far from resolved. In December 2018, more than a decade after plans for changes were first announced, the Royal Museum for Central Africa finally reopened, and a few months later I again rode the trolley to see it. The museum now includes a new glass-and-steel building next to the original chateau, plus more space underground. One of the first things a visitor sees refers to the controversy over whether the place should have been changed at all. A well-known piece of sculpture from the old museum, Leopard Man, was acquired in 1913: a large, menacing figure of an African dressed in leopard skin, with clawlike knives in his hands, about to pounce on a sleeping victim. Now a painting by a Congolese artist, Chéri Samba, titled Reorganization, shows the statue on its pedestal, teetering on the outside steps of the museum. A group of black men and women are pulling on ropes to try to haul it away; several white people strain at another set of ropes, trying to prevent its removal. The museum director, in suit and tie, looks on impassively, arms crossed. Many of the multilingual signs on exhibits are now apologetic. Colonialism “remains a very controversial period,” one says gingerly. “The collections of the Royal Museum for Central Africa have been composed by Europeans; it remains a challenge, therefore, to tell the colonial history from an African perspective.” Another points out, “Collections often say more about who has collected them than about the society in which the objects were made and used. From the outset, Africans opposed colonization in different ways, but this is hardly apparent from the collections of this museum.”\\n\\nSuch apologies are just the sort of thing that enrage the “old colonial” lobby. A former official of a group of colonial-era veterans has denounced the museum for featuring “the worst slanders.” An open letter from another critic accused the director of being “politiquement correct.” An online screed condemned him for “Belgium bashing.” The apologies, however, continue throughout the building. Some of them are vague and bland (their wording no doubt the outcome of testy arguments and compromises), but at their best they implicitly acknowledge that almost anything on public exhibit anywhere is a political statement—something few museums do. For example, the institution has a huge collection of photographs, but signs now explain: “They were almost exclusively made by white people and mainly show their perspective.” “They were carefully staged.” “Political leaders and dignitaries from rural areas were presented as ‘noble savages,’ while laughing city dwellers conveyed the image of a model colony.” This is followed up with a remarkable early photo showing just such a portrait of a “noble savage” being staged. Two sun-helmeted Belgians are preparing to photograph an unsmiling, half-naked Congolese man in profile. One of the white men has his head under a black cloth behind an ancient tripod-mounted camera; the other has his hands sternly on his hips, a few feet away from the black man, as if he has just ordered him into position. It is hard to imagine a more vivid portrayal of the colonial view of Africa, captured in the making.\\n\\nA notorious part of the old museum was its giant rotunda, filled with huge statues of such figures as The Worker (a black man with loincloth and shovel), The Warrior (a black man with spear), Justice (a gilded, robed white woman, scales in one hand, sword in the other), and Belgium Brings Well-Being to the Congo (a gilded, robed, saintly white woman comforting two black children). Now a sign describes the “colonial vision” behind the statues: “Belgians are presented as benefactors and civilizers, as if they had committed no atrocities in the Congo, and as if there had been no civilization there beforehand.” The sign goes on to explain that the statues have landmark status and cannot be removed. So the museum invited a Congolese artist, Aimé Mpane, to create a work as “an explicit response.” This is an enormous chiseled-wood representation of an African man’s head, sitting on a base the shape of Africa. As a piece of art, it did not move me, but I liked the idea of one sculpture as an answer to another. It reminded me of a recent article in The Chronicle of Higher Education, in which the sociologist Troy Duster, a grandson of the anti-lynching crusader Ida B. Wells, suggested something similar for the United States: Why not leave Robert E. Lee in place, but put up a statue of William Lloyd Garrison or Frederick Douglass next to him? Although the museum’s “Colonial History and Independence” gallery takes up a disappointingly small portion of the building’s total space, it does not stint on displaying colonialism’s dark side. Video monitors show historians—almost all of them Congolese—talking about the vast death toll of the slave-labor system, and about Belgian complicity in the 1961 assassination of the independent Congo’s first democratically chosen prime minister, Patrice Lumumba, who gets a corner of the room all to himself. Several of the atrocity photographs that helped rouse world outrage about the slave-labor system are on display. So are examples of the ubiquitous chicotte, a whip made of twisted sun-dried hippopotamus hide with sharp edges, used to beat enslaved laborers, sometimes to death. A photograph and a painting show it in use. Also on exhibit are some of the pamphlets and books written to expose the system, both by Belgians and by foreigners. Visitors can see cartoons mocking Leopold, and transcripts of statements made by black witnesses before a 1904–05 investigative commission—testimony suppressed for more than half a century, first by Leopold and then by the Belgian government.\\n\\nThough this exhibit has drawn the most ire from the “old colonial” lobby, it also clearly reflects some unresolved differences among the museum staff. Whoever chose the chicottes and other objects on display had a far different sense of history than whoever compiled the interactive historical timeline on computers in this gallery and several others. It omits several major anti-colonial rebellions and never mentions the large mutinies among black conscripts in King Leopold’s private army. Slave labor gets mentioned only in passing, and the scale of the international protest movement is barely hinted at. The timeline notes, however, the appointments of various governors-general and ministers of colonies, and the creation of the Congo’s first Boy Scout troop. A good museum should make you start looking at the world beyond its walls with new eyes. A greater shortcoming is that nothing here really links the exploitation of the Congo’s riches—ivory and rubber in the early days; copper, diamonds, uranium, and much more later on—with Belgium’s own prosperity. Congolese profits helped fund, for instance, the giant archway of the Arcades du Cinquantenaire, a Brussels landmark. And how many of the mansions that visitors pass on their trolley ride to the museum were built with such wealth? A 2007 survey showed that the fortunes of nine of the 23 richest families in Belgium had roots in the colonial Congo. A good museum should make you start looking at the world beyond its walls with new eyes.\\n\\nBut few museums do so. Where in the United States can you find a first-rate exhibit showing the connections between American corporate profits and our long string of military interventions in the Caribbean and Central America? You can now see slave quarters at restored southern plantations, but only recently, for example, have Providence and Boston announced plans to create museums linking their city’s prosperity to the slave trade. New York’s enormously wealthy Brown family (of Brown Brothers Harriman) even owned southern slave plantations outright—and, incidentally, were early patrons of the American Museum of Natural History. Related Stories The Museum of Colonialism\\n\\nThe Museum As Bazaar A major problem that museum suffers from is echoed at the Royal Museum in Belgium. Exhibits about the lives and history and art of African peoples continue to share a building with stuffed animals—an elephant, a giraffe, multiple crocodiles, snakes, butterflies, insects—and rocks. The same space remains a container for everything African, whether human, animal, or mineral. One of the African-diaspora scholars consulted by the Royal Museum urged that at least the animals be given to Belgium’s large Museum of Natural Sciences, but her advice was not taken. As an astute critic put it in the Belgian magazine Ensemble, the institution remains un musée des Autres, a museum of Others. Despite the limitations of the revamped Royal Museum, it now has one feature that is quietly stunning. One wall has long held an immense marble panel on which are written the names of 1,508 Belgians who died in the earliest years of colonization, before Leopold’s personal rule over the Congo ended in 1908. The panel also bears a quotation from the king’s successor, his nephew, Albert I: “Death reaped mercilessly among the ranks of the first pioneers. We can never pay sufficient homage to their memory.”\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_data = []\n",
    "with open(os.path.join(dataset_path, 'train.bpe.txt'), 'w', encoding='utf-8') as oh:\n",
    "    for line in data:\n",
    "        bpe_data.append(gpt2_bpe.encode(line).strip())\n",
    "        oh.write(bpe_data[-1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bpe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21478 8981 504 351 257 4301 1700 783 423 257 10595 3108 329 13925 257 1597 355 257 649 1099 12850 8733 319 11524 329 4708 16625 13 198 198 9012 1432 13 10923 756 12652 7504 1539 360 12 25705 11 5495 2097 3941 2608 2154 938 614 11 340 3804 262 3611 10006 287 1737 11 10964 13 449 33 350 29574 6122 4488 340 287 2932 290 340 1718 1245 2365 13 352 13 198'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Illinoisans with a criminal record now have a wider path for launching a business as a new law reduces restrictions on applying for professional licenses.\\n\\nState Rep. Lamont Robinson Jr., D-Chicago, introduced House Bill 2670 last year, it passed the General Assembly in May, Gov. JB Pritzker signed it in August and it took effect Jan. 1.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_bpe.decode(bpe_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = options.get_preprocessing_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = os.path.join(dataset_path, 'roberta-data-bin')\n",
    "os.makedirs(lm_dataset, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.only_source = True\n",
    "args.trainpref = os.path.join(dataset_path, 'train.bpe.txt')\n",
    "args.destdir = lm_dataset\n",
    "args.srcdict = os.path.join(dataset_path, 'dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 09:35:35 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='/mnt/dl/fairseq/Masked_Language_Model/openwebtext/train.bpe.txt', validpref=None, testpref=None, align_suffix=None, destdir='/mnt/dl/fairseq/Masked_Language_Model/openwebtext/roberta-data-bin', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='/mnt/dl/fairseq/Masked_Language_Model/openwebtext/dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=1, dict_only=False)\n",
      "2023-06-25 09:35:36 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
      "2023-06-25 09:36:36 | INFO | fairseq_cli.preprocess | [None] /mnt/dl/fairseq/Masked_Language_Model/openwebtext/train.bpe.txt: 10000 sents, 10333746 tokens, 0.0% replaced (by <unk>)\n",
      "2023-06-25 09:36:36 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to /mnt/dl/fairseq/Masked_Language_Model/openwebtext/roberta-data-bin\n"
     ]
    }
   ],
   "source": [
    "preprocess(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary.load(os.path.join(lm_dataset, 'dict.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '<pad>', '</s>', '<unk>', '13', '262', '11', '284', '290', '286']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.symbols[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 09:36:38 | INFO | fairseq.data.data_utils | loaded 10,000 examples from: /mnt/dl/fairseq/Masked_Language_Model/openwebtext/roberta-data-bin/train\n"
     ]
    }
   ],
   "source": [
    "dataset = fairseq.data.data_utils.load_indexed_dataset(os.path.join(lm_dataset, 'train'), dictionary, args.dataset_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 849,  751,  426, ...,  684, 3770,   79], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(idx):\n",
    "    return gpt2_bpe.decode(dictionary.string(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rumsfeld Memos Won by NSArchive Play Key Role in “The Afghanistan Papers”: FRINFORMSUM 12/13/19\\n\\nRumsfeld Memos Play Key Role in “The Afghanistan Papers”\\n\\nDonald Rumsfeld’s “snowflakes” – memos that the former Secretary of Defense was as fond of sending subordinates as President Trump is of tweeting – play an important role in the Washington Post’s massive exposé on the Afghanistan war, The Afghanistan Papers. The series draws on both “lesson learned” interviews conducted by the Special Inspector General for Afghanistan Reconstruction, as well as Rumsfeld’s “snowflakes” that were obtained by the National Security Archive and provided to the Post (both the interviews and the snowflakes were obtained through FOIA lawsuits).\\n\\nSeveral of the snowflake highlights include:\\n\\nAn April 17, 2002 snowflake, Subject: Afghanistan, in which Rumsfeld states “We are never going to get the U.S. military out of Afghanistan unless we take care to see there is something going on that will provide the stability necessary for us to leave. Help!” (From “At War With the Truth”)\\n\\nAn October 21, 2002 snowflake, Subject: Meeting with President, that shows Afghanistan had become an after-thought as the George W. Bush administration plodded towards the invasion of Iraq. Rumsfeld asked the president if he wanted to meet with Army Lt. Gen. Dan McNeill, who had been the commander of US forces in Afghanistan for six months. “He said, ‘Who is General McNeill?’ I said he is the general in charge of Afghanistan. He said, ‘Well, I don’t need to meet with him.’” (From “Stranded Without a Strategy”)\\n\\nAn April 1, 2002 snowflake, Subject: Warlords, that would be a harbinger for US-sponsored corruption among Afghan warlords. Rumsfeld writes, “It seems to me the interagency group ought to have a plan for how we are going to deal with each of these warlords – who is going to get money from whom, on what basis, in exchange for what, what is the quid pro quo, etc.” On June 26, 2002 Rumsfeld followed up with the question, “Is the DoD giving any food, weapons or money to any of the warlords or to Karzai? Is the CIA doing that? Is State doing it? How are the donor funds coming in? We need to get a sense of the balance.” (From “Consumed by Corruption).\\n\\nNDAA Declassification Provisions\\n\\nTwo provisions of the House-Senate conference version of the FY2020 National Defense Authorization Act could be good news for researchers (thanks to Steve Aftergood for highlighting them in his Secrecy News blog). The first provision would require the Defense Department to plan how it will meet its declassification requirements, including for “legally mandated historical declassification, and reduce its backlog. (Language in the House bill that would have required similar reports from the State Department and the CIA were dropped from the final bill.) The well-meaning provision is not accompanied by any new funding for declassification or development of new technologies, however, and does not specify what happens if the DOD fails to meet its goals.\\n\\nThe second provision requires the DOD to produce an unclassified report on nuclear weapons programs in the US, China, and Russia. This is a welcome development after the DOD stopped releasing the current size of the United States’ nuclear stockpile, which it had been releasing annually since 2010.\\n\\nWhile the declassification provisions are welcome, other components of the NDAA are not. The Reporters Committee’s Melissa Wasser writes that the expansion of the Intelligence Identities Protection Act “could indefinitely criminalize the disclosure of the identity of ‘covert agents,’ regardless of whether the disclosure would present a risk of harm.” In July, the National Security Archive joined Reporters Committee and 27 other open government groups asking Congress to remove the provision criminalizing reasonable disclosure.\\n\\nThe House voted to pass the NDAA on December 12, sending it to President Trump for a likely signature.\\n\\nTrapped in the Archives: The U.S. Government’s System for Declassifying Historical Documents is in Crisis\\n\\nThe government’s processes for declassifying historical records are antiquated at best, and the entire system threatens to buckle under the weight of terabytes of incoming electronic records, this according to Archivist William Burr’s recent must-read Foreign Affairs article. Burr lays out a number of the systemic failures – from Congress not adequately funding key records management agencies, like the National Archives and Records Administration, to individual agencies compounding resource constraints with needless secrecy.\\n\\nThese problems, in addition to being a headache for requesters and FOIA processors, are bad for America’s self-governance. As Burr notes, “Declassification is vital to a thriving democracy. Not only does it help the public hold leaders accountable; it also allows for a more accurate and comprehensive accounting of the past… Only by unsealing its archives can the United States live up to its ideals as an open society and learn from its past.” And perhaps the best first step to unsealing the archives is for Congress to increase NARA’s budget; other suggestions include establishing advisory panels at key agencies like the Defense Department and CIA, requiring the DOD to create a centralized FOIA processing system, and forcing agencies to treat ISCAP declassification decisions as binding precedent.\\n\\nNew Digital National Security Archive Document Collection Covers US Policy toward Iran from 1978-2015\\n\\nAn extensive new Digital National Security Archive collection covering US policy towards Iran from the Carter through Obama years is now available! Most of the documents in the 1,761-document collection (produced with our partners at ProQuest) were obtained through FOIA and have never been published elsewhere.\\n\\nThe extensive breadth and depth of the set encompasses all major events of importance, such as Shah Mohammad Reza Pahlavi’s flight from Iran during the revolution which ultimately led to the 444-day hostage crisis, the Iran-Iraq war of 1980-1988 that continues to shape the narrative of Iran’s rulers, Iran’s explosive internal political scene during the 1990s, and the more recent post 9-11 landscape where terrorism and the nuclear issue have been the main drivers of global concern.\\n\\nNuclear Weapons and Ukraine: American, Ukrainian, and Russian Cooperation Eliminated World’s Third Largest Nuclear Force in 1990s\\n\\nThe global threats faced by the ICBMs, strategic bombers, and nuclear warheads that were left in Ukraine when the Soviet Union dissolved in 1991, were eliminated by cooperation between the US, Ukraine, and the Russian Federation – this according to declassified documents recently published by the Archive.\\n\\nThe documents detail the intensive trilateral diplomacy over Ukraine’s nuclear legacy beginning even before December 1991 and describe the vital role played by the Nunn-Lugar initiative. According to Tom Blanton and Svetlana Savranskaya, the posting also “directly addresses current narratives in all three countries that are historically misleading. In the U.S., the impeachment controversy features almost total amnesia about the extraordinary contribution to U.S. national security made by Ukraine’s decision to disarm, removing over 1,900 strategic weapons targeted on the U.S. In Russia, the new nationalist discourse dismisses the Nunn-Lugar cooperative threat reduction as forced disarmament, forgetting that the consolidation of the Soviet nuclear legacy in Russia directly served Russia’s security interests. In Ukraine, nostalgia for nuclear status is on the rise, fueled by the Russian annexation of Crimea and war in Donbas, while ignoring the enormous costs to Ukraine (diplomatic, financial, environmental, and more) had nuclear weapons been retained in the 1990s.”\\n\\nPeter Kornbluh Interviews Chile’s Mónica González\\n\\nThe Archive’s Chile Documentation Project director, Peter Kornbluh, recently interviewed Chilean journalist Mónica González about the ongoing protests in Chile for The Nation. Chileans are protesting economic inequality (the country is one of the 20 most unequal in the world despite its economic prosperity), corruption, and an array of government abuses. Government forces have killed more than 22 people, blinded more than 200 with rubber bullets, injured more than 2,000, and arrested more than 6,000 since the protests began. Read the wide-ranging interview here.\\n\\nISCAP Releases NSSE Order\\n\\nThis week’s last item comes from FRINFORMSUM reader Austin Nolen, who recently received a previously-classified 2007 George W. Bush administration order on reforming National Special Security Event authorities from the Interagency Security Classification Appeals Panel (ISCAP). (NSSEs are events – like the upcoming Republican and Democratic National Convention – that the Department of Homeland Security deems important enough to be a potential target for terrorist or criminal activity.) As Austin notes, “The document references back to Bush HSPD 7, unclassified, which appears to be the first time the DHS Secretary received NSSE designation authority, which had previously been given to the [Attorney General] and Treasury when NSSEs were first created by Clinton in NSC-62.” Thank you very much to Mr. Nolen for making these available to Unredacted and its readers!\\n\\nSign Up\\n\\nWant to stay on top of the latest FOIA news? Click here to sign up for our weekly FRINFORMSUM (Freedom of Information Summary) email newsletter.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text(dataset[8].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rumsfeld Memos Won by NSArchive Play Key Role in “The Afghanistan Papers”: FRINFORMSUM 12/13/19\\n\\nRumsfeld Memos Play Key Role in “The Afghanistan Papers”\\n\\nDonald Rumsfeld’s “snowflakes” – memos that the former Secretary of Defense was as fond of sending subordinates as President Trump is of tweeting – play an important role in the Washington Post’s massive exposé on the Afghanistan war, The Afghanistan Papers. The series draws on both “lesson learned” interviews conducted by the Special Inspector General for Afghanistan Reconstruction, as well as Rumsfeld’s “snowflakes” that were obtained by the National Security Archive and provided to the Post (both the interviews and the snowflakes were obtained through FOIA lawsuits).\\n\\nSeveral of the snowflake highlights include:\\n\\nAn April 17, 2002 snowflake, Subject: Afghanistan, in which Rumsfeld states “We are never going to get the U.S. military out of Afghanistan unless we take care to see there is something going on that will provide the stability necessary for us to leave. Help!” (From “At War With the Truth”)\\n\\nAn October 21, 2002 snowflake, Subject: Meeting with President, that shows Afghanistan had become an after-thought as the George W. Bush administration plodded towards the invasion of Iraq. Rumsfeld asked the president if he wanted to meet with Army Lt. Gen. Dan McNeill, who had been the commander of US forces in Afghanistan for six months. “He said, ‘Who is General McNeill?’ I said he is the general in charge of Afghanistan. He said, ‘Well, I don’t need to meet with him.’” (From “Stranded Without a Strategy”)\\n\\nAn April 1, 2002 snowflake, Subject: Warlords, that would be a harbinger for US-sponsored corruption among Afghan warlords. Rumsfeld writes, “It seems to me the interagency group ought to have a plan for how we are going to deal with each of these warlords – who is going to get money from whom, on what basis, in exchange for what, what is the quid pro quo, etc.” On June 26, 2002 Rumsfeld followed up with the question, “Is the DoD giving any food, weapons or money to any of the warlords or to Karzai? Is the CIA doing that? Is State doing it? How are the donor funds coming in? We need to get a sense of the balance.” (From “Consumed by Corruption).\\n\\nNDAA Declassification Provisions\\n\\nTwo provisions of the House-Senate conference version of the FY2020 National Defense Authorization Act could be good news for researchers (thanks to Steve Aftergood for highlighting them in his Secrecy News blog). The first provision would require the Defense Department to plan how it will meet its declassification requirements, including for “legally mandated historical declassification, and reduce its backlog. (Language in the House bill that would have required similar reports from the State Department and the CIA were dropped from the final bill.) The well-meaning provision is not accompanied by any new funding for declassification or development of new technologies, however, and does not specify what happens if the DOD fails to meet its goals.\\n\\nThe second provision requires the DOD to produce an unclassified report on nuclear weapons programs in the US, China, and Russia. This is a welcome development after the DOD stopped releasing the current size of the United States’ nuclear stockpile, which it had been releasing annually since 2010.\\n\\nWhile the declassification provisions are welcome, other components of the NDAA are not. The Reporters Committee’s Melissa Wasser writes that the expansion of the Intelligence Identities Protection Act “could indefinitely criminalize the disclosure of the identity of ‘covert agents,’ regardless of whether the disclosure would present a risk of harm.” In July, the National Security Archive joined Reporters Committee and 27 other open government groups asking Congress to remove the provision criminalizing reasonable disclosure.\\n\\nThe House voted to pass the NDAA on December 12, sending it to President Trump for a likely signature.\\n\\nTrapped in the Archives: The U.S. Government’s System for Declassifying Historical Documents is in Crisis\\n\\nThe government’s processes for declassifying historical records are antiquated at best, and the entire system threatens to buckle under the weight of terabytes of incoming electronic records, this according to Archivist William Burr’s recent must-read Foreign Affairs article. Burr lays out a number of the systemic failures – from Congress not adequately funding key records management agencies, like the National Archives and Records Administration, to individual agencies compounding resource constraints with needless secrecy.\\n\\nThese problems, in addition to being a headache for requesters and FOIA processors, are bad for America’s self-governance. As Burr notes, “Declassification is vital to a thriving democracy. Not only does it help the public hold leaders accountable; it also allows for a more accurate and comprehensive accounting of the past… Only by unsealing its archives can the United States live up to its ideals as an open society and learn from its past.” And perhaps the best first step to unsealing the archives is for Congress to increase NARA’s budget; other suggestions include establishing advisory panels at key agencies like the Defense Department and CIA, requiring the DOD to create a centralized FOIA processing system, and forcing agencies to treat ISCAP declassification decisions as binding precedent.\\n\\nNew Digital National Security Archive Document Collection Covers US Policy toward Iran from 1978-2015\\n\\nAn extensive new Digital National Security Archive collection covering US policy towards Iran from the Carter through Obama years is now available! Most of the documents in the 1,761-document collection (produced with our partners at ProQuest) were obtained through FOIA and have never been published elsewhere.\\n\\nThe extensive breadth and depth of the set encompasses all major events of importance, such as Shah Mohammad Reza Pahlavi’s flight from Iran during the revolution which ultimately led to the 444-day hostage crisis, the Iran-Iraq war of 1980-1988 that continues to shape the narrative of Iran’s rulers, Iran’s explosive internal political scene during the 1990s, and the more recent post 9-11 landscape where terrorism and the nuclear issue have been the main drivers of global concern.\\n\\nNuclear Weapons and Ukraine: American, Ukrainian, and Russian Cooperation Eliminated World’s Third Largest Nuclear Force in 1990s\\n\\nThe global threats faced by the ICBMs, strategic bombers, and nuclear warheads that were left in Ukraine when the Soviet Union dissolved in 1991, were eliminated by cooperation between the US, Ukraine, and the Russian Federation – this according to declassified documents recently published by the Archive.\\n\\nThe documents detail the intensive trilateral diplomacy over Ukraine’s nuclear legacy beginning even before December 1991 and describe the vital role played by the Nunn-Lugar initiative. According to Tom Blanton and Svetlana Savranskaya, the posting also “directly addresses current narratives in all three countries that are historically misleading. In the U.S., the impeachment controversy features almost total amnesia about the extraordinary contribution to U.S. national security made by Ukraine’s decision to disarm, removing over 1,900 strategic weapons targeted on the U.S. In Russia, the new nationalist discourse dismisses the Nunn-Lugar cooperative threat reduction as forced disarmament, forgetting that the consolidation of the Soviet nuclear legacy in Russia directly served Russia’s security interests. In Ukraine, nostalgia for nuclear status is on the rise, fueled by the Russian annexation of Crimea and war in Donbas, while ignoring the enormous costs to Ukraine (diplomatic, financial, environmental, and more) had nuclear weapons been retained in the 1990s.”\\n\\nPeter Kornbluh Interviews Chile’s Mónica González\\n\\nThe Archive’s Chile Documentation Project director, Peter Kornbluh, recently interviewed Chilean journalist Mónica González about the ongoing protests in Chile for The Nation. Chileans are protesting economic inequality (the country is one of the 20 most unequal in the world despite its economic prosperity), corruption, and an array of government abuses. Government forces have killed more than 22 people, blinded more than 200 with rubber bullets, injured more than 2,000, and arrested more than 6,000 since the protests began. Read the wide-ranging interview here.\\n\\nISCAP Releases NSSE Order\\n\\nThis week’s last item comes from FRINFORMSUM reader Austin Nolen, who recently received a previously-classified 2007 George W. Bush administration order on reforming National Special Security Event authorities from the Interagency Security Classification Appeals Panel (ISCAP). (NSSEs are events – like the upcoming Republican and Democratic National Convention – that the Department of Homeland Security deems important enough to be a potential target for terrorist or criminal activity.) As Austin notes, “The document references back to Bush HSPD 7, unclassified, which appears to be the first time the DHS Secretary received NSSE designation authority, which had previously been given to the [Attorney General] and Treasury when NSSEs were first created by Clinton in NSC-62.” Thank you very much to Mr. Nolen for making these available to Unredacted and its readers!\\n\\nSign Up\\n\\nWant to stay on top of the latest FOIA news? Click here to sign up for our weekly FRINFORMSUM (Freedom of Information Summary) email newsletter.\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_bpe.decode(bpe_data[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_sample = 512\n",
    "max_tokens = 2048\n",
    "shorten_method = \"none\"\n",
    "shorten_data_split_list = \"\"\n",
    "seed = 0\n",
    "split = \"train\"\n",
    "sample_break_mode = \"complete\"\n",
    "\n",
    "# split_path = os.path.join(os.path.join(lm_dataset, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fairseq.data.shorten_dataset.maybe_shorten_dataset(\n",
    "            dataset,\n",
    "            split,\n",
    "            shorten_data_split_list,\n",
    "            shorten_method,\n",
    "            tokens_per_sample,\n",
    "            seed,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairseq.data.indexed_dataset.MMapIndexedDataset at 0x7f4e5f6036a0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "_dataset = copy.deepcopy(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenBlockDataset(\n",
    "            dataset,\n",
    "            dataset.sizes,\n",
    "            tokens_per_sample - 1,  # one less for <s>\n",
    "            pad=dictionary.pad(),\n",
    "            eos=dictionary.eos(),\n",
    "            break_mode=sample_break_mode,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9381"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dataset[0] == _dataset[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PrependTokenDataset(dataset, dictionary.bos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,  9167, 45781,  1163,    55, 50118, 50118, 32826,    36, 11528])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9167, 45781,  1163,    55, 50118, 50118, 32826,    36, 11528,    43])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dataset[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([849]), torch.Size([850]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dataset[0].size(), dataset[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_bpe_cfg.bpe = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = fairseq.data.encoders.build_bpe(gpt2_bpe_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairseq.data.encoders.gpt2_bpe.GPT2BPE at 0x7f4f8c4cff10>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_whole_words = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_whole_words = fairseq.data.encoders.utils.get_whole_word_mask(gpt2_bpe_cfg, dictionary)\n",
    "no_mask_whole_words = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50264])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_whole_words.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_symbol = \"<mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_symbol in dictionary.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50264"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.add_symbol(mask_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_symbol in dictionary.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50264"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.index(mask_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 1, 1, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_whole_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([    4,     6,    12,  ..., 50256, 50258, 50260]),),\n",
       " torch.Size([17122]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(mask_whole_words != 1), torch.where(mask_whole_words != 1)[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " ',',\n",
       " '-',\n",
       " '�',\n",
       " \"'s\",\n",
       " '�',\n",
       " 's',\n",
       " ':',\n",
       " ')',\n",
       " '�',\n",
       " '�',\n",
       " ',\"',\n",
       " '.\"',\n",
       " '/',\n",
       " \"'t\",\n",
       " 't',\n",
       " 'I',\n",
       " 'a',\n",
       " 'S',\n",
       " 'ë',\n",
       " \"'\",\n",
       " '\"',\n",
       " '?',\n",
       " 'i',\n",
       " 'm',\n",
       " ';',\n",
       " 'The',\n",
       " '1',\n",
       " 'o',\n",
       " '000',\n",
       " 'ing',\n",
       " 'We',\n",
       " 'com',\n",
       " '2',\n",
       " 'in',\n",
       " 'year',\n",
       " 'ed',\n",
       " '%',\n",
       " 'th',\n",
       " \"'re\",\n",
       " 'y',\n",
       " 'en',\n",
       " '),',\n",
       " 're',\n",
       " 'e',\n",
       " 'It',\n",
       " '5',\n",
       " '3',\n",
       " 'A',\n",
       " 'er',\n",
       " 'u',\n",
       " 'an',\n",
       " 'on',\n",
       " 'j',\n",
       " 'ers',\n",
       " 'ar',\n",
       " 'old',\n",
       " 'as',\n",
       " 'n',\n",
       " '0',\n",
       " 'es',\n",
       " 'h',\n",
       " '4',\n",
       " ').',\n",
       " 'ie',\n",
       " '!',\n",
       " 'z',\n",
       " 'k',\n",
       " 'al',\n",
       " 'r',\n",
       " 'C',\n",
       " \"'ve\",\n",
       " 'ly',\n",
       " 'is',\n",
       " 'os',\n",
       " 'or',\n",
       " 'B',\n",
       " 'man',\n",
       " '8',\n",
       " '6',\n",
       " 'it',\n",
       " '7',\n",
       " 'at',\n",
       " 'd',\n",
       " 'am',\n",
       " 'b',\n",
       " \"'m\",\n",
       " 'c',\n",
       " 'M',\n",
       " 'le',\n",
       " 'l',\n",
       " 'and',\n",
       " '9',\n",
       " '.,',\n",
       " 'N',\n",
       " 'ia',\n",
       " 'D',\n",
       " 'R',\n",
       " 'f',\n",
       " 'P',\n",
       " 'el',\n",
       " 'K',\n",
       " 'G',\n",
       " '30',\n",
       " 've',\n",
       " 'to',\n",
       " 'T',\n",
       " 'g',\n",
       " 'L',\n",
       " '—',\n",
       " \"'ll\",\n",
       " 'AP',\n",
       " 'et',\n",
       " 'F',\n",
       " 'w',\n",
       " 'ley',\n",
       " 'ch',\n",
       " '00',\n",
       " 'st',\n",
       " 'ad',\n",
       " 'the',\n",
       " 'ic',\n",
       " '://',\n",
       " 'p',\n",
       " '�',\n",
       " 'up',\n",
       " 'ist',\n",
       " 'O',\n",
       " 'ak',\n",
       " 'us',\n",
       " '10',\n",
       " 'he',\n",
       " 'v',\n",
       " 'ur',\n",
       " '�',\n",
       " 'This',\n",
       " 'E',\n",
       " 'il',\n",
       " 'H',\n",
       " '...',\n",
       " ']',\n",
       " 'im',\n",
       " 'ra',\n",
       " 'W',\n",
       " 'um',\n",
       " 'U',\n",
       " 'based',\n",
       " 'id',\n",
       " 'ian',\n",
       " 'ine',\n",
       " '20',\n",
       " '\".',\n",
       " 'V',\n",
       " 'ir',\n",
       " 'ay',\n",
       " 'ne',\n",
       " 'te',\n",
       " '�',\n",
       " 'J',\n",
       " 'able',\n",
       " 'ab',\n",
       " 'co',\n",
       " 'ate',\n",
       " 'un',\n",
       " 'll',\n",
       " 'He',\n",
       " 'ah',\n",
       " 'ier',\n",
       " 'ies',\n",
       " 'ul',\n",
       " 'ant',\n",
       " '&',\n",
       " \".'\",\n",
       " 'time',\n",
       " 'ik',\n",
       " 'There',\n",
       " 'Y',\n",
       " 'est',\n",
       " 'em',\n",
       " 'ors',\n",
       " 'out',\n",
       " '15',\n",
       " 'X',\n",
       " 'ro',\n",
       " \"'d\",\n",
       " 'io',\n",
       " 'ig',\n",
       " 'ings',\n",
       " '@',\n",
       " 'ac',\n",
       " 'ton',\n",
       " 'ri',\n",
       " 'ated',\n",
       " 'ke',\n",
       " 'ag',\n",
       " 'om',\n",
       " 'se',\n",
       " '12',\n",
       " '50',\n",
       " 'ang',\n",
       " 'If',\n",
       " 'ana',\n",
       " 'ap',\n",
       " 'of',\n",
       " 'ard',\n",
       " 'In',\n",
       " 'é',\n",
       " 'ol',\n",
       " 'ish',\n",
       " '…',\n",
       " 'aj',\n",
       " 'x',\n",
       " 'au',\n",
       " 'ut',\n",
       " 'You',\n",
       " 'nd',\n",
       " 'sh',\n",
       " 'day',\n",
       " 'iz',\n",
       " 'They',\n",
       " '_',\n",
       " 'az',\n",
       " '11',\n",
       " 'ot',\n",
       " 'ina',\n",
       " '25',\n",
       " 'land',\n",
       " 'all',\n",
       " 'Reuters',\n",
       " 'ans',\n",
       " 'ation',\n",
       " 'one',\n",
       " '�',\n",
       " 'term',\n",
       " 'ens',\n",
       " 'ating',\n",
       " '\",',\n",
       " 'Z',\n",
       " 'are',\n",
       " 'led',\n",
       " 'ter',\n",
       " 'ent',\n",
       " 'q',\n",
       " 'ins',\n",
       " 'uk',\n",
       " 'ville',\n",
       " '17',\n",
       " '18',\n",
       " 'ler',\n",
       " '�',\n",
       " 'ner',\n",
       " 'end',\n",
       " 'field',\n",
       " 'www',\n",
       " 'by',\n",
       " 'ov',\n",
       " 'ou',\n",
       " 'ai',\n",
       " 'ib',\n",
       " 'ite',\n",
       " 'av',\n",
       " 'son',\n",
       " 'ó',\n",
       " 'ach',\n",
       " 'ise',\n",
       " '500',\n",
       " 'ion',\n",
       " 'ry',\n",
       " 'ari',\n",
       " 'op',\n",
       " 'á',\n",
       " 'ling',\n",
       " 'off',\n",
       " 'res',\n",
       " 'als',\n",
       " 'ized',\n",
       " 'ani',\n",
       " 'ung',\n",
       " '16',\n",
       " 'twitter',\n",
       " '13',\n",
       " 'ä',\n",
       " '14',\n",
       " 'ity',\n",
       " 'age',\n",
       " 'aw',\n",
       " 'ip',\n",
       " '.)',\n",
       " 'if',\n",
       " \",'\",\n",
       " 'be',\n",
       " 'As',\n",
       " 'ates',\n",
       " 'ale',\n",
       " '$',\n",
       " 'od',\n",
       " 'les',\n",
       " 'ations',\n",
       " 'ok',\n",
       " '(',\n",
       " 'ell',\n",
       " 'back',\n",
       " '19',\n",
       " 'ong',\n",
       " 'Reporting',\n",
       " 'ash',\n",
       " 'less',\n",
       " 'pm',\n",
       " 'ore',\n",
       " 'ED',\n",
       " 'we',\n",
       " 'ino',\n",
       " 'But',\n",
       " 'That',\n",
       " 'ised',\n",
       " 'ow',\n",
       " 'ES',\n",
       " 'ich',\n",
       " 'ane',\n",
       " 'IS',\n",
       " 'per',\n",
       " 'ara',\n",
       " '40',\n",
       " 'ce',\n",
       " 'ment',\n",
       " 'ick',\n",
       " 'ary',\n",
       " 'When',\n",
       " 'ue',\n",
       " 'ons',\n",
       " 'ub',\n",
       " 'me',\n",
       " 'ism',\n",
       " 'ness',\n",
       " 'ous',\n",
       " 'ade',\n",
       " 'her',\n",
       " 'wood',\n",
       " 'ile',\n",
       " 'ain',\n",
       " 'ING',\n",
       " 'Q',\n",
       " '100',\n",
       " 'ts',\n",
       " 'ill',\n",
       " 'iv',\n",
       " 'ire',\n",
       " 'AN',\n",
       " 'ford',\n",
       " '45',\n",
       " 'ge',\n",
       " 'line',\n",
       " 'ud',\n",
       " 'ham',\n",
       " 'ja',\n",
       " 'ma',\n",
       " '?\"',\n",
       " '23',\n",
       " 'ization',\n",
       " 'ee',\n",
       " 'ber',\n",
       " 'ide',\n",
       " 'ek',\n",
       " 'ists',\n",
       " 'org',\n",
       " 'way',\n",
       " 'oc',\n",
       " 'í',\n",
       " '24',\n",
       " 'ast',\n",
       " 'for',\n",
       " 'ha',\n",
       " 'af',\n",
       " 'art',\n",
       " '35',\n",
       " '�',\n",
       " 'ind',\n",
       " '22',\n",
       " 'red',\n",
       " 'IT',\n",
       " 'ru',\n",
       " 'ians',\n",
       " 'ize',\n",
       " 'ER',\n",
       " 'ive',\n",
       " 'mer',\n",
       " 'ada',\n",
       " 'ado',\n",
       " 'AL',\n",
       " 'our',\n",
       " 'na',\n",
       " 'over',\n",
       " '21',\n",
       " 'month',\n",
       " '%.',\n",
       " 'og',\n",
       " 'ze',\n",
       " 'oy',\n",
       " 'ij',\n",
       " 'ows',\n",
       " 'ata',\n",
       " 'ON',\n",
       " 'ities',\n",
       " 'qu',\n",
       " 'ks',\n",
       " 'men',\n",
       " 'What',\n",
       " 'ann',\n",
       " 'point',\n",
       " 'ala',\n",
       " 'AS',\n",
       " 'ka',\n",
       " 'je',\n",
       " 'no',\n",
       " 'ally',\n",
       " 'IC',\n",
       " 'pe',\n",
       " '–',\n",
       " 'My',\n",
       " 'ance',\n",
       " 'ert',\n",
       " 'ass',\n",
       " 'ger',\n",
       " 'ure',\n",
       " 'And',\n",
       " 'Photo',\n",
       " 'ob',\n",
       " 'ck',\n",
       " 'ica',\n",
       " 'IN',\n",
       " 'air',\n",
       " 'hen',\n",
       " 'ys',\n",
       " 'ep',\n",
       " 'ice',\n",
       " '2018',\n",
       " '60',\n",
       " '�',\n",
       " 'za',\n",
       " '26',\n",
       " 'ions',\n",
       " 'UR',\n",
       " 'ien',\n",
       " 'els',\n",
       " 'She',\n",
       " '28',\n",
       " '27',\n",
       " 'ci',\n",
       " 'Our',\n",
       " 'so',\n",
       " 'ini',\n",
       " 'ont',\n",
       " 'Â',\n",
       " 'ga',\n",
       " 'ft',\n",
       " 'int',\n",
       " '75',\n",
       " '05',\n",
       " 'AY',\n",
       " 'ied',\n",
       " 'ene',\n",
       " 'ty',\n",
       " 'NYSE',\n",
       " 'ren',\n",
       " 'la',\n",
       " 'AC',\n",
       " 'AT',\n",
       " 'ting',\n",
       " 'ets',\n",
       " 'itt',\n",
       " 'ama',\n",
       " 'rd',\n",
       " 'ng',\n",
       " 'ney',\n",
       " 'ano',\n",
       " 'ost',\n",
       " 'AD',\n",
       " 'att',\n",
       " 'lin',\n",
       " 'ial',\n",
       " '200',\n",
       " 'AM',\n",
       " 'its',\n",
       " 'ator',\n",
       " 'ya',\n",
       " 'ett',\n",
       " 'ful',\n",
       " \"'.\",\n",
       " '01',\n",
       " 'game',\n",
       " 'yard',\n",
       " 'oh',\n",
       " 'ath',\n",
       " 'ico',\n",
       " 'TS',\n",
       " 'ID',\n",
       " 'ters',\n",
       " 'For',\n",
       " 'let',\n",
       " 'ort',\n",
       " 'ars',\n",
       " 'ón',\n",
       " 'wa',\n",
       " '+',\n",
       " 'AR',\n",
       " 'ew',\n",
       " 'By',\n",
       " 'ü',\n",
       " 'DAY',\n",
       " 'izing',\n",
       " 'de',\n",
       " 'EN',\n",
       " 'ver',\n",
       " 'ere',\n",
       " '99',\n",
       " 'ond',\n",
       " 'iting',\n",
       " 'ron',\n",
       " '�',\n",
       " 'So',\n",
       " 'ny',\n",
       " 'ics',\n",
       " 'berg',\n",
       " 'che',\n",
       " '32',\n",
       " 'IA',\n",
       " '29',\n",
       " 'Most',\n",
       " '!\"',\n",
       " 'pl',\n",
       " 'TV',\n",
       " 'foot',\n",
       " 'ats',\n",
       " 'news',\n",
       " '80',\n",
       " 'uf',\n",
       " 'uh',\n",
       " 'ering',\n",
       " 'run',\n",
       " 'ments',\n",
       " '300',\n",
       " 'go',\n",
       " 'ey',\n",
       " '31',\n",
       " 'ven',\n",
       " 'market',\n",
       " 'ack',\n",
       " 'iss',\n",
       " 'ising',\n",
       " 'ola',\n",
       " 'oo',\n",
       " 'US',\n",
       " 'well',\n",
       " 'IL',\n",
       " '�',\n",
       " '34',\n",
       " '70',\n",
       " 'No',\n",
       " '33',\n",
       " 'ord',\n",
       " '55',\n",
       " 'ita',\n",
       " 'ster',\n",
       " 'DA',\n",
       " 'house',\n",
       " 'ines',\n",
       " 'ki',\n",
       " 'ff',\n",
       " 'amb',\n",
       " 'ank',\n",
       " '38',\n",
       " 'ase',\n",
       " 'ba',\n",
       " 'ix',\n",
       " 'und',\n",
       " 'OS',\n",
       " 'ec',\n",
       " 'eb',\n",
       " 'ered',\n",
       " 'DAQ',\n",
       " '*',\n",
       " 'itch',\n",
       " 'ca',\n",
       " '90',\n",
       " 'ug',\n",
       " '):',\n",
       " 'isation',\n",
       " '37',\n",
       " 'ps',\n",
       " 'ants',\n",
       " 'ern',\n",
       " 'anc',\n",
       " 'OT',\n",
       " 'BS',\n",
       " '44',\n",
       " 'eng',\n",
       " 'pt',\n",
       " 'burg',\n",
       " 'app',\n",
       " 'like',\n",
       " 'ock',\n",
       " 'own',\n",
       " '.-',\n",
       " 'ess',\n",
       " '36',\n",
       " 'related',\n",
       " 'ui',\n",
       " 'CC',\n",
       " 'ç',\n",
       " 'OL',\n",
       " '04',\n",
       " 'SE',\n",
       " 'ette',\n",
       " 'NAS',\n",
       " 'acks',\n",
       " 'que',\n",
       " 'OR',\n",
       " '49',\n",
       " '39',\n",
       " 'port',\n",
       " 'round',\n",
       " 'ith',\n",
       " 'ically',\n",
       " 'PA',\n",
       " 'ages',\n",
       " 'AG',\n",
       " 'ex',\n",
       " 'long',\n",
       " 'gan',\n",
       " '65',\n",
       " 'ning',\n",
       " 'ball',\n",
       " 'high',\n",
       " 'oe',\n",
       " '09',\n",
       " 'jo',\n",
       " 'ical',\n",
       " '07',\n",
       " 'li',\n",
       " 'BC',\n",
       " 'ira',\n",
       " 'week',\n",
       " 'EC',\n",
       " 'hi',\n",
       " '�',\n",
       " 'SA',\n",
       " 'Sh',\n",
       " 'ev',\n",
       " 'der',\n",
       " 'head',\n",
       " 'ators',\n",
       " 'ax',\n",
       " 'SC',\n",
       " 'rie',\n",
       " 'star',\n",
       " 'ali',\n",
       " 'ati',\n",
       " 'not',\n",
       " 'bl',\n",
       " 'ues',\n",
       " '08',\n",
       " 'ö',\n",
       " 'ves',\n",
       " 'All',\n",
       " 'looking',\n",
       " 'ative',\n",
       " 'â',\n",
       " 'vers',\n",
       " 'use',\n",
       " 'ives',\n",
       " '47',\n",
       " 'aff',\n",
       " '42',\n",
       " 'EL',\n",
       " '�',\n",
       " 'side',\n",
       " 'TH',\n",
       " 'eg',\n",
       " 'free',\n",
       " 'buy',\n",
       " 'At',\n",
       " 'IM',\n",
       " 'ail',\n",
       " '46',\n",
       " 'One',\n",
       " 'CH',\n",
       " 'OM',\n",
       " 'any',\n",
       " 'ified',\n",
       " '2017',\n",
       " 'ph',\n",
       " 'inch',\n",
       " 'ched',\n",
       " 'pp',\n",
       " 'IP',\n",
       " 'br',\n",
       " 'ush',\n",
       " 'BA',\n",
       " '48',\n",
       " 'era',\n",
       " 'TA',\n",
       " '�',\n",
       " 'LE',\n",
       " 'ura',\n",
       " 'ko',\n",
       " 'ao',\n",
       " 'oon',\n",
       " 'con',\n",
       " 'sc',\n",
       " 'ten',\n",
       " 'ose',\n",
       " 'PS',\n",
       " 'yn',\n",
       " 'ct',\n",
       " '43',\n",
       " 'den',\n",
       " 'form',\n",
       " 'ova',\n",
       " 'With',\n",
       " '800',\n",
       " 'amp',\n",
       " 'mar',\n",
       " 'ran',\n",
       " 'ging',\n",
       " '03',\n",
       " \"',\",\n",
       " 'ET',\n",
       " 'oll',\n",
       " 'ato',\n",
       " 'va',\n",
       " 'down',\n",
       " 'rit',\n",
       " 'uc',\n",
       " 'alt',\n",
       " 'To',\n",
       " 'inc',\n",
       " 'oz',\n",
       " 'bo',\n",
       " 'ros',\n",
       " 'ark',\n",
       " 'ored',\n",
       " 'cl',\n",
       " 'nt',\n",
       " 'ih',\n",
       " '41',\n",
       " 'ST',\n",
       " '95',\n",
       " '400',\n",
       " '64',\n",
       " 'New',\n",
       " 'ium',\n",
       " 'ram',\n",
       " 'Getty',\n",
       " 'ying',\n",
       " 'CA',\n",
       " '�',\n",
       " 'anna',\n",
       " 'ison',\n",
       " 'ille',\n",
       " 'ric',\n",
       " 'right',\n",
       " 'ured',\n",
       " 'lo',\n",
       " 'ence',\n",
       " 'bound',\n",
       " 'ave',\n",
       " 'ole',\n",
       " '67',\n",
       " 'ays',\n",
       " 'ky',\n",
       " 'ures',\n",
       " '06',\n",
       " 'hip',\n",
       " 'ii',\n",
       " 'han',\n",
       " 'net',\n",
       " 'gen',\n",
       " 'gh',\n",
       " 'On',\n",
       " 'ika',\n",
       " 'UN',\n",
       " 'called',\n",
       " '59',\n",
       " 'season',\n",
       " 'GA',\n",
       " 'sey',\n",
       " 'umb',\n",
       " 'sp',\n",
       " 'hed',\n",
       " 'sw',\n",
       " 'SP',\n",
       " 'ents',\n",
       " 'ust',\n",
       " '02',\n",
       " 'oj',\n",
       " 'hold',\n",
       " 'ão',\n",
       " 'ades',\n",
       " 'aker',\n",
       " 'ening',\n",
       " 'bury',\n",
       " '%,',\n",
       " 'ime',\n",
       " 'Star',\n",
       " 'ena',\n",
       " 'orn',\n",
       " 'kin',\n",
       " 'aut',\n",
       " 'ima',\n",
       " 'ery',\n",
       " 'uch',\n",
       " 'erson',\n",
       " '66',\n",
       " '54',\n",
       " 'ink',\n",
       " 'room',\n",
       " 'but',\n",
       " 'weight',\n",
       " 'och',\n",
       " 'leg',\n",
       " 'American',\n",
       " 'bs',\n",
       " 'more',\n",
       " 'ERS',\n",
       " 'ua',\n",
       " 'ox',\n",
       " 'tr',\n",
       " 'ora',\n",
       " '\",\"',\n",
       " '�',\n",
       " 'ms',\n",
       " 'ame',\n",
       " 'ending',\n",
       " 'ida',\n",
       " 'ike',\n",
       " 'ta',\n",
       " 'ear',\n",
       " 'yl',\n",
       " 'ides',\n",
       " 'GB',\n",
       " 'ology',\n",
       " 'away',\n",
       " '57',\n",
       " 'hel',\n",
       " '�',\n",
       " 'RA',\n",
       " ');',\n",
       " 'ome',\n",
       " 'ory',\n",
       " 'quarter',\n",
       " 'million',\n",
       " '56',\n",
       " 'arn',\n",
       " '52',\n",
       " '58',\n",
       " 'une',\n",
       " 'NA',\n",
       " 'owned',\n",
       " 'ace',\n",
       " 'PR',\n",
       " 'ries',\n",
       " 'ged',\n",
       " 'ute',\n",
       " 'ese',\n",
       " 'cy',\n",
       " 'isc',\n",
       " 'level',\n",
       " 'ability',\n",
       " 'ona',\n",
       " 'ups',\n",
       " 'ened',\n",
       " 'itz',\n",
       " 'yr',\n",
       " 'ring',\n",
       " 'hour',\n",
       " 'outs',\n",
       " 'ONDON',\n",
       " 'arm',\n",
       " 'These',\n",
       " 'minute',\n",
       " '85',\n",
       " 'hand',\n",
       " '53',\n",
       " 'ios',\n",
       " 'AB',\n",
       " 'ef',\n",
       " 'ited',\n",
       " '69',\n",
       " 'med',\n",
       " 'OC',\n",
       " 'ales',\n",
       " 'OW',\n",
       " 'ify',\n",
       " 'row',\n",
       " 'mann',\n",
       " 'oor',\n",
       " 'het',\n",
       " 'aged',\n",
       " 'RE',\n",
       " 'ode',\n",
       " 'ces',\n",
       " 'new',\n",
       " '88',\n",
       " 'š',\n",
       " 'law',\n",
       " 'ris',\n",
       " 'urs',\n",
       " 'stone',\n",
       " '68',\n",
       " 'class',\n",
       " 'An',\n",
       " 'min',\n",
       " '600',\n",
       " 'illa',\n",
       " '51',\n",
       " 'iro',\n",
       " 'ili',\n",
       " 'ye',\n",
       " '77',\n",
       " '�',\n",
       " 'iers',\n",
       " 'enn',\n",
       " 'ably',\n",
       " 'gy',\n",
       " 'Part',\n",
       " 'ister',\n",
       " 'ible',\n",
       " 'iles',\n",
       " 'har',\n",
       " 'People',\n",
       " 'ores',\n",
       " 'Ch',\n",
       " 'eth',\n",
       " 'ld',\n",
       " 'uss',\n",
       " 'my',\n",
       " 'ae',\n",
       " 'PC',\n",
       " '\":\"',\n",
       " 'ć',\n",
       " 'half',\n",
       " 'iff',\n",
       " 'fl',\n",
       " 'ovic',\n",
       " 'ito',\n",
       " 'ient',\n",
       " 'pr',\n",
       " 'ila',\n",
       " 'esh',\n",
       " 'pro',\n",
       " 'ough',\n",
       " 'state',\n",
       " 'bar',\n",
       " 'ret',\n",
       " 'iy',\n",
       " 'add',\n",
       " 'EST',\n",
       " 'rs',\n",
       " 'board',\n",
       " 'ign',\n",
       " 'iti',\n",
       " 'ifying',\n",
       " 'reg',\n",
       " 'č',\n",
       " '72',\n",
       " '�',\n",
       " 'ask',\n",
       " 'nes',\n",
       " 'oma',\n",
       " 'uz',\n",
       " 'una',\n",
       " 'mo',\n",
       " 'After',\n",
       " 'ification',\n",
       " 'ias',\n",
       " 'aci',\n",
       " 'do',\n",
       " 'sts',\n",
       " 'ray',\n",
       " 'ull',\n",
       " 'ker',\n",
       " 'ational',\n",
       " 'uma',\n",
       " 'hr',\n",
       " '89',\n",
       " 'NG',\n",
       " 'OU',\n",
       " 'hes',\n",
       " '76',\n",
       " 'ains',\n",
       " 'acy',\n",
       " 'ras',\n",
       " 'abe',\n",
       " 'under',\n",
       " 'ops',\n",
       " 'Me',\n",
       " 'iner',\n",
       " 'lla',\n",
       " 'ni',\n",
       " 'ater',\n",
       " ...]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [i.item() for i in torch.where(mask_whole_words != 1)[0]]\n",
    "\n",
    "[bpe.decode(dictionary[i]) for i in torch.where(mask_whole_words != 1)[0][:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_whole_words[torch.where(mask_whole_words != 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_idx = dictionary.index(mask_symbol)\n",
    "seed = 1\n",
    "mask_prob = 0.15 # probability of replacing a token with mask\n",
    "leave_unmasked_prob = 0.1 # probability that a masked token is unmasked\n",
    "random_token_prob = 0.1 # probability of replacing a token with a random token\n",
    "freq_weighted_replacement = False\n",
    "mask_multiple_length = 1\n",
    "mask_stdev = 0.0\n",
    "skip_masking = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dataset, tgt_dataset = MaskTokensDataset.apply_mask(\n",
    "            dataset,\n",
    "            dictionary,\n",
    "            pad_idx=dictionary.pad(),\n",
    "            mask_idx= mask_idx,\n",
    "            seed=seed,\n",
    "            mask_prob=mask_prob,\n",
    "            leave_unmasked_prob=leave_unmasked_prob,\n",
    "            random_token_prob=random_token_prob,\n",
    "            freq_weighted_replacement=freq_weighted_replacement,\n",
    "            mask_whole_words=mask_whole_words,\n",
    "            mask_multiple_length=mask_multiple_length,\n",
    "            mask_stdev=mask_stdev,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairseq.data.lru_cache_dataset.LRUCacheDataset at 0x7f4e5f5f9280>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fairseq.data.lru_cache_dataset.LRUCacheDataset at 0x7f4e6fc8b280>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([850])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dataset[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([850])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_dataset[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 1), dtype=torch.int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argwhere(dataset[0] == mask_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  4,   5,   6,   7,  38,  46,  50,  51,  52,  53,  54,  59,  95, 110,\n",
       "        131, 153, 161, 167, 170, 176, 177, 181, 201, 208, 216, 217, 218, 219,\n",
       "        220, 221, 243, 246, 247, 281, 282, 283, 286, 295, 296, 297, 298, 299,\n",
       "        300, 302, 303, 306, 311, 314, 315, 328, 330, 339, 347, 383, 438, 443,\n",
       "        445, 467, 484, 495, 496, 522, 523, 531, 540, 542, 545, 546, 557, 562,\n",
       "        576, 582, 590, 603, 611, 619, 636, 644, 645, 646, 664, 677, 678, 682,\n",
       "        688, 692, 699, 702, 703, 704, 705, 706, 724, 725, 741, 759, 788, 793,\n",
       "        796, 826])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argwhere(src_dataset[0] == mask_idx).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(index):\n",
    "    np.random.seed(seed)\n",
    "    item = dataset[index]\n",
    "    sz = len(item)\n",
    "    assert (\n",
    "        mask_idx not in item\n",
    "    ), \"Dataset contains mask_idx (={}), this is not expected!\".format(\n",
    "        mask_idx,\n",
    "    )\n",
    "\n",
    "    if mask_whole_words is not None:\n",
    "        word_begins_mask = mask_whole_words.gather(0, item)\n",
    "        word_begins_idx = word_begins_mask.nonzero().view(-1)\n",
    "        sz = len(word_begins_idx)\n",
    "        words = np.split(word_begins_mask, word_begins_idx)[1:]\n",
    "        assert len(words) == sz\n",
    "        word_lens = list(map(len, words))\n",
    "\n",
    "    # decide elements to mask\n",
    "    mask = np.full(sz, False)\n",
    "    num_mask = int(\n",
    "        # add a random number for probabilistic rounding\n",
    "        mask_prob * sz / float(mask_multiple_length)\n",
    "        + np.random.rand()\n",
    "    )\n",
    "\n",
    "    # multiple masking as described in the vq-wav2vec paper (https://arxiv.org/abs/1910.05453)\n",
    "    mask_idc = np.random.choice(sz, num_mask, replace=False)\n",
    "    if mask_stdev > 0.0:\n",
    "        lengths = np.random.normal(\n",
    "            mask_multiple_length, mask_stdev, size=num_mask\n",
    "        )\n",
    "        lengths = [max(0, int(round(x))) for x in lengths]\n",
    "        mask_idc = np.asarray(\n",
    "            [\n",
    "                mask_idc[j] + offset\n",
    "                for j in range(len(mask_idc))\n",
    "                for offset in range(lengths[j])\n",
    "            ],\n",
    "            dtype=np.int64,\n",
    "        )\n",
    "    else:\n",
    "        mask_idc = np.concatenate(\n",
    "            [mask_idc + i for i in range(mask_multiple_length)]\n",
    "        )\n",
    "    mask_idc = mask_idc[mask_idc < len(mask)]\n",
    "    try:\n",
    "        mask[mask_idc] = True\n",
    "    except:  # something wrong\n",
    "        print(\n",
    "            \"Assigning mask indexes {} to mask {} failed!\".format(\n",
    "                mask_idc, mask\n",
    "            )\n",
    "        )\n",
    "        raise\n",
    "    # decide unmasking and random replacement\n",
    "    rand_or_unmask_prob = random_token_prob + leave_unmasked_prob\n",
    "    if rand_or_unmask_prob > 0.0:\n",
    "        rand_or_unmask = mask & (np.random.rand(sz) < rand_or_unmask_prob)\n",
    "        if random_token_prob == 0.0:\n",
    "            unmask = rand_or_unmask\n",
    "            rand_mask = None\n",
    "        elif leave_unmasked_prob == 0.0:\n",
    "            unmask = None\n",
    "            rand_mask = rand_or_unmask\n",
    "        else:\n",
    "            unmask_prob = leave_unmasked_prob / rand_or_unmask_prob\n",
    "            decision = np.random.rand(sz) < unmask_prob\n",
    "            unmask = rand_or_unmask & decision\n",
    "            rand_mask = rand_or_unmask & (~decision)\n",
    "    else:\n",
    "        unmask = rand_mask = None\n",
    "\n",
    "    if unmask is not None:\n",
    "        mask = mask ^ unmask\n",
    "    print(mask.shape, len(word_lens))\n",
    "    if mask_whole_words is not None:\n",
    "        mask = np.repeat(mask, word_lens)\n",
    "    new_item = np.copy(item)\n",
    "    new_item[mask] = mask_idx\n",
    "    if rand_mask is not None:\n",
    "        if freq_weighted_replacement:\n",
    "            weights = np.array(dictionary.count)\n",
    "        else:\n",
    "            weights = np.ones(len(dictionary))\n",
    "            \n",
    "        weights[:dictionary.nspecial] = 0\n",
    "        weights = weights / weights.sum()\n",
    "        num_rand = rand_mask.sum()\n",
    "        if num_rand > 0:\n",
    "            if mask_whole_words is not None:\n",
    "                rand_mask = np.repeat(rand_mask, word_lens)\n",
    "                num_rand = rand_mask.sum()\n",
    "\n",
    "            new_item[rand_mask] = np.random.choice(\n",
    "                len(dictionary),\n",
    "                num_rand,\n",
    "                p=weights,\n",
    "            )\n",
    "\n",
    "    return torch.from_numpy(new_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(579,) 579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([50264, 50264, 50264,  1163,    55, 50118, 50118, 32826, 50264, 50264,\n",
       "        50264, 50264, 50264, 50264,    87,   545,     6,   151,  2694,   337,\n",
       "         8111,  3451, 12827, 41649,     5,  7183,  2592,   444,    55,  8422,\n",
       "         3369, 14328, 50264,  2310,   514,     6,   309,     7,    10,    78,\n",
       "          720,  4990,     9,     5,  1293,    18, 50264,  3844,     6,  1027,\n",
       "          302,     4, 50118, 50118,  2709,   358,  6474,   241,     9,  2310,\n",
       "          514, 27380,    31, 50264, 50264,    50,  5378,  2990,  1173, 24416,\n",
       "            6,    10,  6474,   241,    12,   463,    12,   102,    12,  4809,\n",
       "        50264, 31924, 50264, 50264, 50264,   373,  5378,   833,     6,    16,\n",
       "        12961,  2024,   124,    88,     5,  6444,    50,     5,  1255,     4,\n",
       "        50118, 50118,   133,  2422,    12,    29, 12107,  6572,    16, 50264,\n",
       "        50264,    55,  8422, 50264,     5,  8321,   341,    11,     5,  2694,\n",
       "          337,  8111,   609,     6,  2634,   431,    11,     5,  8812,  4662,\n",
       "            9,     5, 50264,  9356,     4, 50118, 50118,  8739,  5961,     8,\n",
       "        32457,     6,    13, 50264, 50264,    32,   258, 10266,   341,     4,\n",
       "        50118, 50118,   133,  1280,     9,  5378,   833,  2622,  3612,   358,\n",
       "           76,   480,    55,    87,   654,   325, 15768,  7472,   480,    16,\n",
       "         3804,     7,  1719,     5,   194,     9,  1261,     6,    50, 50264,\n",
       "        50264,  5295,  2771,     6,    11,    10,   389,    12,  6342, 12965,\n",
       "          241,    36,  1264,    12,  2917,    43, 10490,     9, 31924, 40684,\n",
       "            6,    51,  9658,     4, 50118, 50118,   113,   133,   232,  9108,\n",
       "          540,  2694,   337,  9339,   514,    87,  5378,   833,    60,  1029,\n",
       "           12, 11515, 34608,  4623,  1209,   625,   853,     6,    10,  9744,\n",
       "           23,     5, 50264,    13,  3201,     6,  9356,     8,  1309,    23,\n",
       "          315,  3076,   589, 15788,  4170,     6,   896,     6,   174, 50264,\n",
       "        50264, 50264, 50264, 50264, 50264, 50264,     5,  5378,   833,  1411,\n",
       "          124,    88,     5,  1737,     6,  2260,    11, 50264, 50264, 50264,\n",
       "        50264, 50264, 50264,    14,  1823,  6740,  7700,     5,  5181, 50264,\n",
       "         8095,  5794,     6,     8, 27430,     5,   672,     9, 11747,     6,\n",
       "        50264,    64,  1045,  1695, 24235,  8905,   845, 50118, 50118,   113,\n",
       "          243,    16, 50264,    13, 31092, 28340,     7, 14575, 20050,   209,\n",
       "         1274,   480,    51,   240,   384,   176,     7,  6008,    60,    26,\n",
       "         1209,   625,   853,     4, 50118, 50118,  9690,    87,   457, 50264,\n",
       "            5,  5378,   833,   606,    31,   129,   237,   749,    35,  2030,\n",
       "        50264, 50264, 50264,   135,   238,   315,  4681,  8313,    36,   844,\n",
       "            4,   176,   135,   238, 13857,    36,   401, 50264, 50264,     8,\n",
       "         5978,    36,   245,     4,   398,   135,   322, 50118, 50118, 11073,\n",
       "         1327,     6,     5,  2367,   953,     6, 50264,   514,    12,  3641,\n",
       "         5202, 50264,  2946, 50264,    11,     5, 50264,     8,  5140,    67,\n",
       "         5864,  4008,    15,  2694,   337,  8111, 50264,   694,  1522,  4835,\n",
       "        50264, 50264,    61,  2349,    13,   823,    80,    12, 10224,     9,\n",
       "        50264, 50264, 50264, 50264, 50264,  1079,    16,   341,    11, 50264,\n",
       "        50264,    25,    10,  3035,   927,    11,  1007,   931,     6,     8,\n",
       "           11,  6300,     4, 50118, 50118, 34623, 50264, 34190,   237, 50264,\n",
       "          697,    11, 50264,   147, 50264,  1915,  8790, 16429,   148,   233,\n",
       "            9,     5,    76,     6,     8,   457,    12,   102,    12,  9026,\n",
       "          676,   514, 28227,    76, 50264, 50264,   309,     7,     5,   315,\n",
       "         3076,     4, 50118, 50118,    12,  3201, 28227,   111, 50118, 50118,\n",
       "        11321,   570,     6,     5,   623,  4713,  7665,    18,  1013,  1849,\n",
       "        12366, 50264,    34,  6566,  4173,    22,  5412, 16645,   113,    25,\n",
       "          566,     5,   720,  3455,   480,  1065,  1632, 12884,     6,  2862,\n",
       "         8702,     8,  5381,    12, 25764,     4, 50118, 50118, 25589, 28227,\n",
       "           16, 50264,    30, 50264,   383,     6,  1158,    19,    10, 50264,\n",
       "         1956,  3172,    11,    15,   799,   325,     4, 50118, 50118, 37038,\n",
       "        12323,   117,  1181,  1338,     5,  3342,     6, 16690,  1594,   268,\n",
       "           32,   145, 28635, 50264, 50264,     8,  6631,    16,   326,  1851,\n",
       "         2577,   514, 50264,  1255, 50264,   874,     4, 50118, 50118,  3908,\n",
       "        50264,   464,     6,     5,  1068,    40,   120,  3007,     4, 50118,\n",
       "        50118,  2709,   349,  3093,     9,   720,  8232,     6,    59,   707,\n",
       "        50264,     9, 50264,   232,    18,  1956,   480,   457,    12,   102,\n",
       "           12,  9026,    82,   480,    40, 50264,   291,   135,   540, 50264,\n",
       "        50264,   309,     7,     5, 50264, 50264, 19413,    15, 11001,  7229,\n",
       "           36,  3808,  3376,   322, 50118, 50118,   113, 28324,   337,  8111,\n",
       "          806, 50264, 11916,    10,   739,   346, 50264,    82,    60,    26,\n",
       "         1209,   625,   853,     4,    22,  1708,    52,  1395, 50264,     5,\n",
       "          931,     9,  5378,   833,     6,    61,    16,   164,     7,   555,\n",
       "           41, 50264,  2388,   936,    11,     5,   499,    72, 50118, 50118,\n",
       "        36926, 13700,    12,  8056,   806,    13,  8201,  6740,    31, 50264,\n",
       "           34,    57,   198, 50264,     5,  7571,    29,     4,   870,  4525,\n",
       "            6,    89,    58,   416,   155,     6,   151,  3451,    11,  2513,\n",
       "        50264,     5,  7183,     4, 50118, 50118,  4148,   595,  3926,     6,\n",
       "            5,  1293,    40,   192, 50264,   746,     9,    23,   513,   601,\n",
       "            6,  1497,  3451,    30, 50264, 50264,  1209,   625,   853,    26,\n",
       "            6,  5196,    14,    65,   739,  2195,    64,  2592,    25,   203,\n",
       "         2310,   514,   480,     8,  5378,   833,   480,    25,  1878,    50,\n",
       "         2993,   650,  1980,     4, 50118, 50118,  9690,    87,  1814,   135,\n",
       "            9, 50264, 50264, 50264,  3451, 50264,    11,  8581,  6795,     4,\n",
       "          152, 50264,     5,   754, 50264, 50264, 50264,  1189,  3214,     6,\n",
       "        23729, 44759,  1007, 50264, 50264, 50264, 50264, 50264, 50264,    67,\n",
       "          839,    14,  4066,  3949,    33,     5,  2148,     7,  2179,  1319,\n",
       "            7, 26178,     9,  8422,  5378,   833,    14,    32,   540, 11190,\n",
       "            7,  6444,     8,  1212, 11534,     6,    37,   355,     4, 50118,\n",
       "        50118,  6323,  4792,  1377,    33,   190,  2343,    14, 10639, 50264,\n",
       "        50264, 50264,  2501,  5167,     9,  1402,  3539,  4707,    11, 16690,\n",
       "         1043, 17898,     4, 50118, 50118,   116, 50264,  5040, 50118,     2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_fn(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_target_tokens = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = RightPadDataset(\n",
    "            tgt_dataset,\n",
    "            pad_idx=dictionary.pad(),\n",
    "        )\n",
    "\n",
    "input_dict = {\n",
    "    \"src_tokens\": RightPadDataset(\n",
    "        src_dataset,\n",
    "        pad_idx=dictionary.pad(),\n",
    "    ),\n",
    "    \"src_lengths\": NumelDataset(src_dataset, reduce=False),\n",
    "}\n",
    "if include_target_tokens:\n",
    "    input_dict[\"target_tokens\"] = target_dataset\n",
    "np.random.seed(seed)\n",
    "shuffle = np.random.permutation(len(src_dataset))\n",
    "dataset = SortDataset(\n",
    "    NestedDictionaryDataset(\n",
    "        {\n",
    "            \"id\": IdDataset(),\n",
    "            \"net_input\": input_dict,\n",
    "            \"target\": target_dataset,\n",
    "            \"nsentences\": NumSamplesDataset(),\n",
    "            \"ntokens\": NumelDataset(src_dataset, reduce=True),\n",
    "        },\n",
    "        sizes=[src_dataset.sizes],\n",
    "    ),\n",
    "    sort_order=[\n",
    "        shuffle,\n",
    "        src_dataset.sizes,\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 850,  752,  465, ...,  685, 3771,   80], dtype=uint16)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('id', 1),\n",
       "             ('net_input.src_tokens',\n",
       "              tensor([    0,  5320,  1851,  1672,  8114, 27773,   154, 50264, 50264, 50264,\n",
       "                      50264, 50264, 50264, 50264, 27773,   154,   208,  7396, 50118, 50118,\n",
       "                        246, 10468, 23486,  3689, 11397,   654,  5471, 23486,  3689, 16875,\n",
       "                         73, 30131, 50264, 23486,  3689, 40924,   820,  5471, 23486,  3689,\n",
       "                      16875,    73, 30131,   112, 10468, 50264,  3689, 40924,   564,  5471,\n",
       "                      23486,  3689, 50264, 50264, 50264,   132, 10468, 23486,  3689, 11397,\n",
       "                        158,  5471, 23486,  3689, 16875,    73, 30131, 50264, 10468, 23486,\n",
       "                       3689, 11397,   654,  5471, 23486,  3689, 16875,    73, 30131,   112,\n",
       "                      10468, 23486,  3689, 11397,   971,  5471, 23486,  3689, 16875,    73,\n",
       "                      30131,   204, 23486,  3689, 40924,  2107,  5471, 23486, 50264, 16875,\n",
       "                         73, 30131,   132, 10468, 23486,  3689, 50264,   727,  5471, 23486,\n",
       "                       3689, 16875,    73, 30131,   112,     4,   245, 10468, 23486,  3689,\n",
       "                      11397, 20065,  7097, 23486,  3689, 16875,    73, 30131,   112,     4,\n",
       "                        245, 23486,  3689, 40924,   971,  5471,  7452, 23486,  3689, 16875,\n",
       "                         73, 30131,   204, 10468, 23486,  3689, 40924,   727,  5471, 23486,\n",
       "                       3689, 16875,    73, 30131,   112,     4,   245, 10468, 23486,  3689,\n",
       "                      50264,   820,  5471, 23486,  3689, 16875,    73, 30131,   231, 10468,\n",
       "                      23486,  3689, 50264, 50264, 50264, 23486,  3689, 16875,    73, 30131,\n",
       "                      43642, 10468, 23486,  3689, 40924,   564,  5471, 23486,  3689, 16875,\n",
       "                         73, 30131,   195, 10468, 23486,  3689, 50264,  3337,  5471, 23486,\n",
       "                       3689, 16875,    73, 30131,   132,     4,   245, 10468, 23486,  3689,\n",
       "                      40924, 50264, 50264, 23486,  3689, 16875,    73, 30131, 50264, 50264,\n",
       "                      50264, 33121, 23486,  3689, 40924,   843,  5471, 23486,  3689, 16875,\n",
       "                         73, 30131,  2843,  5471, 50264,  3689, 16875,    73, 30131,  2107,\n",
       "                       5471, 23486,  3689, 16875,    73, 30131,   654,  5471, 23486,  3689,\n",
       "                      50264, 50264, 50264,  1812,  5471, 23486,  3689, 16875,    73, 30131,\n",
       "                        316,  5471, 23486,  3689, 16875,    73, 30131, 50118, 50118,  2709,\n",
       "                      50264,  2975,     6,    52,   904,   842, 15597,  7085,  3183,   215,\n",
       "                         25, 28410,   225, 31128, 23486,  3689,     6, 17073, 14726,   359,\n",
       "                       2422, 17073, 14726,  3689,     6, 19500,    70, 15022,  3689,  4753,\n",
       "                          4,  4487, 50264, 25587, 19438,     9, 19745, 40924,   359, 36736,\n",
       "                        196, 40924,     4, 30410,   196, 23486,  3689, 40924,    64, 50264,\n",
       "                      18822,     8,   847,     7,  1552,  3989,     8,  5933,     4,    85,\n",
       "                         16, 45719, 18137,    19, 12142, 10070,     6, 50264,  5631,    13,\n",
       "                        304, 50264,   183,    12,   560,    12,  1208, 11397,   173,     6,\n",
       "                         25, 29926,    25,    13, 50264, 50264, 50264, 35729,   268,     6,\n",
       "                       4704, 25587,     6,  1413, 10816,     6,     8, 38877,     4, 31276,\n",
       "                      43522,  8114,  1534,     5, 50264, 16106,     8, 18808,   341,     9,\n",
       "                      23486,  4204, 11165,     6,   528,     7, 50264,  4069,     9, 37092,\n",
       "                      50264, 50264,  1026,  4484,     6,     8, 29259, 13005,     4, 36922,\n",
       "                      43522,  8114,  1639,     5,    78,    12,  4684, 50264,     7,   181,\n",
       "                       9451,     8, 37092,     9,   143,     9, 50264, 28410,   225, 31128,\n",
       "                         36,  2965,   651,    43, 23486, 50264, 50264, 50264,    85,    16,\n",
       "                       2778,  3792, 17252, 49121, 43701,  7063,  4382, 50264,  2782,  3971,\n",
       "                          4,  1608,   424,  1672, 40924,    64,    28,  2622,    19, 50264,\n",
       "                       2204, 50264, 50264, 50264, 12418, 50264, 50264,    85,    16,   275,\n",
       "                         13, 29215,     8, 10320,  1258, 40924,    11,  4747,     6, 21664,\n",
       "                          6,     8, 32053,     8,  2225,  4510,     6,     8,    11,  9580,\n",
       "                      11534,     4, 42509,   225, 31128, 43522,  8114,    35, 15993, 31276,\n",
       "                          6, 50264, 50264, 50264, 50264, 31847, 31276,   725,     6, 31847,\n",
       "                      31276,   574,   487,     6, 31847, 35092,    73, 15993, 35092,   574,\n",
       "                          6, 31847, 35092,   725,     6, 31847, 35092, 42631,     6, 50264,\n",
       "                      50264, 50264, 50264, 50264, 50264, 50264, 50264, 31847, 30111,   725,\n",
       "                          6, 31847, 32072,     6, 50264, 50264, 50264, 50264, 31847, 32532,\n",
       "                          6, 50264, 50264, 50264, 50264, 31847, 23417,   104,     6, 31847,\n",
       "                      23417,   725,     6, 31847,   466,  3387,   574,     6, 35381, 17357,\n",
       "                          4,   104, 31276,  2881,     6,   208, 23417,  3714,     6,   234,\n",
       "                       3669, 34036,   495,   658, 14726,   359,  1582, 20264, 14726,  8114,\n",
       "                         35,   104, 28355,   612,     6,   208, 30111,  2663,     6,   208,\n",
       "                        246, 35153,     6,   208,  2881, 31276,     6,   208, 34025,  3933,\n",
       "                          6,   208,  2881, 22814,     6,   208, 35253,  1096,     6,   208,\n",
       "                      35253,  2466, 31988,  5576, 43497,  8114,    35,   487,   288,  4280,\n",
       "                        612,     6,   234,   288,  4280,  2663,     6,   234,   288,  4280,\n",
       "                       1244,     6,   234,  3570, 38590,     6,   234,  3669,  3913,     6,\n",
       "                        234, 40892,  1244,     6, 50264, 50264, 50264, 50264,   234, 40847,\n",
       "                        612,     6,   234,  2546,  1497,     6,   234,   288, 41573,     6,\n",
       "                        234,  4197, 16295,     6,   234, 32089,  2517,     6,   234,  3669,\n",
       "                      35269,     6,   234, 40892,  1225,     6,   234, 32089,   844,     6,\n",
       "                        234, 18427,  1978,     6,   234, 18427,  2890,     6, 50264, 50264,\n",
       "                      50264, 50264, 10070,     9, 23486,  3689, 50264,    64,    28, 50264,\n",
       "                          7,  3031, 10070,     4,  6965,   157,  6421, 10070,    64,    28,\n",
       "                        156,     7,   645,     4,  4493,    10,   647,  6754, 25708,    13,\n",
       "                      50264,  2167, 50264,  3689, 50264,  7404,     4,  7111,   321,     4,\n",
       "                       1646,  5471,  5579,  3305,     4,   245,  5471,   565,     4,   771,\n",
       "                         35,   321,     4,  3669,  5471,  5579,   246,     4,  6668,  5471,\n",
       "                      50118,     2])),\n",
       "             ('net_input.src_lengths', 752),\n",
       "             ('target',\n",
       "              tensor([    1,     1,     1,     1,  8114, 27773,   154, 13982, 50118, 50118,\n",
       "                       5320,  1851,  1672,  8114,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,   112,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1, 23486,     1,     1,     1,     1,\n",
       "                          1,     1, 16875,    73, 30131,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,   204,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1, 23486,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,  3689,     1,\n",
       "                          1,     1,     1,     1,     1,     1, 40924,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,   158,  5471,     1,     1, 16875,    73, 30131,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                      16875,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1, 40924,  3330,  5471, 23486,     1,     1,     1,     1,\n",
       "                        290,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1, 40924,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,   971,  5471,     1,     1,     1,     1,     1,   132,     4,\n",
       "                        245,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1, 23486,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                      16875,    73, 30131,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                        430,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1, 23486,     1,     1, 17073, 14726,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,   178,   209,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,    28,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     8,     1,     1,\n",
       "                          1,    11,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,   157,     1,     1, 18080,  1033,     6,     1,     1,     1,\n",
       "                          1, 25587,     6,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,   144,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,    63,     1,     1,     1,\n",
       "                       5910,     6,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                      43522,     1,     1,     1,     1,     1,     1,  5910,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     5,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1, 11235,  2507,     4,     1,     1,\n",
       "                          1, 29259,  1848,     6,    19,  7063,     1,    23,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1, 19351,\n",
       "                          1, 31345,   293,    13,     1,  2975,     4,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1, 43522,     1,     1,     1,     1,\n",
       "                          1, 31847, 31276,   574,     6,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1, 31847,\n",
       "                      35092,   574,   487,     6, 31847, 30111,     6,     1,     1,     1,\n",
       "                          1,     1,     1,     1, 31847, 32072,   574,     6,     1,     1,\n",
       "                          1, 31847, 32532,   725,     6,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,   234,   698, 31782,     6,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,   234, 18427,\n",
       "                       2545,  2895,     1,     1,     1,     1, 25587,     1,     1,   847,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                        110,     1, 23486,     1, 40924,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "                          1,     1])),\n",
       "             ('nsentences', 1),\n",
       "             ('ntokens', 752)])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9167, 45781,  1163,    55, 50118, 50118, 32826,    36, 11528,    43,\n",
       "        50118, 50118,  9690,    87,   545,     6,   151,  2694,   337,  8111,\n",
       "         3451, 12827,   420,     5,  7183,  2592,   444,    55,  8422,  3369,\n",
       "        14328,    87,  2310,   514,     6,   309,     7,    10,    78,   720,\n",
       "         4990,     9,     5,  1293,    18,  2683,  3844,     6,  1027,   302,\n",
       "            4, 50118, 50118,  2709,   358,  6474,   241,     9,  2310,   514,\n",
       "        27380,    31,     5,  3342,    50,  5378,  2990,  1173, 24416,     6,\n",
       "           10,  6474,   241,    12,   463,    12,   102,    12,  4809,     9,\n",
       "        31924,  3369, 27358,     6,   373,  5378,   833,     6,    16, 12961,\n",
       "         2024,   124,    88,     5,  6444,    50,     5,  1255,     4, 50118,\n",
       "        50118,   133,  2422,    12,    29, 12107,  6572,    16,   156,   190,\n",
       "           55,  8422,    30,     5,  8321,   341,    11,     5,  2694,   337,\n",
       "         8111,   609,     6,  2634,   431,    11,     5,  8812,  4662,     9,\n",
       "            5,  5480,  9356,     4, 50118, 50118,  8739,  5961,     8, 32457,\n",
       "            6,    13,  1246,     6,    32,   258, 10266,   341,     4, 50118,\n",
       "        50118,   133,  1280,     9,  5378,   833,  2622,  3612,   358,    76,\n",
       "          480,    55,    87,   654,   325, 15768,  7472,   480,    16,   615,\n",
       "            7,  1719,     5,   194,     9,  1261,     6,    50,  1156,     8,\n",
       "         5295,  2771,     6,    11,    10,   389,    12,  6342, 12965,   241,\n",
       "           36,  1264,    12,  2917,    43, 10490,     9, 31924, 40684,     6,\n",
       "           51,  9658,     4, 50118, 50118,   113,   133,   232,  9108,   540,\n",
       "         2694,   337,  9339,   514,    87,  5378,   833,    60,  1029,    12,\n",
       "        11515, 34608,  4623,  1209,   625,   853,     6,    10,  9744,    23,\n",
       "            5,  2534,    13,  3201,     6,  9356,     8,  1309,    23,   315,\n",
       "         3076,   589,    11,  4170,     6,   896,     6,   174,  5040,     4,\n",
       "        50118, 50118,   113, 32136,    70,     5,  5378,   833,  1411,   124,\n",
       "           88,     5,  1737,     6,  2260,    11,     5,  6444,    72, 50118,\n",
       "        50118,  3684,    14,  1823,  6740,  7700,     5,  5181,     9,  8095,\n",
       "         5794,     6,     8, 27430,     5,   672,     9, 11747,     6,    61,\n",
       "           64,  1045,    22, 18758,  8905,   845, 50118, 50118,   113,   243,\n",
       "           16,  1202,    13, 31092, 28340,     7, 14575,    11,   209,  1274,\n",
       "          480,    51,   240,   384,   176,     7,  6008,    60,    26,  1209,\n",
       "          625,   853,     4, 50118, 50118,  9690,    87,   457,     9,     5,\n",
       "         5378,   833,   606,    31,   129,   237,   749,    35,  2030,  3466,\n",
       "           36,  2036,   135,   238,   315,  4681,  8313,    36,   844,     4,\n",
       "          176,   135,   238, 13857,    36,   401,   135,    43,     8,  5978,\n",
       "           36,   245,     4,   398,   135,   322, 50118, 50118, 11073,  1327,\n",
       "            6,     5,  2367,   953,     6,     8,   514,    12,  3641,  5202,\n",
       "          650,  2946,   982,    11,     5,  3073,     8,  5140,    67,  5864,\n",
       "         4008,    15,  2694,   337,  8111,     7,   694,  1522,  4835,   514,\n",
       "            6,    61,  2349,    13,   823,    80,    12, 10224,     9,  4850,\n",
       "            4, 50118, 50118,   133,  1079,    16,   341,    11,   539,     6,\n",
       "           25,    10,  3035,   927,    11,  1007,   931,     6,     8,    11,\n",
       "         6300,     4, 50118, 50118, 34623,    65,    11,   237,    82,   697,\n",
       "           11,  3806,   147,   514,  1915,    32, 16429,   148,   233,     9,\n",
       "            5,    76,     6,     8,   457,    12,   102,    12,  9026,   676,\n",
       "          514, 28227,    76,  1062,     6,   309,     7,     5,   315,  3076,\n",
       "            4, 50118, 50118,    12,  3201, 28227,   111, 50118, 50118, 11321,\n",
       "          570,     6,     5,   623,  4713,  7665,    18,  1013,  1849, 12366,\n",
       "         2872,    34,  6566,  4173,    22,  5412, 16645,   113,    25,   566,\n",
       "            5,   720,  3455,   480,  1065,  1632, 12884,     6,  2862,  8702,\n",
       "            8,  5381,    12, 25764,     4, 50118, 50118, 25589, 28227,    16,\n",
       "         1726,    30,   171,   383,     6,  1158,    19,    10,   720,  1956,\n",
       "         3172,    11,    15,   799,   325,     4, 50118, 50118, 37038, 12323,\n",
       "          117,  1181,  1338,     5,  3342,     6, 16690,  1594,   268,    32,\n",
       "          145, 28635,  3841,     6,     8,  6631,    16,   326,  1851,  2577,\n",
       "          514,  1065,  1255,     8,   874,     4, 50118, 50118,  3908,  2147,\n",
       "          464,     6,     5,  1068,    40,   120,  3007,     4, 50118, 50118,\n",
       "         2709,   349,  3093,     9,   720,  8232,     6,    59,   707,   135,\n",
       "            9,     5,   232,    18,  1956,   480,   457,    12,   102,    12,\n",
       "         9026,    82,   480,    40,    33,   291,   135,   540, 34769,     6,\n",
       "          309,     7,     5,  3870, 21494, 19413,    15, 11001,  7229,    36,\n",
       "         3808,  3376,   322, 50118, 50118,   113, 28324,   337,  8111,   806,\n",
       "           34, 11916,    10,   739,   346,     9,    82,    60,    26,  1209,\n",
       "          625,   853,     4,    22,  1708,    52,  1395,  8861,     5,   931,\n",
       "            9,  5378,   833,     6,    61,    16,   164,     7,   555,    41,\n",
       "          190,  2388,   936,    11,     5,   499,    72, 50118, 50118, 36926,\n",
       "        13700,    12,  8056,   806,    13,  8201,  6740,    31,   514,    34,\n",
       "           57,   198,   187,     5,  7571,    29,     4,   870,  4525,     6,\n",
       "           89,    58,   416,   155,     6,   151,  3451,    11,  2513,   198,\n",
       "            5,  7183,     4, 50118, 50118,  4148,   595,  3926,     6,     5,\n",
       "         1293,    40,   192,    10,   746,     9,    23,   513,   601,     6,\n",
       "         1497,  3451,    30, 10380,     6,  1209,   625,   853,    26,     6,\n",
       "         5196,    14,    65,   739,  2195,    64,  2592,    25,   203,  2310,\n",
       "          514,   480,     8,  5378,   833,   480,    25,  1878,    50,  2993,\n",
       "          650,  1980,     4, 50118, 50118,  9690,    87,  1814,   135,     9,\n",
       "         2694,   337,  8111,  3451,    32,    11,  8581,  6795,     4,   152,\n",
       "         6771,     5,   754,    14,     5,   806,  1189,  3214,     6,   941,\n",
       "           11,  1007,  1042,     4, 50118, 50118,  1708,    24,    67,   839,\n",
       "           14,  4066,  3949,    33,     5,  2148,     7,  2179,  1319,     7,\n",
       "        26178,     9,  8422,  5378,   833,    14,    32,   540, 11190,     7,\n",
       "         6444,     8,  1212, 11534,     6,    37,   355,     4, 50118, 50118,\n",
       "         6323,  4792,  1377,    33,   190,  2343,    14, 10639,  5378,   833,\n",
       "           64,  2501,  5167,     9,  1402,  3539,  4707,    11, 16690,  1043,\n",
       "        17898,     4, 50118, 50118,   116,   954,  5040, 50118,     2])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7460, 9116, 3627, ...,  905, 5192,  235])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.lexsort([shuffle, dataset.sizes[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6058, 5953, 3843, ..., 7316, 7439, 6659])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9381"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = dataset.sizes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   10,    12,    13, ..., 26119, 32135, 39828], dtype=uint16)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = indices[sizes[indices] <= tokens_per_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6058, 5953, 3843, ..., 3656, 3521, 1275])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3294"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  12,  13, ..., 512, 512, 512], dtype=uint16)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(dataset, \n",
    "            indices, \n",
    "            max_tokens=-1, \n",
    "            max_sentences=8, \n",
    "            bsz_mult=8):\n",
    "    num_tokens_vec = np.array([dataset.num_tokens(idx) for idx in indices]) # The  number of max tokens in the batch position\n",
    "    indices_len = indices.shape[0]\n",
    "    \n",
    "    batches_ends = np.zeros(indices_len, dtype=np.int32)\n",
    "    batches_ends_view = batches_ends[:]\n",
    "    num_tokens_view = num_tokens_vec\n",
    "\n",
    "    pos = 0\n",
    "    new_batch_end = 0\n",
    "\n",
    "    new_batch_max_tokens = 0\n",
    "    new_batch_sentences = 0\n",
    "    new_batch_num_tokens = 0\n",
    "\n",
    "    overflow = False\n",
    "    size_matches_with_bsz_mult = False\n",
    "\n",
    "    batches_count = 0\n",
    "    batch_start = 0\n",
    "    tail_max_tokens = 0\n",
    "    batch_max_tokens = 0\n",
    "\n",
    "    for pos in range(indices_len):\n",
    "        # At every pos we keep stats about the last complete batch [batch_start:batch_end),\n",
    "        #      and tail [batch_end:pos].\n",
    "        # 1) Every time when (batch + tail) forms a valid batch\n",
    "        #      (according to max_tokens, max_sentences and bsz_mult) we append tail to batch.\n",
    "        # 2) When (batch+tail) violates max_tokens or max_sentences constraints\n",
    "        #      we finalize running batch, and tail becomes a new batch.\n",
    "        # 3) There is a corner case when tail also violates constraints.\n",
    "        #      In that situation [batch_end:pos-1] (tail without the current pos)\n",
    "        #      gets added to the finalized batches, while [pos:pos] becomes a new tail.\n",
    "        #\n",
    "        # Important: For the sake of performance try to avoid using function calls within this loop.\n",
    "\n",
    "        tail_max_tokens = tail_max_tokens \\\n",
    "                            if tail_max_tokens > num_tokens_view[pos] \\\n",
    "                            else num_tokens_view[pos]\n",
    "        new_batch_end = pos + 1\n",
    "        new_batch_max_tokens = batch_max_tokens \\\n",
    "                                if batch_max_tokens > tail_max_tokens \\\n",
    "                                else tail_max_tokens\n",
    "        new_batch_sentences = new_batch_end - batch_start\n",
    "        new_batch_num_tokens = new_batch_sentences * new_batch_max_tokens\n",
    "\n",
    "        overflow = (new_batch_sentences > max_sentences > 0 or\n",
    "                    new_batch_num_tokens > max_tokens > 0)\n",
    "        size_matches_with_bsz_mult = (new_batch_sentences < bsz_mult or\n",
    "                                      new_batch_sentences % bsz_mult == 0)\n",
    "\n",
    "        if overflow:\n",
    "            tail_num_tokens = tail_max_tokens * \\\n",
    "                    (new_batch_end - batches_ends_view[batches_count])\n",
    "            tail_overflow = tail_num_tokens > max_tokens > 0\n",
    "            # In case of a tail overflow finalize two batches\n",
    "            if tail_overflow:\n",
    "                batches_count += 1\n",
    "                batches_ends_view[batches_count] = pos\n",
    "                tail_max_tokens = num_tokens_view[pos]\n",
    "            batch_start = batches_ends_view[batches_count]\n",
    "            batches_count += 1\n",
    "            new_batch_max_tokens = tail_max_tokens\n",
    "\n",
    "        if overflow or size_matches_with_bsz_mult:\n",
    "            batches_ends_view[batches_count] = new_batch_end\n",
    "            batch_max_tokens = new_batch_max_tokens\n",
    "            tail_max_tokens = 0\n",
    "    if batches_ends_view[batches_count] != indices_len:\n",
    "        batches_count += 1\n",
    "    # Memory and time-efficient split\n",
    "    return np.split(indices, batches_ends[:batches_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices = batchify(dataset, indices, max_sentences=batch_size, bsz_mult=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6058, 5953, 3843, 4885, 4548, 2248, 4967,  144, 3506, 6045, 6165,\n",
       "       9239,  205, 3108, 7472, 9243, 2159, 3307, 2025,  971])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([6058, 5953, 3843, 4885, 4548, 2248, 4967,  144, 3506, 6045, 6165,\n",
       "        9239,  205, 3108, 7472, 9243, 2159, 3307, 2025,  971, 7130, 1777,\n",
       "        2164, 6368, 8819, 1480, 1759, 2192, 1432, 4939, 2373, 2988]),\n",
       " array([8687, 4821, 3761, 1143, 1749, 3224, 3662, 1382, 7798, 5165, 4091,\n",
       "        2087, 6222, 5552, 8658, 3092,  496, 1615, 5353, 2148, 7442, 8023,\n",
       "        5097, 1473,  667, 1096, 1744, 8065, 7626, 1502, 6701, 7783]),\n",
       " array([ 954, 7094,  748, 6795, 1038, 2279, 3051, 6140, 3014, 4779, 1427,\n",
       "         285, 2311, 5875, 8725, 6338, 4903, 3898, 7144, 8858, 4184, 2047,\n",
       "        9360, 6620, 4789, 8927, 6862, 2648, 7128, 5391, 1829, 8482]),\n",
       " array([1404, 7113, 3969,  142,  862,  555, 5765, 3527, 2466, 6388, 1415,\n",
       "        9269, 7501, 3474, 2329, 1177,  122, 2512, 8640, 7426, 2873, 8651,\n",
       "        1691, 1146, 7859, 3262, 8716, 3386, 9214, 9056, 7274, 6136])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_indices[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 32]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(len(s) for s in batch_indices[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6058 5953 3843 4885 4548 2248 4967  144 3506 6045 6165 9239  205 3108\n",
      " 7472 9243 2159 3307 2025  971 7130 1777 2164 6368 8819 1480 1759 2192\n",
      " 1432 4939 2373 2988]\n",
      "[7368 7387 6144 6758 6880 1740 7065 6558  321 9064 6328 2020 8969 6883\n",
      " 8197 2608 5338 5625  902 8268 6262 7734 4848 8547 4670 9039 2728 6886\n",
      " 1547 9350 8883 1999]\n"
     ]
    }
   ],
   "source": [
    "for idx in batch_indices:\n",
    "    if 321 in idx or 6058 in idx:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([381, 381, 381, 381, 381, 381, 381, 381], dtype=uint16)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes[[ 321, 9064, 6328, 2020, 8969, 6883, 8197, 2608]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, batch_indices):\n",
    "        super().__init__(dataset)\n",
    "        self.indices = batch_indices\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for idx in self.indices:\n",
    "            for i in range(self.batch_size):\n",
    "                try:\n",
    "                    yield idx[i]\n",
    "                except Exception:\n",
    "                    return\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collater(batches):\n",
    "    sample = OrderedDict([(k, []) for k in batches[0].keys()] )\n",
    "    max_length = max(batch['ntokens'] for batch in batches)\n",
    "    for k in sample:\n",
    "        is_tensor = False\n",
    "        for i in range(len(batches)):\n",
    "            is_tensor = torch.is_tensor(batches[i][k])\n",
    "            item = batches[i][k]\n",
    "            if k in ['net_input.src_tokens', 'target']:\n",
    "                item = torch.cat((item, torch.tensor([dictionary.pad()]*(max_length - batches[i]['ntokens'])).to(torch.int64)) )\n",
    "            sample[k].append(item)                \n",
    "            \n",
    "        if is_tensor:\n",
    "            sample[k] = torch.stack(sample[k])\n",
    "        else:\n",
    "            sample[k] = torch.tensor(sample[k])\n",
    "    return sample\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, sampler=DataSampler(dataset, batch_size, batch_indices), batch_size=batch_size, shuffle=False, collate_fn=collater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 25]) torch.Size([32, 25])\n",
      "torch.Size([32, 34]) torch.Size([32, 34])\n",
      "torch.Size([32, 41]) torch.Size([32, 41])\n",
      "torch.Size([32, 52]) torch.Size([32, 52])\n",
      "torch.Size([32, 61]) torch.Size([32, 61])\n",
      "torch.Size([32, 74]) torch.Size([32, 74])\n",
      "torch.Size([32, 87]) torch.Size([32, 87])\n",
      "torch.Size([32, 96]) torch.Size([32, 96])\n",
      "torch.Size([32, 106]) torch.Size([32, 106])\n",
      "torch.Size([32, 114]) torch.Size([32, 114])\n",
      "torch.Size([32, 123]) torch.Size([32, 123])\n",
      "torch.Size([32, 133]) torch.Size([32, 133])\n",
      "torch.Size([32, 144]) torch.Size([32, 144])\n",
      "torch.Size([32, 152]) torch.Size([32, 152])\n",
      "torch.Size([32, 158]) torch.Size([32, 158])\n",
      "torch.Size([32, 167]) torch.Size([32, 167])\n",
      "torch.Size([32, 174]) torch.Size([32, 174])\n",
      "torch.Size([32, 182]) torch.Size([32, 182])\n",
      "torch.Size([32, 188]) torch.Size([32, 188])\n",
      "torch.Size([32, 196]) torch.Size([32, 196])\n",
      "torch.Size([32, 204]) torch.Size([32, 204])\n",
      "torch.Size([32, 210]) torch.Size([32, 210])\n",
      "torch.Size([32, 216]) torch.Size([32, 216])\n",
      "torch.Size([32, 221]) torch.Size([32, 221])\n",
      "torch.Size([32, 226]) torch.Size([32, 226])\n",
      "torch.Size([32, 234]) torch.Size([32, 234])\n",
      "torch.Size([32, 239]) torch.Size([32, 239])\n",
      "torch.Size([32, 243]) torch.Size([32, 243])\n",
      "torch.Size([32, 248]) torch.Size([32, 248])\n",
      "torch.Size([32, 253]) torch.Size([32, 253])\n",
      "torch.Size([32, 258]) torch.Size([32, 258])\n",
      "torch.Size([32, 264]) torch.Size([32, 264])\n",
      "torch.Size([32, 271]) torch.Size([32, 271])\n",
      "torch.Size([32, 275]) torch.Size([32, 275])\n",
      "torch.Size([32, 278]) torch.Size([32, 278])\n",
      "torch.Size([32, 281]) torch.Size([32, 281])\n",
      "torch.Size([32, 287]) torch.Size([32, 287])\n",
      "torch.Size([32, 292]) torch.Size([32, 292])\n",
      "torch.Size([32, 296]) torch.Size([32, 296])\n",
      "torch.Size([32, 301]) torch.Size([32, 301])\n",
      "torch.Size([32, 306]) torch.Size([32, 306])\n",
      "torch.Size([32, 309]) torch.Size([32, 309])\n",
      "torch.Size([32, 314]) torch.Size([32, 314])\n",
      "torch.Size([32, 320]) torch.Size([32, 320])\n",
      "torch.Size([32, 325]) torch.Size([32, 325])\n",
      "torch.Size([32, 328]) torch.Size([32, 328])\n",
      "torch.Size([32, 332]) torch.Size([32, 332])\n",
      "torch.Size([32, 335]) torch.Size([32, 335])\n",
      "torch.Size([32, 339]) torch.Size([32, 339])\n",
      "torch.Size([32, 342]) torch.Size([32, 342])\n",
      "torch.Size([32, 346]) torch.Size([32, 346])\n",
      "torch.Size([32, 349]) torch.Size([32, 349])\n",
      "torch.Size([32, 354]) torch.Size([32, 354])\n",
      "torch.Size([32, 357]) torch.Size([32, 357])\n",
      "torch.Size([32, 360]) torch.Size([32, 360])\n",
      "torch.Size([32, 364]) torch.Size([32, 364])\n",
      "torch.Size([32, 368]) torch.Size([32, 368])\n",
      "torch.Size([32, 372]) torch.Size([32, 372])\n",
      "torch.Size([32, 376]) torch.Size([32, 376])\n",
      "torch.Size([32, 379]) torch.Size([32, 379])\n",
      "torch.Size([32, 383]) torch.Size([32, 383])\n",
      "torch.Size([32, 388]) torch.Size([32, 388])\n",
      "torch.Size([32, 391]) torch.Size([32, 391])\n",
      "torch.Size([32, 394]) torch.Size([32, 394])\n",
      "torch.Size([32, 398]) torch.Size([32, 398])\n",
      "torch.Size([32, 402]) torch.Size([32, 402])\n",
      "torch.Size([32, 404]) torch.Size([32, 404])\n",
      "torch.Size([32, 407]) torch.Size([32, 407])\n",
      "torch.Size([32, 409]) torch.Size([32, 409])\n",
      "torch.Size([32, 412]) torch.Size([32, 412])\n",
      "torch.Size([32, 416]) torch.Size([32, 416])\n",
      "torch.Size([32, 419]) torch.Size([32, 419])\n",
      "torch.Size([32, 422]) torch.Size([32, 422])\n",
      "torch.Size([32, 425]) torch.Size([32, 425])\n",
      "torch.Size([32, 429]) torch.Size([32, 429])\n",
      "torch.Size([32, 432]) torch.Size([32, 432])\n",
      "torch.Size([32, 435]) torch.Size([32, 435])\n",
      "torch.Size([32, 438]) torch.Size([32, 438])\n",
      "torch.Size([32, 441]) torch.Size([32, 441])\n",
      "torch.Size([32, 445]) torch.Size([32, 445])\n",
      "torch.Size([32, 449]) torch.Size([32, 449])\n",
      "torch.Size([32, 451]) torch.Size([32, 451])\n",
      "torch.Size([32, 455]) torch.Size([32, 455])\n",
      "torch.Size([32, 457]) torch.Size([32, 457])\n",
      "torch.Size([32, 459]) torch.Size([32, 459])\n",
      "torch.Size([32, 461]) torch.Size([32, 461])\n",
      "torch.Size([32, 464]) torch.Size([32, 464])\n",
      "torch.Size([32, 466]) torch.Size([32, 466])\n",
      "torch.Size([32, 471]) torch.Size([32, 471])\n",
      "torch.Size([32, 474]) torch.Size([32, 474])\n",
      "torch.Size([32, 477]) torch.Size([32, 477])\n",
      "torch.Size([32, 480]) torch.Size([32, 480])\n",
      "torch.Size([32, 482]) torch.Size([32, 482])\n",
      "torch.Size([32, 485]) torch.Size([32, 485])\n",
      "torch.Size([32, 487]) torch.Size([32, 487])\n",
      "torch.Size([32, 491]) torch.Size([32, 491])\n",
      "torch.Size([32, 493]) torch.Size([32, 493])\n",
      "torch.Size([32, 496]) torch.Size([32, 496])\n",
      "torch.Size([32, 499]) torch.Size([32, 499])\n",
      "torch.Size([32, 502]) torch.Size([32, 502])\n",
      "torch.Size([32, 505]) torch.Size([32, 505])\n",
      "torch.Size([32, 509]) torch.Size([32, 509])\n",
      "torch.Size([30, 512]) torch.Size([30, 512])\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(dataloader):\n",
    "    src_tokens, tgt_tokens = sample['net_input.src_tokens'], sample['target']\n",
    "    print(src_tokens.size(), tgt_tokens.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3733, 4785, 8557, 8415, 7367, 7083, 9144, 5081, 5711, 2688, 3401, 2990,\n",
       "        8305, 6438, 3718,  626, 1980, 2503, 7457, 3203, 8070, 5592, 4959, 6476,\n",
       "        3949, 5487, 7198, 3656, 3521, 1275])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 512])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['net_input.src_tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50264, 50264,   692,  ...,     1,     1,     1],\n",
       "        [    0,   170,    32,  ...,     1,     1,     1],\n",
       "        [    0, 15852, 48853,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,  5762,   578,  ...,   494, 50118,     2],\n",
       "        [    0, 19993,   324,  ...,  1090, 50118, 50264],\n",
       "        [    0,  3632,  3293,  ..., 50264, 50264,     2]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['net_input.src_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 28061,     1,  ...,     1,     1,     1],\n",
       "        [    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [    1,     1,   682,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [    1,     1,     1,  ...,     1,     1,     2],\n",
       "        [    1,     1,     1,  ...,     4, 50118,     1]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'src_tokens': src_tokens, 'tgt_tokens': tgt_tokens, 'dictionary': dictionary,\n",
    "            'tokens_per_sample': tokens_per_sample}, \n",
    "           '/mnt/dl/fairseq/Masked_Language_Model/openwebtext/model.inp.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "from fairseq import options\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = torch.load(  '/mnt/dl/fairseq/Masked_Language_Model/openwebtext/model.inp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = state['dictionary']\n",
    "# src_tokens = state['src_tokens']\n",
    "# tgt_tokens = state['tgt_tokens']\n",
    "# tokens_per_sample = state['tokens_per_sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLMTask:\n",
    "    \n",
    "    def __init__(self, dictionary) -> None:\n",
    "        self.source_dictionary = dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = MaskedLMTask(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = options.get_parser(\"Model\", default_task='masked_lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.max_positions = tokens_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaModel.build_model(model_args, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaEncoder(\n",
       "  (sentence_encoder): TransformerEncoder(\n",
       "    (dropout_module): FairseqDropout()\n",
       "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "    (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayerBase(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (activation_dropout_module): FairseqDropout()\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (encoder): RobertaEncoder(\n",
       "    (sentence_encoder): TransformerEncoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): RobertaLMHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classification_heads): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, src_tokens, tgt_tokens):\n",
    "    print(src_tokens.size())\n",
    "    print(tgt_tokens.size())\n",
    "    masked_tokens = tgt_tokens.ne(dictionary.pad())\n",
    "    with torch.no_grad():\n",
    "        logits = model(src_tokens.cuda(), masked_tokens=masked_tokens.cuda())\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 512])\n",
      "torch.Size([30, 512])\n"
     ]
    }
   ],
   "source": [
    "logits, *_ = forward(model, src_tokens, tgt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2319, 50265])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tgt_tokens[torch.where(tgt_tokens.ne(dictionary.pad()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0, 28061,     5,  ...,   353,     4, 50118])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2319])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5911,  0.0000, -0.1709,  ..., -0.7963, -0.6580, -0.3671],\n",
       "        [-0.5502,  0.0000,  0.8713,  ...,  0.6290, -0.0139, -0.3900],\n",
       "        [-0.6489,  0.0000, -0.2749,  ...,  0.8538,  0.2589, -0.2691],\n",
       "        ...,\n",
       "        [ 0.8187,  0.0000,  0.3288,  ..., -0.6236, -0.0106,  0.0048],\n",
       "        [ 0.3306,  0.0000,  0.0701,  ...,  0.2340, -0.3663, -1.1104],\n",
       "        [ 0.0511,  0.0000, -0.1746,  ..., -0.1997,  0.3947, -0.3209]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy( logits, labels.to(torch.int64).cuda(), reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.9865, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fairseq-hydra-train -m --config-dir /env_nlp/lib/python3.9/site-packages/fairseq/examples/roberta/config/pretraining --config-name base2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " fairseq-preprocess     --only-source     --srcdict /mnt/dl/fairseq/Masked_Language_Model/openwebtext2/dict.txt     --trainpref /mnt/dl/fairseq/Masked_Language_Model/openwebtext2/train.raw.txt     --validpref /mnt/dl/fairseq/Masked_Language_Model/openwebtext2/valid.raw.txt     --testpref /mnt/dl/fairseq/Masked_Language_Model/openwebtext2/test.raw.txt     --destdir /mnt/dl/fairseq/Masked_Language_Model/openwebtext2/roberta-data-bin     --workers  2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiofiles==22.1.0\n",
      "aiohttp==3.8.4\n",
      "aiosignal==1.3.1\n",
      "aiosqlite==0.19.0\n",
      "antlr4-python3-runtime==4.8\n",
      "anyio==3.6.2\n",
      "argon2-cffi==21.3.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.2.3\n",
      "astroid==2.15.4\n",
      "asttokens==2.2.1\n",
      "async-timeout==4.0.2\n",
      "attrs==23.1.0\n",
      "autopep8==2.0.2\n",
      "Babel==2.12.1\n",
      "backcall==0.2.0\n",
      "beautifulsoup4==4.12.2\n",
      "bitarray==2.7.3\n",
      "bleach==6.0.0\n",
      "certifi==2022.12.7\n",
      "cffi==1.15.1\n",
      "charset-normalizer==3.1.0\n",
      "click==8.1.3\n",
      "cmake==3.26.3\n",
      "colorama==0.4.6\n",
      "comm==0.1.3\n",
      "contourpy==1.0.7\n",
      "cycler==0.11.0\n",
      "Cython==0.29.34\n",
      "datasets==2.12.0\n",
      "debugpy==1.6.7\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "dill==0.3.6\n",
      "docopt==0.6.2\n",
      "docstring-to-markdown==0.12\n",
      "executing==1.2.0\n",
      "fairseq==0.12.2\n",
      "fastBPE==0.1.1\n",
      "fastjsonschema==2.16.3\n",
      "filelock==3.12.0\n",
      "flake8==6.0.0\n",
      "fonttools==4.39.4\n",
      "fqdn==1.5.1\n",
      "frozenlist==1.3.3\n",
      "fsspec==2023.5.0\n",
      "huggingface-hub==0.15.1\n",
      "hydra-core==1.0.7\n",
      "idna==3.4\n",
      "importlib-metadata==6.6.0\n",
      "importlib-resources==5.12.0\n",
      "ipykernel==6.22.0\n",
      "ipython==8.13.1\n",
      "ipython-genutils==0.2.0\n",
      "isoduration==20.11.0\n",
      "isort==5.12.0\n",
      "jedi==0.18.2\n",
      "Jinja2==3.1.2\n",
      "joblib==1.2.0\n",
      "json5==0.9.11\n",
      "jsonpointer==2.3\n",
      "jsonschema==4.17.3\n",
      "jupyter-events==0.6.3\n",
      "jupyter-lsp==2.1.0\n",
      "jupyter-ydoc==0.2.4\n",
      "jupyter_client==8.2.0\n",
      "jupyter_core==5.3.0\n",
      "jupyter_server==2.5.0\n",
      "jupyter_server_fileid==0.9.0\n",
      "jupyter_server_terminals==0.4.4\n",
      "jupyter_server_ydoc==0.8.0\n",
      "jupyterlab==3.6.3\n",
      "jupyterlab-lsp==4.1.0\n",
      "jupyterlab-pygments==0.2.2\n",
      "jupyterlab_server==2.22.1\n",
      "kiwisolver==1.4.4\n",
      "lazy-object-proxy==1.9.0\n",
      "lit==16.0.2\n",
      "lxml==4.9.2\n",
      "MarkupSafe==2.1.2\n",
      "matplotlib==3.7.1\n",
      "matplotlib-inline==0.1.6\n",
      "mccabe==0.7.0\n",
      "mistune==2.0.5\n",
      "mock==5.0.2\n",
      "mosestokenizer==1.2.1\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.4\n",
      "multiprocess==0.70.14\n",
      "mypy==1.2.0\n",
      "mypy-extensions==1.0.0\n",
      "nbclassic==0.5.6\n",
      "nbclient==0.7.4\n",
      "nbconvert==7.3.1\n",
      "nbformat==5.8.0\n",
      "nest-asyncio==1.5.6\n",
      "networkx==3.1\n",
      "notebook==6.5.4\n",
      "notebook_shim==0.2.3\n",
      "numpy==1.24.3\n",
      "omegaconf==2.0.6\n",
      "openfile==0.0.7\n",
      "opt-einsum==3.3.0\n",
      "packaging==23.1\n",
      "pandas==2.0.1\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.3\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.5.0\n",
      "platformdirs==3.5.0\n",
      "pluggy==1.0.0\n",
      "portalocker==2.7.0\n",
      "prometheus-client==0.16.0\n",
      "prompt-toolkit==3.0.38\n",
      "protobuf==3.20.3\n",
      "psutil==5.9.5\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pyarrow==12.0.0\n",
      "pycodestyle==2.10.0\n",
      "pycparser==2.21\n",
      "pydocstyle==6.3.0\n",
      "pyflakes==3.0.1\n",
      "Pygments==2.15.1\n",
      "pylint==2.17.3\n",
      "pyparsing==3.0.9\n",
      "pyro-api==0.1.2\n",
      "pyro-ppl==1.8.4\n",
      "pyrsistent==0.19.3\n",
      "python-dateutil==2.8.2\n",
      "python-json-logger==2.0.7\n",
      "python-lsp-jsonrpc==1.0.0\n",
      "python-lsp-server==1.7.2\n",
      "pytoolconfig==1.2.5\n",
      "pytz==2023.3\n",
      "PyYAML==6.0\n",
      "pyzmq==25.0.2\n",
      "regex==2023.3.23\n",
      "requests==2.29.0\n",
      "responses==0.18.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rope==1.7.0\n",
      "sacrebleu==2.3.1\n",
      "sacremoses==0.0.53\n",
      "safetensors==0.3.1\n",
      "Send2Trash==1.8.2\n",
      "six==1.16.0\n",
      "sniffio==1.3.0\n",
      "snowballstemmer==2.2.0\n",
      "soupsieve==2.4.1\n",
      "stack-data==0.6.2\n",
      "subword-nmt==0.3.8\n",
      "sympy==1.11.1\n",
      "tabulate==0.9.0\n",
      "tensorboardX==2.6\n",
      "terminado==0.17.1\n",
      "tinycss2==1.2.1\n",
      "tokenizers==0.13.3\n",
      "toml==0.10.2\n",
      "tomli==2.0.1\n",
      "tomlkit==0.11.8\n",
      "toolwrapper==2.1.0\n",
      "torch==2.0.0+cu118\n",
      "torchaudio==2.0.1+cu118\n",
      "torchvision==0.15.1+cu118\n",
      "tornado==6.3.1\n",
      "tqdm==4.65.0\n",
      "traitlets==5.9.0\n",
      "transformers==4.30.0\n",
      "triton==2.0.0\n",
      "typing_extensions==4.5.0\n",
      "tzdata==2023.3\n",
      "uctools==1.3.0\n",
      "ujson==5.7.0\n",
      "uri-template==1.2.0\n",
      "urllib3==1.26.15\n",
      "wcwidth==0.2.6\n",
      "webcolors==1.13\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.5.1\n",
      "whatthepatch==1.0.4\n",
      "wrapt==1.15.0\n",
      "xxhash==3.2.0\n",
      "y-py==0.5.9\n",
      "yapf==0.32.0\n",
      "yarl==1.9.2\n",
      "ypy-websocket==0.8.2\n",
      "zipp==3.15.0\n",
      "zstandard==0.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
