{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.linalg as sl\n",
    "from PIL import Image\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "from torch import nn, distributions as dist, autograd\n",
    "from torch.func import jacfwd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, RandomHorizontalFlip, RandomVerticalFlip, ToTensor, Normalize\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "# torch.set_default_device(\"cuda\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = \"/mnt/dl/datasets/Oxford102FlowersSplits/\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "LABELS = {i: k.strip() for i, k in enumerate(open(os.path.join(DATASET_PATH, \"names.txt\")))}\n",
    "img_size = 112\n",
    "batch_size = 32\n",
    "num_classes = len(LABELS)\n",
    "patch_size = 16\n",
    "num_patches = img_size ** 2 / patch_size **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, path, split, cache=True, transforms=None):\n",
    "        super().__init__()\n",
    "        self.load_data(path, split)\n",
    "        self.samples = dict()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def load_data(self, path, split):\n",
    "        path = os.path.join(path, split, )\n",
    "        img_files = os.listdir(os.path.join(path, \"jpeg\"))\n",
    "        img_files = sorted(img_files, key=lambda x: int(x.replace(\".jpeg\", \"\")))\n",
    "        img_files = list(img_files)\n",
    "        \n",
    "        labels = list(open(os.path.join(path, \"label\", \"label.txt\"),))\n",
    "        self.labels = [int(l.strip()) for l in labels]\n",
    "        \n",
    "        self.img_files = [os.path.join(path, \"jpeg\", name) for name in img_files]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index not in self.samples:\n",
    "            self.load_sample(index)\n",
    "        sample = self.samples[index]\n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "\n",
    "        return (sample, self.labels[index])\n",
    "        \n",
    "    def load_sample(self, idx):\n",
    "        img = Image.open(self.img_files[idx])\n",
    "        img = np.array(img).astype(np.float32)\n",
    "        self.samples[idx] = img\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FlowerDataset(DATASET_PATH, \"train\", transforms=Compose([\n",
    "    ToTensor(),\n",
    "    Resize((img_size, img_size)),\n",
    "    RandomHorizontalFlip(0.1),\n",
    "    RandomVerticalFlip(0.),\n",
    "    Normalize(0., 255.0)\n",
    "]))\n",
    "val_ds = FlowerDataset(DATASET_PATH, \"validation\", transforms=Compose([\n",
    "    ToTensor(),\n",
    "    Resize((img_size, img_size)),\n",
    "    Normalize(0., 255.0)\n",
    "    \n",
    "]))\n",
    "\n",
    "test_ds = FlowerDataset(DATASET_PATH, \"test\", transforms=Compose([\n",
    "    ToTensor(),\n",
    "    Resize((img_size, img_size)),\n",
    "    Normalize(0., 255.0)\n",
    "    \n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34\n",
    "ebm = resnet34(num_classes)\n",
    "ebm.fc = nn.Linear(512, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = torch.tensor([torch.std(p) for p in ebm.parameters()])\n",
    "mean = torch.tensor([torch.mean(p) for p in ebm.parameters()])\n",
    "params = torch.column_stack([std, mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.sort(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([p.sum().item() for p in ebm.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for p in ebm.parameters():\n",
    "        if len(p.size()) == 1:\n",
    "            nn.init.zeros_(p)\n",
    "            continue\n",
    "        # nn.init.uniform_(p, -5e-3, 5e-3)\n",
    "        nn.init.xavier_normal_(p)\n",
    "print(sum([p.sum().item() for p in ebm.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTrainer:\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader=None, epochs=1, eval_epochs=0, savepath=None):\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        self.train_loader = train_loader \n",
    "        self.val_loader = val_loader\n",
    "        self.epochs = epochs\n",
    "        self.eval_epochs = eval_epochs\n",
    "        self.savepath = savepath\n",
    "        self.eval_savepath = os.path.join(self.savepath, \"eval\")\n",
    "        self.model_savepath = os.path.join(self.savepath, \"model\")\n",
    "        \n",
    "        os.makedirs(self.model_savepath, exist_ok=True)\n",
    "        os.makedirs(self.eval_savepath, exist_ok=True)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        \n",
    "        \n",
    "    def train(self,):\n",
    "        self.train_losses = []\n",
    "        self.acc = []\n",
    "        best_loss = 0.\n",
    "        for i in range(self.epochs):\n",
    "            ep_losses = self.run_epoch(i)\n",
    "            self.train_losses.extend(ep_losses)\n",
    "            if self.eval_epochs > 0 and i % self.eval_epochs == 0:\n",
    "                acc = self.eval_epoch(i)\n",
    "                if acc > best_loss:\n",
    "                    best_loss = acc\n",
    "                    self.save_model(fname=\"best_model\", epoch=i)\n",
    "                print(\"**\" * 20 + f\"Epoch {i} acc: {acc}\")\n",
    "                self.acc.append(acc.item())\n",
    "        print(\"Succesfully trained...\")\n",
    "        self.save_model(f\"last_model\", self.epochs)\n",
    "        return True\n",
    "    \n",
    "    def save_model(self, fname, epoch=0):\n",
    "        torch.save({\"model\": self.model.state_dict(),\n",
    "                    \"optimizers\": self.optimizer.state_dict(),\n",
    "                    \"losses\": self.train_losses,\n",
    "                    \"epoch\": epoch\n",
    "                    \n",
    "            }, os.path.join(self.model_savepath, fname))\n",
    "\n",
    "    def run_epoch(self, epoch):\n",
    "        losses = []\n",
    "        self.model.train()\n",
    "        for j, (img, label) in enumerate(self.train_loader):\n",
    "            img, label = img.cuda(), label.cuda()\n",
    "            pred = self.model(img)\n",
    "            loss = self.loss_fn(pred, label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10.)\n",
    "            self.optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            # print(sum([p.sum() for p in ebm.parameters()]), img.mean(), label.float().mean())\n",
    "            if j % 5 == 0:\n",
    "                print(f\"Epoch {epoch}, step {j}, loss: {np.mean(losses)}\")\n",
    "                \n",
    "        return losses\n",
    "    \n",
    "    def eval_epoch(self, epoch):\n",
    "        savepath = os.path.join(self.eval_savepath, f\"{epoch:05d}\")\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        self.model.eval()\n",
    "        print(f\"Evaluating {epoch}\")\n",
    "        ep_acc = []\n",
    "        with torch.no_grad():\n",
    "            for k, (img, label) in enumerate(self.val_loader):\n",
    "                img, label = img.cuda(), label.cuda()\n",
    "                pred = self.model(img)\n",
    "                acc = self.get_accuracy(pred, label)\n",
    "                ep_acc.extend(acc)\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        return  torch.stack(ep_acc).mean() * 100.\n",
    "    \n",
    "\n",
    "    def get_accuracy(self, input, target):\n",
    "        inp_argmax = input.argmax(axis=1)\n",
    "        acc = inp_argmax == target\n",
    "        acc = acc.to(torch.float32)\n",
    "        \n",
    "        return acc\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_classifier = ClassificationTrainer(ebm, train_loader=train_loader, val_loader=val_loader,\n",
    "                                      epochs=1000, eval_epochs=20, savepath=\"/mnt/dl/generation/ebm/classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.sum() for p in ebm.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.sum() for p in ebm.parameters()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
