{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env_dl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.linalg as sl\n",
    "from PIL import Image\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, RandomHorizontalFlip, RandomVerticalFlip, ToTensor, Normalize\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "# torch.set_default_device(\"cuda\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/mnt/dl/datasets/Oxford102FlowersSplits/\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "LABELS = {i: k.strip() for i, k in enumerate(open(os.path.join(DATASET_PATH, \"names.txt\")))}\n",
    "img_size = 64\n",
    "batch_size = 32\n",
    "num_classes = len(LABELS)\n",
    "patch_size = 16\n",
    "num_patches = img_size ** 2 / patch_size **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerDataset(Dataset):\n",
    "    def __init__(self, path, split, cache=True, transforms=None):\n",
    "        super().__init__()\n",
    "        self.load_data(path, split)\n",
    "        self.samples = dict()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def load_data(self, path, split):\n",
    "        path = os.path.join(path, split, )\n",
    "        img_files = os.listdir(os.path.join(path, \"jpeg\"))\n",
    "        img_files = sorted(img_files, key=lambda x: int(x.replace(\".jpeg\", \"\")))\n",
    "        img_files = list(img_files)\n",
    "        \n",
    "        labels = list(open(os.path.join(path, \"label\", \"label.txt\"),))\n",
    "        self.labels = [int(l.strip()) for l in labels]\n",
    "        \n",
    "        self.img_files = [os.path.join(path, \"jpeg\", name) for name in img_files]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index not in self.samples:\n",
    "            self.load_sample(index)\n",
    "        sample = self.samples[index]\n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "\n",
    "        return sample\n",
    "        \n",
    "    def load_sample(self, idx):\n",
    "        img = Image.open(self.img_files[idx])\n",
    "        img = np.array(img).astype(np.float32)\n",
    "        self.samples[idx] = img\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FlowerDataset(DATASET_PATH, \"train\", transforms=Compose([\n",
    "    ToTensor(),\n",
    "    Resize((img_size, img_size)),\n",
    "    RandomHorizontalFlip(0.1),\n",
    "    RandomVerticalFlip(0.),\n",
    "    # Normalize(0., 255.),\n",
    "]))\n",
    "val_ds = FlowerDataset(DATASET_PATH, \"validation\", transforms=Compose([\n",
    "    ToTensor(),\n",
    "    Resize((img_size, img_size)),\n",
    "    # Normalize(0., 255.),\n",
    "]))\n",
    "\n",
    "test_ds = FlowerDataset(DATASET_PATH, \"test\", transforms=Compose([\n",
    "    ToTensor(),\n",
    "    Resize((img_size, img_size)),\n",
    "    # Normalize(0., 255.),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env_dl/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "img = train_ds[0].view((1, 3, img_size, img_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_logp(x, mean, log_std):\n",
    "    return -0.5 * np.log(2 * np.pi) - log_std - 0.5 * (x - mean).square() / (2*log_std).exp()\n",
    "\n",
    "def gaussian_sample(mean, log_std, eps=None):\n",
    "    if eps is None:\n",
    "        eps = torch.randn(log_std.size()).to(mean.device)\n",
    "    return mean + log_std.exp() * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones((1, in_channel, 1, 1)))\n",
    "        self.bias =  nn.Parameter(torch.zeros((1, in_channel, 1, 1)))   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.scale * x + self.bias\n",
    "        h, w = x.shape[-2:]\n",
    "        log_det = h * w * self.scale.abs().log().sum() \n",
    "        return z, log_det\n",
    "\n",
    "    def reverse(self, z):\n",
    "        x = z - self.bias\n",
    "        x = x / self.scale\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvConvLU(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channel):\n",
    "        super().__init__()\n",
    "        self.build_weights(in_channel)\n",
    "    \n",
    "    def build_weights(self, in_channel):\n",
    "        weights = np.random.normal(size=(in_channel, in_channel)).astype(np.float32)\n",
    "        q, *_ = sl.qr(weights)\n",
    "        P, L, U = sl.lu(q)\n",
    "        l_mask = torch.tril(torch.ones((in_channel, in_channel)), -1)\n",
    "\n",
    "        self.register_buffer(\"P\", torch.tensor(P, dtype=torch.float32))\n",
    "        self.register_buffer(\"l_mask\", l_mask)\n",
    "        self.register_buffer(\"u_mask\", l_mask.transpose(0, 1))\n",
    "        self.register_buffer(\"diag_mask\", torch.eye(in_channel))\n",
    "\n",
    "        self.diag = nn.Parameter(torch.tensor(np.diag(U), dtype=torch.float32))\n",
    "        self.L = nn.Parameter(torch.tensor(L, dtype=torch.float32))\n",
    "        self.U = nn.Parameter(torch.tensor(U, dtype=torch.float32))\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.P @ (self.L * self.l_mask + self.diag_mask) @ (torch.diag(self.diag) + self.U * self.u_mask)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        W = self.get_weights()\n",
    "        # det = torch.linalg.det(W)\n",
    "        # c =  W.size(0)\n",
    "        # W = W.view((c, c, 1, 1))\n",
    "        # z = F.conv2d(x, W, padding=0)\n",
    "        # # TODO use torch.slogdet\n",
    "        # log_det = h * w * det.abs().log() \n",
    "        \n",
    "        # New z = W x\n",
    "        z = torch.einsum(\"bchw, dc -> bdhw\", x, W)\n",
    "        log_det = h * w * self.diag.abs().log().sum()\n",
    "        # new_x = self.reverse(z)\n",
    "        \n",
    "        return z, log_det\n",
    "        \n",
    "    def reverse(self, z):\n",
    "        W = self.get_weights()\n",
    "        # c =  W.size(0)\n",
    "        W_inv = torch.linalg.inv(W)\n",
    "        x = torch.einsum(\"bdhw, dc -> bchw\", z, W)\n",
    "        # W = W.view((c, c, 1, 1))\n",
    "        # x =  F.conv2d(z, W, )\n",
    "        return x\n",
    "# torch.manual_seed(0)\n",
    "# np.random.seed(0)\n",
    "# InvConvLU(3)(torch.randn((2, 3, 4, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCouplingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channel, filters):\n",
    "        super().__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Conv2d(in_channel // 2, filters, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filters, filters, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filters, in_channel, 3, padding=1),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            for i, layer in enumerate(self.modules()):\n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    layer.weight.data.copy_(torch.normal(-0.005, 0.005, layer.weight.size()))\n",
    "                    layer.bias.data.copy_(torch.zeros_like(layer.bias))\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        xa, xb = x.chunk(2, 1)\n",
    "        flow_param = self.nn(xb)\n",
    "        log_s, t = flow_param.chunk(2, 1)\n",
    "        s = log_s.exp()\n",
    "        za = xa * s + t\n",
    "        zb = xb\n",
    "        z = torch.cat([za, zb], 1)\n",
    "        assert z.size() == x.size() \n",
    "        log_det = s.abs().log().reshape((x.size(0), -1)).sum(-1)\n",
    "        return z, log_det\n",
    "        \n",
    "    def reverse(self, z):\n",
    "        za, zb = z.chunk(2, 1)\n",
    "        flow_param = self.nn(zb)\n",
    "        log_s, t = flow_param.chunk(2, 1)\n",
    "        s = log_s.exp()\n",
    "        xa = (za - t) / s\n",
    "        xb = zb\n",
    "        x = torch.concatenate([xa, xb], dim=1)\n",
    "        return x\n",
    "    \n",
    "# AffineCouplingLayer(3, 512)(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Squeeze(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bz, c, h, w = x.size()\n",
    "        x = x.permute((0, 2, 3, 1))\n",
    "        sqz_x = x.view((bz, h//2, 2, w//2, 2, c)).contiguous()\n",
    "        sqz_x = sqz_x.permute((0, 1, 3, 2, 4, 5))\n",
    "        sqz_x = sqz_x.reshape( sqz_x.shape[:-3] + (4*c,) )\n",
    "        sqz_x = sqz_x.permute((0, 3, 1, 2))\n",
    "        return sqz_x\n",
    "\n",
    "    def reverse(self, x):\n",
    "        x = x.permute((0, 2, 3, 1))\n",
    "        bz, h, w, c = x.size()\n",
    "        unsqz_x = x.view((bz, h, w, 2, 2, c // 4))\n",
    "        unsqz_x = unsqz_x.permute((0, 1, 3, 2, 4, 5))\n",
    "        unsqz_x = unsqz_x.reshape((bz, 2*h, 2*w, unsqz_x.shape[-1]))\n",
    "        unsqz_x = unsqz_x.permute((0, 3, 1, 2))\n",
    "        return unsqz_x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowModule(nn.Module):\n",
    "    def __init__(self, in_channel, filters):\n",
    "        super().__init__()\n",
    "        self.act_norm = ActNorm(in_channel)\n",
    "        self.inv_conv = InvConvLU(in_channel)\n",
    "        self.affine_coupling = AffineCouplingLayer(in_channel, filters)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z, log_det1 = self.act_norm(x)\n",
    "        z, log_det2 = self.inv_conv(z)\n",
    "        z, log_det3 = self.affine_coupling(z)\n",
    "        return z, log_det1 + log_det2 + log_det3\n",
    "    \n",
    "    def reverse(self, z):\n",
    "        x = self.affine_coupling.reverse(z)\n",
    "        x = self.inv_conv.reverse(x)\n",
    "        x = self.act_norm.reverse(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(nn.Module):\n",
    "    \n",
    "    def __init__(self,  in_channel, out_channel, split=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self._split = split\n",
    "        self.conv_net = nn.Conv2d(in_channel, out_channel, 3, padding=\"same\")\n",
    "        with torch.no_grad():\n",
    "            # self.conv_net.weight.data.copy_(torch.normal(-0.005, 0.005, self.conv_net.weight.size()))\n",
    "            self.conv_net.weight.data.copy_(torch.zeros_like(self.conv_net.weight))\n",
    "            self.conv_net.bias.data.copy_(torch.zeros_like(self.conv_net.bias))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        bz = x.size(0)\n",
    "        if self._split:\n",
    "            out, z = x.chunk(2, 1)\n",
    "            mean, log_std = self.conv_net(out).chunk(2, 1)\n",
    "        else:\n",
    "            z, out = x, x\n",
    "            zeros = torch.zeros_like(x, device=x.device, dtype=x.dtype)\n",
    "            mean, log_std =  self.conv_net(zeros).chunk(2, 1)\n",
    "            \n",
    "        log_p = gaussian_logp(z, mean, log_std)\n",
    "        log_p = log_p.reshape((bz, -1)).sum(-1)\n",
    "        return out, z, log_p\n",
    "    \n",
    "    def reverse(self, out):\n",
    "        if self._split:\n",
    "            mean, log_std = self.conv_net(out).chunk(2, 1)\n",
    "            z = gaussian_sample(mean, log_std)\n",
    "            z = torch.cat([out, z], axis=1)\n",
    "        else:\n",
    "            zeros = torch.zeros_like(out).to(out.device)\n",
    "            mean, log_std = self.conv_net(zeros).chunk(2, 1)\n",
    "            # TODO\n",
    "            z = gaussian_sample(mean, log_std)\n",
    "        return z\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_flows, in_channel, filters=512, split=True):\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.num_flows = num_flows\n",
    "        self.in_channel = in_channel\n",
    "        self._split = split\n",
    "        self.squeeze= Squeeze()\n",
    "        self.flow_step = nn.ModuleList([\n",
    "            FlowModule(in_channel * 4, filters) for i in range(num_flows)])\n",
    "        self.split = Split(in_channel * 2 if self._split else in_channel * 4, \n",
    "                           in_channel * 4 if self._split else in_channel * 8, \n",
    "                           self._split) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.squeeze(x)\n",
    "        log_det = 0.\n",
    "        for i in range(self.num_flows):\n",
    "            out, flow_log_det = self.flow_step[i](out)\n",
    "            log_det = log_det + flow_log_det\n",
    "        out, z, log_p = self.split(out)\n",
    "        return out, z, log_p, log_det\n",
    "\n",
    "    def reverse(self, z):\n",
    "        z = self.split.reverse(z)\n",
    "        for flow_fn in self.flow_step[::-1]:\n",
    "            z = flow_fn.reverse(z)\n",
    "        z = self.squeeze.reverse(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glow(nn.Module):\n",
    "    def __init__(self, num_flows, num_blocks):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_flows = num_flows\n",
    "\n",
    "        channels = [np.power(2, i) * 3 for i in range(self.num_blocks)]\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(num_flows, channels[i], split=(i < self.num_blocks - 1)) \n",
    "            for i in range(self.num_blocks)\n",
    "            ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x \n",
    "        z_list = []\n",
    "        \n",
    "        log_det = 0.\n",
    "        log_p = 0.\n",
    "        \n",
    "        for block_fn in self.blocks:\n",
    "            out, z, block_log_p, block_log_det = block_fn(out)\n",
    "            log_p = log_p + block_log_p\n",
    "            log_det = log_det + block_log_det\n",
    "            z_list.append(z)\n",
    "\n",
    "        return out, z_list, log_p, log_det\n",
    "    \n",
    "    def reverse(self, z):\n",
    "        for i in reversed(range(self.num_blocks)):\n",
    "            z = self.blocks[i].reverse(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Dequantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorical(probs: torch.Size([9]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(52160)\n",
    "p = torch.empty((9,)).uniform_(-10, 10.)\n",
    "p[6] = 12\n",
    "p = p.exp() / p.exp().sum()\n",
    "cat = torch.distributions.Categorical(p)\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0953e-04, 3.0426e-05, 2.3248e-02, 7.2751e-07, 2.3572e-03, 1.0028e-03,\n",
       "        9.4462e-01, 8.7241e-03, 1.9808e-02])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 9 artists>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(len(p)), cat.probs, width=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = cat.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_unif = torch.distributions.Uniform(0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_samples = prior_unif.sample((100, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7932, 0.3263, 0.6868, 0.3985, 0.5662, 0.8730, 0.4212, 0.4010, 0.8613,\n",
       "        0.7082, 0.6234, 0.8298, 0.0427, 0.6418, 0.3125, 0.5551, 0.4838, 0.1427,\n",
       "        0.3227, 0.5601, 0.5588, 0.8902, 0.7768, 0.5416, 0.4552, 0.7620, 0.4774,\n",
       "        0.6018, 0.5677, 0.1575, 0.9263, 0.0632, 0.1470, 0.3061, 0.9333, 0.9293,\n",
       "        0.8432, 0.4871, 0.0163, 0.8957, 0.4081, 0.8811, 0.9030, 0.3579, 0.6751,\n",
       "        0.7747, 0.1295, 0.4308, 0.9556, 0.4329, 0.4428, 0.0304, 0.4744, 0.3321,\n",
       "        0.8374, 0.3871, 0.4276, 0.9798, 0.7708, 0.2186, 0.8286, 0.2185, 0.5796,\n",
       "        0.4996, 0.7953, 0.0362, 0.2441, 0.4488, 0.7355, 0.8618, 0.5947, 0.7233,\n",
       "        0.1673, 0.0616, 0.6808, 0.8702, 0.6077, 0.2176, 0.0839, 0.2297, 0.7050,\n",
       "        0.1763, 0.3916, 0.4180, 0.3362, 0.1391, 0.1202, 0.9193, 0.4544, 0.0733,\n",
       "        0.3431, 0.7541, 0.2225, 0.2909, 0.6722, 0.7381, 0.6580, 0.5017, 0.5327,\n",
       "        0.0860])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_samples_prob = prior_unif.log_prob(prior_samples).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_samples_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(len(z)):\n",
    "    ax.scatter(i + prior_samples, z[i] + prior_samples_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = cat.sample((10000,))\n",
    "u = prior_unif.sample((10000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp, yp = z + u, p.index_select(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd834934a30>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(xp, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorical(probs: torch.Size([10]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(52160)\n",
    "p = torch.zeros(10, dtype=int)\n",
    "p[8] = 1\n",
    "cat = torch.distributions.Categorical(p)\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.6793)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_x = x + prior_unif.sample()\n",
    "u_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0339)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_x / 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd6b9f929b0>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = torch.linspace(0, 1., 50)\n",
    "plt.plot(m, torch.sigmoid(m),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd6b9e3e7a0>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqQAAAHTCAYAAADxiQpNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTVElEQVR4nO3deXwU9f3H8dfsbu47JAQIR7ghByEEAogXagVbJWhFAdFStdjqjwqUKm2xWkvtoWjBo9XacrSIqCiKFzXWeiJ3IAQCJAQCISSBJITNnez+/gikRFCyGDK7yfv5eOSRMjvJvrcfF9/OznzHcDqdTkRERERETGIxO4CIiIiIdGwqpCIiIiJiKhVSERERETGVCqmIiIiImEqFVERERERMpUIqIiIiIqZSIRURERERU6mQioiIiIipVEhFRERExFQ2swN8G8XFJ9vsuSwWg/DwAEpKKnA4dHMrT6QZej7N0LNpfp5PM/R8ZswwMjLovPvoCGkLWSwGhmFgsRhmR5ELpBl6Ps3Qs2l+nk8z9HzuOkMVUhERERExlQqpiIiIiJhKhVRERERETKVCKiIiIiKmUiEVEREREVOpkIqIiIiIqVRIRURERMRUKqQiIiIiYioVUhERERExlQqpiIiIiJhKhVRERERETKVCKiIiIiKmUiEVEREREVO5XEjz8/OZMWMGI0eOZOzYsTz++OM4HI5z7rty5UrGjRtHUlISqamppKWlNT02b948YmNjSUhIaPoaPnz4hb8SEREREfFINld/YObMmcTFxZGWlsbx48e55557iIiI4Ic//GGz/datW8fChQt5/vnnGTJkCGvWrGHWrFm899579OjRA4Cf/OQnzJw5s3VeiYiIiIh4JJeOkGZkZJCVlcXcuXMJCgoiJiaG6dOns2rVqrP2ra6uZs6cOSQnJ+Pl5cWkSZMICAggPT29tbK3qfoGB7lHTnC42E7B8QqKSis5dqKK0pM1lFfUYq+qo6qmnpq6BuobHDicTrMji4iIiHgEl46QZmZmEh0dTUhISNO2uLg4cnNzsdvtBAYGNm1PTU1t9rPl5eVUVFQQFRXVtO3LL7/kww8/5ODBg/Tt25dHHnmE+Pj4FuexWAwsFsOVl3BBHE4nv35xA3mFdpd+zstmwdfbiq+37dT3U//bx4qvlxVfH9up71b8fGwE+XsT5O9F8Knvgf5eWC06zbe1WK2WZt/F82iGnk3z83yaoedz1xm6VEjLysoIDg5utu10OS0tLW1WSM/kdDqZP38+iYmJpKSkANCjRw8sFgv3338/AQEBPPPMM9x5552sW7eOsLCwFuUJDw/AMNqgkDqcNDhcP+JZV++grt7Bycq6C3pew4BAPy+CA3wICfQmOMCbkEAfQgJ9CA/2JTLUj8gwPyJC/Qj082qT/y/ag+BgP7MjyLekGXo2zc/zaYaez91m6PI5pE4XP4quq6tj3rx5ZGdns3z58qbt9913X7P9fv7zn/P222+TlpbGpEmTWvS7S0oq2uQIKcCjd4+k8EQNJ09WU1fvoMHhaCqqp783NJz6s9NJQ4ODmroGqmtPfdXUU13XQHVNA9W19VTXNlBT20BVbT01tQ3U1p99YZjTCScr6zhZWUd+8Tfn8/ay0CnYl/Bg31PffZr+HBHaWF5tbvZfQ23NarUQHOxHeXkVDQ3nvhBP3Jtm6Nk0P8+nGXo+M2YYFhZw3n1cKqTh4eGUlZU121ZWVoZhGISHh5+1f3V1Nffeey9VVVWsWLHiG498Wq1WunbtSlFRUYvzOE6VwbbgZbOQ0DeC0tIK6s9RHr+tuvoGTlbWYa+qO1VCaxu/VzV+t5/eVlVHeUUtFdX1zX6+ts5BwfFKCo5XnvP3WwyDiBBfOof70SXMn6hwf6LC/YgK86dTsG+bFXt30NDguCgzlLajGXo2zc/zaYaez91m6FIhjY+Pp6CggJKSkqYCmpGRQb9+/QgIaN5+nU4ns2fPxmazsXTpUnx8fJo99oc//IEbb7yRQYMGAVBbW0teXl7TFfgdjZfNSniwlfBg3xbtX1PXQOnJGkrKqzleXk1peQ0lJ6spKa+h5GQNx8urqaltaNrf4XRSVFZFUVkVOylp9rtsVoPI0MZy2iXcn24RAXTvHEC3TgF4e1lb9XWKiIiIfJVLhfT0uqELFy7kF7/4BYWFhSxZsoQ777wTgPHjx7NgwQKGDx/O2rVryc7O5q233mpWRgEMw+Dw4cP85je/4c9//jOBgYEsWrQILy8vrrnmmtZ7de2Yj5eVLuGNBfJcnE4nVTX1lJTXUFxWRWFpFYWllRSWVFJY2rg6wGn1Dc5zHl01DOgS7k/3yEC6RwbQvXMgPSID6RTiq/NVRUREpNW4fA7p4sWLeeihhxgzZgyBgYFMnjyZqVOnApCbm0tlZWOpWb16Nfn5+U0XMZ2WmprKggUL+N3vfscf//hHbrrpJux2O0OGDGHZsmX4+5+7YIlrDMPA39cLf18vunc++2KzmtqGxoJaWtVYUk8V1YLjFU2nAzidNBXVTVn/+1k/HyvREYGNBbVzIH26BhMdGdDhz1EVERGRC2M4Xb1KyY0UF59ss+ey2SyEhQVctHNI3YXT6eRERS2Hi+0cLqrgUJGd/GI7R45XUN/w9f+oeNks9IwKpHfXYPp0DaZ3t2A6h/q51ZHUjjLD9kwz9Gyan+fTDD2fGTOMjAw67z4uHyGV9s0wDEIDfQgN9CG+d6em7fUNDgpLKjlUbCe/uLGoHi62U1Le+NF/Xb2DnPxycvLLm34mwNdG767BjV/dGr+HBHi3+WsSERER96ZCKi1is1qIjgwkOrL5x//llbUcKChn/5FycgtOkltQjr2qcd3Viup6duaWsDP3fxdRdQ71Y0CPUAb2DGVAj1AidD6qiIhIh6dCKt9KsL83Q/pGMKRvBND4kX/xiWpyj5STW1DO/oJyDh49Sd2pjwVOX+n/WUYBAGFBPgzsEcqAnqEM7BFKl3B/FVQREZEORoVUWpVhGHQO9aNzqB8jYxtvE1vf4ODIsQpyjpSz71AZew6VNV3lX3qyhi93FfLlrkIAgvy9GNCj8ejpoJ5hdI9sm7txiYiIiHlUSOWis1kt9IwKomdUEGOTonE6nRw7Uc3eU+V0b14ZRWVVQOOdqbbsKWbLnsZbUwUHeBMXE0Z8707E9g7XOagiIiLtkAqptDnDaFyIPzLUjzEJXYHGI6V7D5U1feUfqwCgvKKW9ZmFrM9sPILao3Mg8b3DiesdTv/uIXjZtHC/iIiIp1MhFbcQFuTDyNiopo/5yytryTpYSuapi6JOf8R/qMjOoSI7723Iw9tmYUDPUOJ7dyKudzjdOun8UxEREU+kQipuKdjfm5TBUaQMjsLpbLyT1M7cEjJzS9iTV0ptvYPaegc795ewc3/jVfydgn0Z2j+CpP4RDOgRqoX6RUREPIQKqbg9wzDoFhFAt4gArh3Rg7r6BvYdPkHmqYKaV2QH4Hh5NR9uOcyHWw7j52NjSN9ODO0XQUKfcPx9vUx+FSIiIvJ1VEjF43jZrMTGhBMbE86ksXDCXsPO3BLSs4+xc38JNXUNVNXUs2FXIRt2FWK1GAzsGcqwgZGMHd4LL32qLyIi4lZ069AW0u3SPENdfQO7D5aRnn2M9H3FlNlrz9qnZ1QgQ/tFMHxQZ7p/ZaF/cW96H3o2zc/zaYaez11vHapC2kJ6E3oep9PJgaMnSd93jG37jnG42H7WPt0iAhgxqDMjBnWmW0SACSnFFXofejbNz/Nphp5PhfQiUCEVV5Taa9hzuJzP0vPJOliK4yv/6HePbCynKYOjiAr3NymlfBO9Dz2b5uf5NEPP566FVOeQSocRGerHgN4RXBofRUl5NVv3FrNpdxFZeaU4nXC4uILDxbm88WkuPTsHMmJwZ0YMjqJzqJ/Z0UVERNo1FVLpkIL9vblyaDRXDo3mREUtW/cUsXF3EXsPleEE8ors5BXZWf3xfmK6BDEyNopRsVGEBPqYHV1ERKTdUSGVDi8kwJuxw7ozdlh3yuw1bNlTzMbdhew7fAKAA0dPcuDoSV79KIf4PuFcEt+Fof0i8PbSXaJERERagwqpyBlCA324Ork7Vyd3p/RkDZuzitiwu5D9R8pxOJ3syDnOjpzj+PnYGDGoM5fEd6F/9xDdIUpERORbUCEV+RphQT58Z0QPvjOiBwXHK1ifeZT1O49yvLyGqpp6Ptl+hE+2HyEy1JdL4rsyOr6LzjcVERG5ALrKvoV0ZaHna40ZOpxO9uSV8cXOAjbvKaamtqHZ4wO6h3BJQldGDOqMn4/+e6+16X3o2TQ/z6cZej5dZS/SDlgMg8G9whjcK4xp32lg695ivthZwK4DpTiBvYdPsPfwCVZ+uI/RsVFcMTSaXl3O/0YUERHpyFRIRS6Qj7eV0fFdGB3fhZLyar7cVcjnGQUUHK+kpraB/6Yf4b/pR+jVJYgrhnZj5OAoHTUVERE5B31k30L6mMLztcUMnU4nOfnlfJyez8asIurOeB4fbyujYqO4Ymg3YroEX5Tnb+/0PvRsmp/n0ww9nz6yF+kADMOgX/cQ+nUPYfI1/fkys5D/pueTX1xBTW0DH6cf4eP0I/SKOnXUNFZHTUVERPRvQpGLJMDXi6uTu3PVsGhyjjQeNd20u4jaegcHC0+yfN0eVv0nm9FxUVyd3J3oyECzI4uIiJhChVTkIjMMg37RIfSLDmHK1f1Zn1nIx+n5HC6uoKbuf+eaDu4VxjXDu5PYNwKLReuaiohIx6FCKtKG/M84arq/oJyPtuazcXch9Q1Odh8sZffBUiJDfbl6WHcuHdIVf18vsyOLiIhcdCqkIiYwDIO+3ULo2y2EW8b24+P0fP6zLZ8T9lqKy6p5+T/ZvPFpLpfEd+Hq5O50iwgwO7KIiMhFo0IqYrLgAG9uGNOb60b1YsueYtK2HCInv5yaugY+2pbPR9vyiYsJ4+rhPRjStxMW3aZURETaGRVSETdhs1oYGRvFyNgocgvKSdt8mI27C2lwOMk8UErmgVI6h/pxbUoPxiR0xcfLanZkERGRVqF1SFtIa695Pk+c4Ql7DR+nH+GjbfmcqKht2h7o58U1yd25Krk7gX4d5zxTT5yh/I/m5/k0Q8+ndUhFxGUhgT5MuLQ33x3di01ZRazbmEdeoR17VR1rPsvl3Q0HuSyhG9em9CAy1M/suCIiIhdEhVTEA9isFkbHdWFUbBS7Dpby/oY8MnNLqK1z8OHWw/xn22FGDOrM+JE9dRcoERHxOCqkIh7EMAziYsKJiwknr/Ak72/IY+PuIhxOJxt3F7FxdxGDe4Vx3aiexMWEY+gCKBER8QAqpCIeqmdUEDMmxHHTFX3496ZDfLL9CLV1jqb1THt0DuS6UT1JGRSlhfZFRMStWcwOICLfTkSIH1OvGcAT947hxsv7EOzfeJHToSI7L7y1i1+9uIHPMwpocOgCBBERcU86QirSTgT6eXHDJTGMG9GDL3Ye5f0NeRSVVVFYUsnf39nNW5/n8r3RMVwS3wWbVf8tKiIi7kOFVKSd8faycmVSNJcldmXjriLeXn+AguOVFJdVs/S9LNZ+nst1o3px2ZCueNm0lqmIiJhPhVSknbJaLIyO78LI2Cg27yli7RcHyC+u4Hh5Df/6917e/uIA143sxeVDu2mRfRERMZUKqUg7Z7EYpAyOYvigzmzbe4y1X+SSV2inzF7Lyg/38c76A4wb2ZOxSdH4euuvBBERaXv6t49IB2ExDJIHRjJsQATbc46z9vMD5BaUU15Zx6sf5fDel3lcN6onVw3rriOmIiLSplRIRToYwzAY2i+CxL6dyDxQwlufHyD78AnsVY3F9N+bDnH96BiuGNpNFz+JiEibUCEV6aAMwyC+dyfiYsLJOljKG5/mkp1/ghP2WlZ8sJf3N+Qx4dLGq/KtFhVTERG5eFRIRTo4wzAYHBPOoF5hZOwv4Y1P9nOw8CTHy6tZ8m4W736Zx8RLezNicGcsuvOTiIhcBCqkIgI0FtMhfTuR0CecLXuKeePT/RQcr6SwpJLn38rknfUHufHy3gztF6FbkoqISKtSIRWRZgzDYPigzgwbEMmXu47y5me5FJdVc7jYztOrM+jdNZibruhDbK8wFVMREWkVKqQick4Wi8El8V1JGRzFZxkFrP38AKUna8gtKGfhy+kM7hXGpLF9iekSbHZUERHxcLpSQUS+kc1q4cqh0fzhnlFMvro/Qf5eAOw+WMqjSzfzwluZFJdVmZxSREQ8mY6QikiLeNmsXDuiB5cnduWDTYd4d0MeNbUNfLmrkM17irhqWHeuvySGQD8vs6OKiIiH0RFSEXGJr7eNG8b05o/3jOaqYdFYLQb1DU7+vekQ8/66nvc2HKSuvsHsmCIi4kFUSEXkggQHeDPt2oH89u6RJA+MBKCypp5XP8rhly98yRc7C3A4nSanFBERT6BCKiLfSpdwf+67MYFfTkumX3QIAMfLa3jx7d08umQTmQdKTE4oIiLuToVURFpFv+4h/GLaMO67MYGocH8A8orsLHw5nSdXpXO42G5yQhERcVe6qElEWo1hGCQPjCSxXyc+3X6ENz/Lpbyyjp25Jez6xyauTOrGxMv66MInERFpRoVURFqdzWph7LDujIrrwvsb8nh/Yx519Q7+szWfDbsKSb20N1cmRWOz6kMaERHRR/YichH5+di48fI+/O5HI0kZ3BmAiup6XkrbxyNLNrEz97jJCUVExB2okIrIRRcR4sePU+OZd9swekUFAXDkWAVPrtrO4td2UFhSaXJCERExkwqpiLSZAT1CeegHw/nhdYMIPnXHp/TsY8x/cQOv/Cebyup6kxOKiIgZVEhFpE1ZLAaXJXbj9/eM5rqRPbFaDBocTt7fmMcvX1jPJ9uP4HBo/VIRkY5EhVRETOHnY2PS2H4s+NFIkvpHAFBeWcfS97L47bLN5Bw5YXJCERFpKyqkImKqqDB/Zn5/CD+bPJToiAAADhae5LHlW1j6Xhb2qjqTE4qIyMWmQioibiEuJpxH7hzBlGv64+djxQl8sv0Iv3h+PR+n5+s2pCIi7ZjLhTQ/P58ZM2YwcuRIxo4dy+OPP47D4TjnvitXrmTcuHEkJSWRmppKWlraOffLzMwkNjaW119/3dU4ItKOWC0WvjO8B4/9aBSj46KAxmWilr2/h98t30JuQbnJCUVE5GJwuZDOnDmTqKgo0tLSWLJkCWlpaSxbtuys/datW8fChQt57LHH2LhxI9OmTWPWrFkcOnSo2X4Oh4OHH34Yf3//C38VItKuhAT68KMb4nhwalLTx/i5BeU88veN/GX1dir0Mb6ISLviUiHNyMggKyuLuXPnEhQURExMDNOnT2fVqlVn7VtdXc2cOXNITk7Gy8uLSZMmERAQQHp6erP9Vq5cSVBQEIMHD/5WL0RE2p+BPcN4+IcjuGVsP3y8Gz/Gf/eLAzzwly/4bEeBPsYXEWknXLp1aGZmJtHR0YSEhDRti4uLIzc3F7vdTmBgYNP21NTUZj9bXl5ORUUFUVFRTduKi4t59tln+de//sXDDz/scniLxcBiMVz+uQthPXWLQ6tudeixNEPPZLNZuH5MDJckdOHlD7P5MvMoJyvr+Me7u/l0xxF+cN0gep5abF/cm96Dnk8z9HzuOkOXCmlZWRnBwcHNtp0up6Wlpc0K6ZmcTifz588nMTGRlJSUpu2///3vmTRpEn369HE1NwDh4QEYRtsU0tOCg/3a9Pmk9WmGniksLIBf3dmJ7XuL+esbOzhcZGff4RP8+sUNTLi8L7eNG4Svj0t/pYlJ9B70fJqh53O3Gbr8t7fTxY/I6urqmDdvHtnZ2Sxfvrxp++eff056ejqPPfaYqxGalJRUtOkR0uBgP8rLq2hoOPdFXOLeNEPPZ7VaSBwQye9mjOLtzw/w5mf7qa1zsObjHD5Lz+cH1w0isV+E2THla+g96Pk0Q89nxgzDwgLOu49LhTQ8PJyysrJm28rKyjAMg/Dw8LP2r66u5t5776WqqooVK1YQFhYGQG1tLY8++ii//vWv8fX1dSVCMw6Hs83v6NLQ4KC+Xm9CT6YZej4DuG5kT0YMiuRf/97LjpzjHDtRzcKX00kZ3Jkp1wwgJMDb7JjyNfQe9Hyaoedztxm6VEjj4+MpKCigpKSkqYBmZGTQr18/AgKat1+n08ns2bOx2WwsXboUHx+fpsfS09M5ePAgDz74YNM2u93Ozp07+eCDD/jLX/7ybV6TiHQQESF+3H/zEDbvKealD/ZyoqKWjbuL2Lm/hFuu6selQ7piaePTekRExHUuFdLY2FgSEhJYuHAhv/jFLygsLGTJkiXceeedAIwfP54FCxYwfPhw1q5dS3Z2Nm+99VazMgowdOhQ/vvf/zbbdv/993PdddcxYcKEb/eKRKRDMQyDEYM6ExcTxmv/zeG/6UeorKln6XtZfLHzKD8YP5Cunc7/cZGIiJjH5XNIFy9ezEMPPcSYMWMIDAxk8uTJTJ06FYDc3FwqKysBWL16Nfn5+c0uYoLGq+8XLFhAly5dmm339vYmODj4nB/9i4icj7+vF3eMH8SouC4sez+LguOV7D1UxsP/2Mj1o2O4blQvvGzudVWpiIg0MpyuXqXkRoqLT7bZc9lsFsLCAigtrXCrcy6k5TRDz9fSGdbVO3hvw0He/uIA9Q2Nf8V17eTPD8YPYkCP0DZKK1+l96Dn0ww9nxkzjIw8/9J8OlwgIu2Ol83ChDG9+c2dKQw8VUALjlfyhxVbWf5+FlU19eYGFBGRZlRIRaTd6topgAemJvHD6wYR4Nt4htJ/048w/8UNZOw/bnI6ERE5TYVURNo1wzC4LLEbv/vRKFIGdwag9GQNT72ynb+/s4uK6jqTE4qIiAqpiHQIwQHe/Dg1nvtuTCD41Bqln2ccZf6LG9i2r9jkdCIiHZsKqYh0KMkDI1lw90hGxzWu9HHCXsvTqzN44a1MTlbWmpxORKRjUiEVkQ4n0M+LH90Qy/03DyE0sPFo6Ze7CnnoxQ1szioyOZ2ISMejQioiHVZivwgW3D2Sy4Z0BaC8so7n1uzk2TcyOFGho6UiIm1FhVREOjR/Xy9++N3B/OzWoXQKbryr3JY9xcz/25eszzyKBy/VLCLiMVRIRUSAuN7hPHrXSMYOiwagorqev63dxbNv7KRc55aKiFxUKqQiIqf4+di4/dqBPDAlichQXwC27i3moRc3sHWvrsQXEblYVEhFRL5iUK8wfnNnCmOTGo+Wnqys45nXM/j727uorNZdnkREWpsKqYjIOfh627h93EDm3JpIWFDjuaWf7zzKr/+xgV0HSkxOJyLSvqiQioh8g/jenXj0rhRGxUUBUFJewxMvp7Pig73U1DWYnE5EpH1QIRUROY8AXy9m3BDHvRPjCfTzAuDDLYd5ZMkmco6cMDmdiIjnUyEVEWmh4YM689u7UhjaLwKAwpJKHvvnFl7/JIf6BofJ6UREPJcKqYiIC0ICfZj5/QR++N1B+HpbcTrh7S8OsmDZZg4X2c2OJyLikVRIRURcZBgGlw3pxqN3pjCoZygAeUV2Hl22mQ82HcKhxfRFRFyiQioicoEiQv2YOyWJKVf3x2a1UN/gYOWH+/jzK9sps9eYHU9ExGOokIqIfAsWw+A7I3rw8PThdI8MBGBnbgm//vtGtu3TYvoiIi2hQioi0gqiIwN56AfJXDuiBwD2qjqeXp3B8nV7tDyUiMh5qJCKiLQSL5uVyVf3Z86tiYQEeAPw3235PLp0EwePnjQ5nYiI+1IhFRFpZacX00/q37g8VMHxShYs38z7G/J0wZOIyDmokIqIXARB/t78300J/GD8QLy9LDQ4nLzyUTYLX06n9KQueBIROZMKqYjIRWIYBlcMjebh6SPo1SUIgN0HS/n13zewOavI5HQiIu5DhVRE5CLr2imAX92ezHdH9cIAKqrreW7NTpa9n0WtLngSEVEhFRFpCzarhZuv7MvPpyQRFuQDwMfpR/jt8s3kH6swOZ2IiLlUSEVE2tCgXmH85s7/XfCUX1zBb5du4pPtR3DqgicR6aBUSEVE2lignxf/d1MCU6/pj81qUFvvYOl7WTz/ViZVNfVmxxMRaXMqpCIiJjAMg2uG9+BXtw8nKswPgI27i3hkyUZyC8pNTici0rZUSEVETNSrSxC/nj6C0XFdACguq+axf27h3xvz9BG+iHQYKqQiIibz87Hxoxtiuet7g5vWLH35P9ksem0HJytrzY4nInLRqZCKiLiJMQldeXj6CHp0DgRgR85xHv7HRvbklZqcTETk4lIhFRFxI107BTD/jmSuGhYNQJm9lj+t3Mabn+XicOgjfBFpn1RIRUTcjJfNyrRrB3LfjQn4+9hwOuHNz3J56pV0yvURvoi0QyqkIiJuKnlgJI/cOYK+3YIByDxQym+WbGLf4TJzg4mItDIVUhERNxYR4seDtw3jO8N7AFB6soY/rtjG+xt0Fb6ItB8qpCIibs5mtTDlmv7cOzEePx8rDqeTVz7K5pnXM6isrjM7nojIt6ZCKiLiIYYP6syvz7gKf9u+YzyyZBMHjmohfRHxbCqkIiIeJCrMn1/dnszliV0BOHaicSH9j7bl6yN8EfFYKqQiIh7G28vK9OsGNy6kb7NQ3+Dkn+v28Le1u6iurTc7noiIy1RIRUQ81JiErsz/wXC6hPsD8OWuQn67bDP5xypMTiYi4hoVUhERD9Y9MpCHfjCclMGdASg4Xslvl23iy11HTU4mItJyKqQiIh7Oz8fGPRPiuP3aAdisBrV1Dl54axcvpe2lvsFhdjwRkfNSIRURaQcMw2DssO78Yloy4cE+AKRtPswTK7dxwl5jcjoRkW+mQioi0o707hrMr6ePYHCvMAD2Hj7BI0s3kX34hMnJRES+ngqpiEg7E+zvzZxbE7luVE8ATthr+eNLW/lwy2EtDSUibkmFVESkHbJaLEy6sh/33RiPj7eVBoeTFR/s5cW3d1FT12B2PBGRZlRIRUTaseSBnfn1D4bTtVPj0lDrMwt57J9bKCqrMjmZiMj/qJCKiLRzXTsFMP+O4SQPjATgUJGdR5dsYkfOMZOTiYg0UiEVEekA/Hxs3Dsxnklj+2IYUFlTz6JXd/DmZ7k4dF6piJhMhVREpIMwDIPrRvZi7q1DCfTzwgm8+VkuT7+2g8pq3XJURMyjQioi0sEMjgnnkR+OoHfXYAC25xxnwfLNFBzXLUdFxBwqpCIiHVB4sC/zbhvGZUO6AnC0pJIFyzfrvFIRMYUKqYhIB+VlszD9ukHc9p0BWAyDqpoGFr26g3fWH9B6pSLSplRIRUQ6MMMwuDq5O3Mn/++80tUf7+f5tzKpqdV6pSLSNlRIRUSEQb3C+PUPhtOjcyAAG3cX8di/tnBM65WKSBtQIRUREQAiQv345bRkRgzqDJxar3TZZrIOlpqcTETaOxVSERFp4uNt5cepcXz/ij4YgL2qjideTufDLYd1XqmIXDQqpCIi0oxhGHxvdAz3TxqCn48Nh9PJig/2suS9LOrqHWbHE5F2SIVURETOaUjfCObfkUyXcH8APttRwJ9e2kqZvcbkZCLS3qiQiojI1+raKYD5dwxnSN9OAOQcKee3yzZz8OhJk5OJSHviciHNz89nxowZjBw5krFjx/L444/jcJz7I5yVK1cybtw4kpKSSE1NJS0tremx2tpaFixYwKWXXkpSUhI33XQTH3/88YW/EhERuSj8fW389OYhfG90LwBKT9bw+xVb2JxVZHIyEWkvXC6kM2fOJCoqirS0NJYsWUJaWhrLli07a79169axcOFCHnvsMTZu3Mi0adOYNWsWhw4dAuDxxx9nx44dvPbaa2zatIkJEyYwc+ZMiouLv/2rEhGRVmUxDL5/RV9+dEMsNquF2joHz63ZydrPc3Wxk4h8ay4V0oyMDLKyspg7dy5BQUHExMQwffp0Vq1adda+1dXVzJkzh+TkZLy8vJg0aRIBAQGkp6cDMGrUKH73u9/RpUsXbDYbN998MzU1NeTl5bXKCxMRkdY3Oq4LD96WRHCANwBvfJrLC2t3UVunRfRF5MLZXNk5MzOT6OhoQkJCmrbFxcWRm5uL3W4nMDCwaXtqamqzny0vL6eiooKoqCgArr766qbH7HY7zz//PDExMcTFxbU4j8ViYLEYrryEC2a1Wpp9F8+jGXo+zdA9DOwZxm/uTOGpV9LJK7SzYVchxWVV3D8pkbAgn6/9Oc3P82mGns9dZ+hSIS0rKyM4OLjZttPltLS0tFkhPZPT6WT+/PkkJiaSkpLS7LE777yTzz//nIEDB/Lcc8/h6+vb4jzh4QEYRtsU0tOCg/3a9Pmk9WmGnk8zNF9YWAAL77+CJ1duZX1GAfuPlPPo0k3Mv3Mk/bqHfuPPan6eTzP0fO42Q5cKKeDyuUJ1dXXMmzeP7Oxsli9fftbj//jHP7Db7bz00ktMmzaNNWvWNB1FPZ+Skoo2PUIaHOxHeXkVDQ1ah88TaYaeTzN0P/dMiCUyxJe3Psvl+IlqHnz6U2akxpEy+Oy/xzU/z6cZej4zZhgWFnDefVwqpOHh4ZSVlTXbVlZWhmEYhIeHn7V/dXU19957L1VVVaxYsYKwsLBz/t7AwEBmzJjB6tWrefvtt7nrrrtalMfhcOJwtO3J9A0NDuq1MLRH0ww9n2boXiZe2psuYX78490sausdPLM6g4mX2rlhTMw5P8XS/DyfZuj53G2GLp1AEB8fT0FBASUlJU3bMjIy6NevHwEBzduv0+lk9uzZ2Gw2li5delYZnThxIh9++GHzMBYLNpvLB21FRMRko05d7BRy6mKnNZ/l8vxbmbrYSURaxKVCGhsbS0JCAgsXLsRut5OTk8OSJUuYMmUKAOPHj2fz5s0ArF27luzsbBYtWoSPz9knuScmJrJo0SLy8vKoq6tj1apVHDp0iEsvvbQVXpaIiLS1vt1CeOgHw+kVFQTAxt1F/PGlrZzQnZ1E5DxcvsRq8eLFFBUVMWbMGO644w4mTpzI1KlTAcjNzaWyshKA1atXk5+fT0pKCgkJCU1f8+fPB2DevHmMHDmSSZMmkZKSwqpVq3j22Wfp27dvK748ERFpS+HBvsy7bRjDB0YCkFtwkgXLN3O42G5yMhFxZ4bTg1c0Li5uu1vX2WwWwsICKC2tcKtzLqTlNEPPpxl6DofTyRuf7Oed9QcB8PW28n/fH8IVw3tqfh5M70HPZ8YMIyODzruPey1CJSIi7cLpOzv98LuDsFoMqmsbePLldN77ItfsaCLihlRIRUTkorlsSDfm3DoUfx8bDqeT51bv4KUP9rb5Ciki4t5USEVE5KIa3CuMX92RTOfQxoW439+Qx7NvZFBTqyvwRaSRCqmIiFx0XTsF8PCdIxgc07hm9bZ9x/jDS1spPakr8EVEhVRERNpIkL83C358CaPjugBw8GjjFfh5hW13gaqIuCcVUhERaTPeXlZ+PDGOCWNiACg9WcPvV2xlR84xc4OJiKlUSEVEpE0ZhsHEy/pw9/WDsVoMamobWPTaDj7cctjsaCJiEhVSERExxSXxXZk7eSgBvjacTljxwV5e/nAfDs9dHltELpAKqYiImGZgzzDm3zGcqLDGK/D/vekQf1mzk9o6XYEv0pGokIqIiKmiwv355e3J9IsOAWDLnmKeeDmdk5W1JicTkbaiQioiIqYL8vdm7uShJA+MBCA7/wSP/XMLRaWVJicTkbagQioiIm7B28vKTybGc+2IHgAUllbxu39uIefICZOTicjFpkIqIiJuw2IYTL66P1Ou7o8BnKys4/GXtrFtb7HZ0UTkIlIhFRERt/OdET2498Z4vGwWausdPPN6hpaFEmnHVEhFRMQtJQ/szM+nJBHo54WTxmWhXvlPtpaFEmmHVEhFRMRt9YsO4Ve3J9M5tHFZqPc35vH8m5nU1WtZKJH2RIVURETcWlS4P7+8I5k+3YIB2JRVxBMvp2OvqjM5mYi0FhVSERFxe8H+3vx8ShJJ/SMA2He4cVmoYyeqTE4mIq1BhVRERDyCj5eV+25M4Ork7gAcLankd//cwqEiu8nJROTbUiEVERGPYbEYTL2mP5PG9gXghL2WP6zYQtbBUpOTici3oUIqIiIexTAMrhvZi7uvH4zVYlBV08CTr6SzKavI7GgicoFUSEVExCNdEt+V+28ego+XlfoGJ39ds5O0zYfMjiUiF0CFVEREPFZ8n048MDWJIP/GtUpfStvH6o9zcGqtUhGPokIqIiIerXfXYH55ezKRob4AvLP+IP94Zzf1DQ6Tk4lIS6mQioiIx4sK8+eXtw+nV1QQAJ/vPMrTqzOoqdUC+iKeQIVURETahZAAbx6YmkRcTBgAGfuP86eVWymvrDU5mYicjwqpiIi0G34+Nu6flMiouCgAcgtO8vt/bqGoTAvoi7gzFVIREWlXbFYLd18fy/iRPQEoLK3isX9uIa/wpMnJROTrqJCKiEi7YzEMbhnbj8lX9QOgvKKWP760lT15WkBfxB2pkIqISLt1bUpPZkyIPWMB/e1s21dsdiwR+QoVUhERaddGxXbhpzcPwdvLQl29g2df38lnOwrMjiUiZ1AhFRGRdi+hTyfmTk4iwNeGw+nkH+/u5v0NeWbHEpFTVEhFRKRD6BcdwrzbhhEa6A3AKx9l8+pH2bqrk4gbUCEVEZEOIzoykF9OSyYqzA+A9zbkseS9LBocuquTiJlUSEVEpEOJCPXjF9OSm+7q9NmOAp57Yyd19bqrk4hZVEhFRKTDCT51V6dBPUMB2LbvGE+9sp2qmnpzg4l0UCqkIiLSIfn52Jh9SyLDBkQCkJVXxh9f2sqJCt1qVKStqZCKiEiH5WWz8pOJcVw2pCsAeYV2fv+vLRzTrUZF2pQKqYiIdGhWi4Xp1w3iu6N6AVBUWsVj/9pC/rEKk5OJdBwqpCIi0uEZhsHNV/bl1lO3Gi2z1/LHFVvJLSg3OZlIx6BCKiIicsq4lJ788LpBGAbYq+p4fOU29uSVmh1LpN1TIRURETnDZYnd+ElqPFaLQXVtA0++sp3t2cfMjiXSrqmQioiIfMXwQZ25/+YheNss1NU7eOb1DDbuLjQ7lki7pUIqIiJyDvF9OjHn1qH4+VhpcDh5/s1MPk7PNzuWSLukQioiIvI1BvQI5YEpwwjy98IJLHt/D+9vyDM7lki7o0IqIiLyDXp1CWLebcMIC/IB4JWPsnn9kxycTqfJyUTaDxVSERGR8+jaKYBfTBtGVJgfAG9/cZCXPtiHQ6VUpFWokIqIiLRARIgf86Yl0z0yEIAPtx7m72/vpsHhMDmZiOdTIRUREWmhkABvHrwtib7dggFYn3mU597YSV19g8nJRDybCqmIiIgLAny9+NnkoQzuFQbAtn3HWPzaDmrqVEpFLpQKqYiIiIt8vW3MmpRIUv8IADIPlPLUqnSqaupNTibimVRIRURELoCXzcJPJsYzMjYKgL2HT/DEy+nYq+pMTibieVRIRURELpDNauFH18dy2ZCuAOQWlPP4ym2UV9SanEzEs6iQioiIfAsWi8EPrhvE1cndAThUZOePL22l9GSNyclEPIcKqYiIyLdkMQymXtOf60b1BKDgeCV/WLGFY2VVJicT8QwqpCIiIq3AMAxuvqIvEy/rDUBxWTV/eGkrhSWVJicTcX8qpCIiIq3EMAwmjOnNrVf1A6CkvIY/rNhKfrHd5GQi7k2FVEREpJWNS+nJ7dcOAOBERS1/fGkbB4+eNDmViPtSIRUREbkIxg7rzl3fG4xhgL2qjj+t3EZ2/gmzY4m4JRVSERGRi2RMQlfumRCH1WJQVVPPwpfT2X2w1OxYIm5HhVREROQiShkcxX03JmCzGtTUNfDnV7ezM/e42bFE3IrLhTQ/P58ZM2YwcuRIxo4dy+OPP47D4TjnvitXrmTcuHEkJSWRmppKWlpa02MOh4NnnnmGq666iqSkJG699VY2b9584a9ERETETQ3tH8H9NyfibbNQV+9g8WsZ7Mg5ZnYsEbfhciGdOXMmUVFRpKWlsWTJEtLS0li2bNlZ+61bt46FCxfy2GOPsXHjRqZNm8asWbM4dOgQAEuXLmX16tU8//zzbNiwgUsvvZT77rsPu11XIoqISPsT1zuc2bck4uNlpb7BwdOrM9i2t9jsWCJuwaVCmpGRQVZWFnPnziUoKIiYmBimT5/OqlWrztq3urqaOXPmkJycjJeXF5MmTSIgIID09PTGJ7ZYeOCBB+jfvz/e3t7ceeedlJWVsXfv3lZ5YSIiIu5mYM8w5tyaiK+3lQaHk+fW7GRzVpHZsURMZ3Nl58zMTKKjowkJCWnaFhcXR25uLna7ncDAwKbtqampzX62vLyciooKoqKiAJg+fXqzx48ePQpA586dW5zHYjGwWAxXXsIFs1otzb6L59EMPZ9m6Nk0v0aDY8J5YOownli5jcqaev76ZiY/NmBUXBezo52XZuj53HWGLhXSsrIygoODm207XU5LS0ubFdIzOZ1O5s+fT2JiIikpKWc9Xltby69+9SsmTJhA9+7dW5wnPDwAw2ibQnpacLBfmz6ftD7N0PNphp5N84MRYQEsCPXj18+vx15Vx1/X7MTH14urhvc0O1qLaIaez91m6FIhhcZy6Yq6ujrmzZtHdnY2y5cvP+txu93Offfdh9Vq5Te/+Y1Lv7ukpKJNj5AGB/tRXl5FQ8O5L+IS96YZej7N0LNpfs1FBHrz4G3D+NNLWzlZWcefV26jvLyaK5KizY72tTRDz2fGDMPCAs67j0uFNDw8nLKysmbbysrKMAyD8PDws/avrq7m3nvvpaqqihUrVhAWFtbs8ZKSEu688066d+/OE088ga+vrytxcDicOByuFeRvq6HBQX293oSeTDP0fJqhZ9P8/ic6IoCfT0niiZXbKK+s4+/v7Ka23sFYNy6loBm2B+42Q5dOIIiPj6egoICSkpKmbRkZGfTr14+AgObt1+l0Mnv2bGw2G0uXLj2rjNbU1HDPPfcQFxfH4sWLXS6jIiIi7UH3yEAevG0YIYHeAPxz3R4+2HzI5FQibculQhobG0tCQgILFy7EbreTk5PDkiVLmDJlCgDjx49vWkt07dq1ZGdns2jRInx8fM76Xf/4xz/w8vLit7/9LRaLe51YKyIi0pa6dgpg3tRhhAU1/vtyZdo+3t+QZ3IqkbbjchNcvHgxRUVFjBkzhjvuuIOJEycydepUAHJzc6msrARg9erV5Ofnk5KSQkJCQtPX/Pnzmx7fvn07iYmJzR5/7rnnWvHliYiIeIaocH8evG0YnYIbS+krH2XzzvoD5oYSaSOG09WrlNxIcfHJNnsum81CWFgApaUVbnXOhbScZuj5NEPPpvm1zLETVfzppW0cO1ENwMTLejNhTG+TUzXSDD2fGTOMjAw67z76rFxERMSNRIT4Me+2YUSFNS7Ls+bTXN78LNfkVCIXlwqpiIiImwkP9uWBqcOICvcH4M3Pclnz6X6TU4lcPCqkIiIibigsyIcHpiQ1ldK3Pj/Amk/3u7weuIgnUCEVERFxU2FBPjw4NYkuzUpprkqptDsqpCIiIm4sNLCxlHbt1FhK135xgDdUSqWdUSEVERFxcyGBjR/fny6lb39xgNc/0cf30n6okIqIiHiAkEAfHpg6jG4RjXdGfGf9QZVSaTdUSEVERDxESIA3P5+S1KyUvvZxjkqpeDwVUhEREQ8SEuDNA1OSiD5VSt/7Mo/X/qtSKp5NhVRERMTDBJ86UhodeaqUbsjjVZVS8WAqpCIiIh7odCntfqqUvr8hj1c+ylYpFY+kQioiIuKhgv29mXtGKV238ZA+vhePpEIqIiLiwYL9mx8pfW9Dnq6+F4+jQioiIuLhgk4dKY0+4+p7LZ4vnkSFVEREpB04/fH9mYvnv/lZrsmpRFpGhVRERKSdOL0k1OlS+tbnB3hLpVQ8gAqpiIhIOxIS6MPPpyQRFd5YStd8lsvaLw6YG0rkPFRIRURE2pnQQB8emJJE5zA/AN74ZD/vrD9gbiiRb6BCKiIi0g6FBZ0qpaGNpXT1x/t5b8NBk1OJnJsKqYiISDsVHuzLA1OTiAjxBeDVj3JYtzHP5FQiZ1MhFRERace+WkpX/Sebf286ZHIqkeZUSEVERNq5iBA/HpiSRKdgHwBe/nAfaZtVSsV9qJCKiIh0ABGhfvx86jDCT5XSl9L28dHWwyanEmmkQioiItJBdA5tPFIaFtRYSv/57718sv2IyalEVEhFREQ6lM5h/jwwJYmQAG8Alr2XxRc7C0xOJR2dCqmIiEgHExXuz9wpSQT5e+EE/v7ObjbuLjQ7lnRgKqQiIiIdUHREAHMnJxHga8PphBfe2sXWvcVmx5IOSoVURESkg+rROZC5k5Pw97HhcDr5y5qdbM8+ZnYs6YBUSEVERDqwXl2CmH1rIr7eVhocTp59Yyc7c4+bHUs6GBVSERGRDq5vtxBmTUrEx8tKfYODp1dnkHWw1OxY0oGokIqIiAgDeoTy05uH4GWzUFfvYNFrO9h3uMzsWNJBqJCKiIgIAIN7hTHz+wnYrAY1dQ089cp29h8pNzuWdAAqpCIiItIkvncn7rsxAavFoLq2gSdXpXPw6EmzY0k7p0IqIiIizST2i+DHqfFYDIPKmnqeeHkbh4vsZseSdkyFVERERM6SPDCSGRNiMQyoqG4spUeOVZgdS9opFVIRERE5p5TBUdz9vVgMoLyyjj+u2MrR4yql0vpUSEVERORrjY7vwh3jBwJQerKGX/31C0rKq01OJe2NCqmIiIh8oyuGRjPl6v4AFJVU8od/beVERa3JqaQ9USEVERGR8/rOiB7cfGVfAI6WVLLw5W3Yq+pMTiXthQqpiIiItMiES3sz6dSR0sPFFTy5Kp3K6nqTU0l7oEIqIiIiLXb7dYO5NqUHAAeOnuTPr22nprbB5FTi6VRIRUREpMUMw+C27wzg8sRuAGQfPsHTr++grl6lVC6cCqmIiIi4xDAM7hg3kFFxUQDsOlDKc2/spL7BYXIy8VQqpCIiIuIyi8Xgru8NJnlAJADbc47zwtpdNDhUSsV1KqQiIiJyQawWC/ekxjGkbycANmcVseTdLBxOp8nJxNOokIqIiMgFs1kt3DsxnsG9wgD4YudR/vXvvThVSsUFKqQiIiLyrXh7WZn5/QT6RYcA8N9t+bz6UY5KqbSYCqmIiIh8a77eNmZNSqRXVBAA72/M4+0vDpgbSjyGCqmIiIi0Cn9fG3NuTaRrJ38A3vg0lw82HTI5lXgCFVIRERFpNUH+3sydnEREiC8AKz/cx6fbj5icStydCqmIiIi0qrAgH+ZOSSI00BuApe9nsSmryORU4s5USEVERKTVdQ7142eTkwj088LphBfeymRHzjGzY4mbUiEVERGRiyI6IoA5tybi52OlweHk2Td2siev1OxY4oZUSEVEROSiiekSzP03J+Jts1BX72DRazvILSg3O5a4GRVSERERuagG9Ajl/25KwGoxqK5t4MlV6RwutpsdS9yICqmIiIhcdPF9OnHPhDgMAyqq61n4cjqFpZVmxxI3oUIqIiIibWL4oM7c+d3BAJyoqOWJlemUlFebnErcgQqpiIiItJkxCV257TsDADheXs0TL6dTXlFrcioxmwqpiIiItKmrk7tz0+V9ADhaUsmTq9KprK4zOZWYSYVURERE2tz3RvfiulE9AcgrsrPotR3U1DWYnErMokIqIiIibc4wDG6+oi9XDu0GwL7DJ3jujZ3UNzhMTiZmcLmQ5ufnM2PGDEaOHMnYsWN5/PHHcTjO/Q/PypUrGTduHElJSaSmppKWltbs8WPHjnHXXXcxcOBAampqLuwViIiIiEcyDINp1w4kZXBnADL2H+fFt3fhcDhNTiZtzeVCOnPmTKKiokhLS2PJkiWkpaWxbNmys/Zbt24dCxcu5LHHHmPjxo1MmzaNWbNmcejQIQD27NnDzTffTGho6Ld+ESIiIuKZLBaDu6+PJb5POAAbdxex4oO9OJ0qpR2JS4U0IyODrKws5s6dS1BQEDExMUyfPp1Vq1adtW91dTVz5swhOTkZLy8vJk2aREBAAOnp6QCUlJTw5JNPcsstt7TKCxERERHPZLNauO/GBPp1DwHgo235vP7JfpNTSVuyubJzZmYm0dHRhISENG2Li4sjNzcXu91OYGBg0/bU1NRmP1teXk5FRQVRUVEAjB49GoANGzZccHiLxcBiMS74511htVqafRfPoxl6Ps3Qs2l+nu9iztBms/CzyUP5/T+3kFdo5531Bwny9+a7o3u1+nN1ZO76PnSpkJaVlREcHNxs2+lyWlpa2qyQnsnpdDJ//nwSExNJSUm5wKhnCw8PwDDappCeFhzs16bPJ61PM/R8mqFn0/w838WaYRiw4CdjePCZzyg4VsHLH+4jslMA145UKW1t7vY+dKmQAi6f01FXV8e8efPIzs5m+fLlrj7dNyopqWjTI6TBwX6Ul1fRoCsAPZJm6Pk0Q8+m+Xm+tprh3MlDWbBsM6Una3jm1XRoaGDE4KiL9nwdiRnvw7CwgPPu41IhDQ8Pp6ysrNm2srIyDMMgPDz8rP2rq6u59957qaqqYsWKFYSFhbnydOflcDjb/Eq8hgYH9fX6i9STaYaeTzP0bJqf57vYMwwL9OFntw7lDyu2Yq+q4y9rduJtsxLX++yuIRfG3d6HLp1AEB8fT0FBASUlJU3bMjIy6NevHwEBzduv0+lk9uzZ2Gw2li5d2uplVERERNqvbhEBzL4lER9vK/UNTp55PYOc/BNmx5KLxKVCGhsbS0JCAgsXLsRut5OTk8OSJUuYMmUKAOPHj2fz5s0ArF27luzsbBYtWoSPj0/rJxcREZF2rXfXYH76/SHYrBZq6hr486vbOVxkNzuWXAQuX2K1ePFiioqKGDNmDHfccQcTJ05k6tSpAOTm5lJZWQnA6tWryc/PJyUlhYSEhKav+fPnAzB//nwSEhK46667ABg+fDgJCQmsWbOmlV6aiIiIeLrBvcL4SWocFsOgorqehavSKSqrMjuWtDLD6cErzxYXn2yz57LZLISFBVBaWuFW51xIy2mGnk8z9Gyan+czc4afZxTw93d2AxAZ6ssvpyUTEqhPYF1lxgwjI4POu497LUIlIiIicg5jEroy5Zr+ABSXVfPkK9uprK4zOZW0FhVSERER8QjfGd6D6y+JAeBQkZ3Fr+2gtq7B3FDSKlRIRURExGPceFlvrhzaDYC9h0/w1zczaXDoFBBPp0IqIiIiHsMwDKZdO5DhAyMBSM8+xtJ3s3B47iUxggqpiIiIeBiLxeBHN8QRG9O4xvnnO4/y6kfZLt9NUtyHCqmIiIh4HC+bhftuTKB318YruNdtPMR7G/JMTiUXSoVUREREPJKfj41ZkxLpEu4PwGv/zeGT7UdMTiUXQoVUREREPFaQvzc/u3UoYUGNa5Iuez+LLXuKTE4lrlIhFREREY/WKcSXn906lEA/L5xOeP6tTHYfLDU7lrhAhVREREQ8XreIAGZNSsTHy0p9g5OnV+/gwNFys2NJC6mQioiISLvQp1sw/3dTAlaLQXVtA0+9sp2jJZVmx5IWUCEVERGRdiOudzg/uiEWAzhZWcfCl9MpPVljdiw5DxVSERERaVdSBkcx7doBABwvr+bJV9J133s3p0IqIiIi7c7YYd1JvbQ3APnFFSzSfe/dmgqpiIiItEsTxsRwZVI0APt033u3pkIqIiIi7ZJhGEz7zgCSz7jv/T/X7dEtRt2QCqmIiIi0WxaLwYwbYhnUMxSAT7YX8Man+80NJWdRIRUREZF2zctm5f9uGkLPzoEAvP3FQdI2HzI5lZxJhVRERETaPX9fG7NvSSQy1BeAlWn72Li70ORUcpoKqYiIiHQIIYE+zLl1KMH+XjiBv63dReaBErNjCSqkIiIi0oFEhfkz+5ah+HpbaXA4eeb1DN1i1A2okIqIiEiH0qtLEDNvSsBmNag5dYvRQt1i1FQqpCIiItLhDI4J50c3xP3vFqOr0imz6xajZlEhFRERkQ5pxKDO3HbqFqPHTlTz1CvbqayuNzlVx6RCKiIiIh3WVcO6c8MlMQAcKrLzzOs7qKvX3ZzamgqpiIiIdGgTL+vNFUO7AZCVV8bf3t6FQ3dzalMqpCIiItKhGYbBtGsHMLRfBACbs4p4OW2fbjHahlRIRUREpMOzWizckxpHv+gQANK2HOb9DXkmp+o4VEhFREREAB8vKz+9eQhdO/kD8Op/c/g8o8DkVB2DCqmIiIjIKYF+Xsy5ZSihgd4ALH0vi4z9x01O1f6pkIqIiIicoVOIL3NuGYqfj40Gh5Pn3thJboHu5nQxqZCKiIiIfEX3zoH89Pun7uZU18CfX91OYanu5nSxqJCKiIiInMPAnmHMOONuTk+uSudERa3ZsdolFVIRERGRrzF8UGemXNMfgOKyav78ynaqanQ3p9amQioiIiLyDa4Z3oPvje4FwMHCkzz3Rgb1DbqbU2tSIRURERE5j5su78OY+C4AZB4o5R/v7tbdnFqRCqmIiIjIeRiGwQ+uG0RCn04AfJlZyGv/zTE5VfuhQioiIiLSAjarhZ9MjKN31yAA3t+QxwebD5mcqn1QIRURERFpIV9vG/dPSqRzmB8AL6ftY3NWkcmpPJ8KqYiIiIgLgv29mXNLIkH+XjiBF9buYu+hMrNjeTQVUhEREREXdQ7zZ9akRLy9LNQ3OFj82g7yj1WYHctjqZCKiIiIXIDeXYO5d2I8FsOgsqaeP7+STunJGrNjeSQVUhEREZELNKRvBHeMHwjA8fIannplO5XVWjjfVSqkIiIiIt/C5YndSL20NwCHi+08q4XzXaZCKiIiIvItTRgTw+WJXQHYfVAL57tKhVRERETkWzIMg9vHDWRI3/8tnL/6Yy2c31IqpCIiIiKtwGqx8JPU+KaF89/7Mo8Ptxw2OZVnUCEVERERaSU+3lbuvzmRzqGNC+e/9MFetuzRwvnno0IqIiIi0oqCA7yZfWsigX7/Wzh/3+Eys2O5NRVSERERkVYWdcbC+XX1jQvnFxzXwvlfR4VURERE5CLo0y2YH6fGYxhQUV3Pk6u2c8KuhfPPRYVURERE5CIZ2i+CO8adXji/mj+/uoPqWi2c/1UqpCIiIiIX0RVDo7n+kl4AHCw8yV/fzKTBoYXzz6RCKiIiInKR3XhZH0bHdQFgR85x/rluL04tnN9EhVRERETkIjMMgx9+dxCDe4UB8Mn2I7y9/qDJqdyHCqmIiIhIG7BZLdx3YwLdIwMAeOOT/XyeUWByKvegQioiIiLSRvx9bcyalEhYkA8AS9/LIvNAicmpzKdCKiIiItKGwoN9mTUpET8fKw0OJ8++nsGhIrvZsUylQioiIiLSxnp0DuS+GxOwWgyqaxv486vbKSmvNjuWaVRIRUREREwQGxPOD787CIDSkzU89ep2Kqs75hqlKqQiIiIiJrkkvis3Xt4HgPziCp59I4P6ho63RqnLhTQ/P58ZM2YwcuRIxo4dy+OPP47jaxZ3XblyJePGjSMpKYnU1FTS0tKaHnM4HDz11FNcffXVjBgxgrvuuotDhw5d+CsRERER8UDXj+7F5YndANh9sJQl72Z1uDVKXS6kM2fOJCoqirS0NJYsWUJaWhrLli07a79169axcOFCHnvsMTZu3Mi0adOYNWtWU+lcsWIFa9eu5YUXXuCjjz4iJiaG++67r8MNQERERDo2wzC4fdwAhvTtBMD6zKO88el+k1O1LZcKaUZGBllZWcydO5egoCBiYmKYPn06q1atOmvf6upq5syZQ3JyMl5eXkyaNImAgADS09MBWLVqFdOnT6dv374EBgYye/ZscnJy2L59e6u8MBERERFPYbVY+HFqHL26BAHw9hcH+Tg93+RUbcelQpqZmUl0dDQhISFN2+Li4sjNzcVub75cQWpqKlOnTm36c3l5ORUVFURFRVFdXU12djaxsbFNjwcGBtKrVy8yMjIu9LWIiIiIeCxfbxuzbh5CRIgvAP9ct5eM/cdNTtU2bK7sXFZWRnBwcLNtp8tpaWkpgYGB5/w5p9PJ/PnzSUxMJCUlhcLCQpxOZ7Nie/p3lZaWtjiPxWJgsRiuvIQLZrVamn0Xz6MZej7N0LNpfp5PM7z4OoX68bMpSfx26SYqq+v5y5qd/OqO4U1HTr8td52hS4UUcPkcz7q6OubNm0d2djbLly//Vr/rq8LDAzCMtimkpwUH+7Xp80nr0ww9n2bo2TQ/z6cZXlxhYQHMv3Mkv37+C6prG3jqle088dPLiQxrvf/f3W2GLhXS8PBwysrKmm0rKyvDMAzCw8PP2r+6upp7772XqqoqVqxYQVhYGAChoaFYLJZz/q5OnTq1OE9JSUWbHiENDvajvLyKhg64HEN7oBl6Ps3Qs2l+nk8zbDvdw/24+/pY/vpmJiXl1Tz8whf86o7h+Pu6fCyxGTNmGBYWcN59XHpV8fHxFBQUUFJS0lRAMzIy6NevHwEBzZ/M6XQye/ZsbDYbS5cuxcfHp+kxHx8f+vfvT2ZmJikpKUDjOaZ5eXkMGTKkxXkcDicOR9teld/Q4KC+Xm9CT6YZej7N0LNpfp5PM2wbKYOjKCyt4o1P9nOoyM7Tr23n/kmJ2Frh43Z3m6FLryg2NpaEhAQWLlyI3W4nJyeHJUuWMGXKFADGjx/P5s2bAVi7di3Z2dksWrSoWRk9bcqUKSxfvpycnBzsdjtPPPEEgwcPJiEhoRVeloiIiIjna1yjtCsAmQdKWf7+nna5RKbLx30XL17MQw89xJgxYwgMDGTy5MlNV9Pn5uZSWVkJwOrVq8nPz286AnpaamoqCxYsYPLkyRQXF3P77bdTUVHByJEjeeaZZ1rhJYmIiIi0D4ZhMO3agZSU17Azt4TPMgqICPVlwpjeZkdrVYbTg2t2cfHJNnsum81CWFgApaUVbnWIW1pOM/R8mqFn0/w8n2Zonqqaen7/r60cLm5cZvPu6wdzSXxXl3+PGTOMjDz/CgHudc2/iIiIiJzFz8fGrElDCAtqPA1yybtZ7D5QYnKq1qNCKiIiIuIBwoN9mTUpEV9vKw0OJ8+8sZP8Yvv5f9ADqJCKiIiIeIgenQO598Z4LIZBVU09f351O2X2GrNjfWsqpCIiIiIeJL53J34wfiAAx8trWPTqDqpr601O9e2okIqIiIh4mMsSu3H9JTEAHCw8yV/fzKTB4bkXmqmQioiIiHigGy/rzei4KAB25BxnZdo+j12jVIVURERExAMZhsH06wYzoEcoAP/Zms8Hmw6ZG+oCqZCKiIiIeCgvm4X/uymBLuH+AKz6TzZb9hSbnMp1KqQiIiIiHizQz4tZtyQS5O+FE/jb2kz2Hyk3O5ZLVEhFREREPFznUD9++v0heNks1NY7WPzado6VVZkdq8VUSEVERETagb7RIfzo+lgAyivreOrV7VRW15mcqmVUSEVERETaieGDOjNpbF8ACo5X8uwbO6lvcP/loFRIRURERNqR8Sk9uXJoNwB2Hyxl2ftZbr8clAqpiIiISDtiGAa3XTuA+D7hAHyecZS3vzhgbqjzUCEVERERaWesFgs/SY2nR+dAAN74NJf1mUdNTvX1VEhFRERE2iE/Hxv33zyE0EBvAJa8u5usg6Umpzo3FVIRERGRdio82JdZkxLx8bZS3+Bk0avbOVx00uxYZ1EhFREREWnHekYF8ZPUOAwDKqrreWrlVrMjnUWFVERERKSdG9I3gtuvHYhhQH2D0+2uureZHUBERERELr4rk6JJGhBJz+hQKuzVgPuUUh0hFREREekgOoX44u1lNTvGWVRIRURERMRUKqQiIiIiYioVUhERERExlQqpiIiIiJhKhVRERERETKVCKiIiIiKmUiEVEREREVOpkIqIiIiIqVRIRURERMRUKqQiIiIiYioVUhERERExlQqpiIiIiJhKhVRERERETKVCKiIiIiKmUiEVEREREVOpkIqIiIiIqQyn0+k0O4SIiIiIdFw6QioiIiIiplIhFRERERFTqZCKiIiIiKlUSEVERETEVCqkIiIiImIqFVIRERERMZUKqYiIiIiYSoVUREREREylQioiIiIiplIhFRERERFTqZCeIT8/nxkzZjBy5EjGjh3L448/jsPhOOe+y5cvZ9y4cQwbNowpU6awc+fONk4r5+LKDFeuXMm4ceNISkoiNTWVtLS0Nk4rX+XK/E4rLCwkKSmJp59+uo1SyjdxZYY5OTncfvvtJCYmcsUVV7B06dK2DSvn1NIZOhwOFi9ezFVXXUVSUhI33HAD7777rgmJ5as+/fRTLrnkEmbPnv2N+zkcDp566imuvvpqRowYwV133cWhQ4faKGVzKqRnmDlzJlFRUaSlpbFkyRLS0tJYtmzZWfv95z//4emnn+ZPf/oTX3zxBWPHjuXHP/4xlZWVJqSWM7V0huvWrWPhwoU89thjbNy4kWnTpjFr1izT3ojSqKXzO9OCBQuwWq1tlFDOp6UzrK6u5u677+aKK67gyy+/5Omnn+a1114jJyfHhNRyppbOcOXKlbz66qu8+OKLbN68mTlz5vDzn/+crKwsE1LLaX/7299YsGABvXr1Ou++K1asYO3atbzwwgt89NFHxMTEcN999+F0OtsgaXMqpKdkZGSQlZXF3LlzCQoKIiYmhunTp7Nq1aqz9l21ahU33XQTiYmJ+Pr6cvfddwPw0UcftXVsOYMrM6yurmbOnDkkJyfj5eXFpEmTCAgIID09ve2DC+Da/E77+OOPyc7O5sorr2y7oPK1XJnhe++9R2BgIHfffTd+fn4MGTKEt99+m759+5qQXE5zZYaZmZkkJyfTp08frFYrY8eOJTQ0lD179piQXE7z8fHhtddea1EhXbVqFdOnT6dv374EBgYye/ZscnJy2L59exskbU6F9JTMzEyio6MJCQlp2hYXF0dubi52u/2sfWNjY5v+bLFYGDx4MBkZGW2WV87mygxTU1OZOnVq05/Ly8upqKggKiqqzfJKc67MDxr/o+LRRx/l4YcfxmaztWVU+RquzHDLli0MGDCAX/ziFwwfPpzx48fz1ltvtXVk+QpXZnjllVeyceNGdu/eTW1tLR9++CFVVVWkpKS0dWw5wx133EFQUNB596uuriY7O7tZnwkMDKRXr16m9BkV0lPKysoIDg5utu30G7K0tPSsfc98s57e96v7SdtyZYZncjqdzJ8/n8TERP1FaiJX5/fss88ydOhQRo0a1Sb55PxcmeHRo0f58MMPueSSS/j000+55557ePDBB9m1a1eb5ZWzuTLDa6+9lltvvZWJEyeSkJDAz372M37/+9/TtWvXNssrF+7EiRM4nU636TM6rHAGV86ZMOP8Cjk/V+dSV1fHvHnzyM7OZvny5RcplbRUS+eXnZ3Nq6++ytq1ay9yInFVS2fodDqJi4vjhhtuAODGG2/k5Zdf5v333292xEbaXktnuGbNGtasWcOrr77KwIEDWb9+PT/72c/o2rUrQ4YMucgppbW4S5/REdJTwsPDKSsra7atrKwMwzAIDw9vtj0sLOyc+351P2lbrswQGj+uuOeeezhy5AgrVqwgIiKijZLKubR0fk6nk0ceeYSZM2cSGRnZxinlm7jyHoyMjDzrY8Xo6GiKi4svdkz5Bq7M8F//+he33norQ4YMwcfHhyuvvJJRo0bp1AsPERoaisViOee8O3Xq1OZ5VEhPiY+Pp6CggJKSkqZtGRkZ9OvXj4CAgLP2zczMbPpzQ0MDu3btIjExsc3yytlcmaHT6WT27NnYbDaWLl1KWFhYW8eVr2jp/I4cOcKmTZtYvHgxI0eOZOTIkbzzzju8+OKL3HjjjWZEl1NceQ/27duXvXv3Njs6k5+fT3R0dJvllbO5MkOHw0FDQ0OzbbW1tW2SU749Hx8f+vfv36zPlJeXk5eXZ8oRbhXSU2JjY0lISGDhwoXY7XZycnJYsmQJU6ZMAWD8+PFs3rwZgClTprBmzRrS09OpqqriL3/5C97e3rrS12SuzHDt2rVkZ2ezaNEifHx8zIwtp7R0fl26dOHjjz/mzTffbPq66qqrmDx5Mi+88ILJr6Jjc+U9OGHCBEpLS/nrX/9KdXU1b7/9NpmZmUyYMMHMl9DhuTLDq666itdee42srCzq6+v57LPPWL9+PVdffbWZL0G+QWFhIePHj29a4nDKlCksX76cnJwc7HY7TzzxBIMHDyYhIaHNs+kc0jMsXryYhx56iDFjxhAYGMjkyZObrsTOzc1tWmf08ssvZ86cOcyaNYvjx4+TkJDACy+8gK+vr5nxhZbPcPXq1eTn5591EVNqaioLFixo89zSqCXzs1qtdOnSpdnP+fn5ERgYqI/w3UBL34NRUVE8//zz/O53v+O5556jW7duPPvss/Ts2dPM+ELLZ3jPPfdQX1/PfffdR0lJCdHR0SxYsIDRo0ebGb/DO10m6+vrAZpu+pKRkUFdXR25ublNR7InT55McXExt99+OxUVFYwcOZJnnnnGlNyG013OZhURERGRDkkf2YuIiIiIqVRIRURERMRUKqQiIiIiYioVUhERERExlQqpiIiIiJhKhVRERERETKVCKiIiIiKmUiEVEREREVOpkIqIiIiIqVRIRURERMRUKqQiIiIiYioVUhEREREx1f8DvMoh5NPtmo8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(m, torch.sigmoid(m) * ( 1 - torch.sigmoid(m)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd6b9f913c0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(m, torch.log(m) - torch.log( 1 - m),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dequantization(nn.Module):\n",
    "    \n",
    "    def __init__(self,  quants=256, alpha=1e-5):\n",
    "        super().__init__()\n",
    "        self.quants = quants\n",
    "        self.alpha = alpha\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        # Uniform Distribution\n",
    "        z = (x + torch.rand_like(x).detach()) / self.quants\n",
    "        log_det = -np.prod(z.shape[1:]) * np.log(self.quants)\n",
    "        \n",
    "        # Boundary fix for the sigmoid below\n",
    "        z = z * (1 - self.alpha) + self.alpha\n",
    "        log_det = log_det  + np.prod(z.shape[1:]) * np.log(1 - self.alpha)\n",
    "        \n",
    "        # Inverse sigmoid log(z) - log(1-z)\n",
    "        # J = 1/z + 1 /(1-z) = 1 / z(1-z) \n",
    "        # detJ = prod(1 / z(1-z), dim=[1, 2, 3]) \n",
    "        # logdetJ = -sum(log(z) - log(1-z), dim=[1, 2, 3])\n",
    "        log_det = log_det - (z.abs().log() + (1-z).abs().log()).sum(dim=[1, 2, 3]) \n",
    "        z = torch.log(z) - torch.log(1-z)\n",
    "        \n",
    "        # new_x = self.reverse(z)\n",
    "        # print(new_x)\n",
    "        # print(x)\n",
    "        # print((new_x - x).max())\n",
    "        return z, log_det\n",
    "    \n",
    "    def reverse(self, z):\n",
    "        x = torch.sigmoid(z)\n",
    "        x = (x - self.alpha) / (1 - self.alpha)\n",
    "        x = x * self.quants\n",
    "        x = x.floor().clamp(min=0, max=self.quants - 1)\n",
    "        return x\n",
    "\n",
    "# Dequantization()(img)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_flows=32, num_blocks=3):\n",
    "        super().__init__()\n",
    "        self.glow = Glow(num_flows=32, num_blocks=3)   \n",
    "        self.dequantize = Dequantization() \n",
    "         \n",
    "    def forward(self, x):\n",
    "        z, qlog_det = self.dequantize(x)\n",
    "        out, z_list, log_p, log_det = self.glow(z)\n",
    "        log_det = log_det + qlog_det\n",
    "        return out, z_list, log_p, log_det \n",
    "    \n",
    "    def reverse(self, z):\n",
    "        x = self.glow.reverse(z)\n",
    "        x = self.dequantize.reverse(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NFModel(\n",
       "  (glow): Glow(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (squeeze): Squeeze()\n",
       "        (flow_step): ModuleList(\n",
       "          (0-31): 32 x FlowModule(\n",
       "            (act_norm): ActNorm()\n",
       "            (inv_conv): InvConvLU()\n",
       "            (affine_coupling): AffineCouplingLayer(\n",
       "              (nn): Sequential(\n",
       "                (0): Conv2d(6, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (1): ReLU()\n",
       "                (2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (3): ReLU()\n",
       "                (4): Conv2d(512, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (split): Split(\n",
       "          (conv_net): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (squeeze): Squeeze()\n",
       "        (flow_step): ModuleList(\n",
       "          (0-31): 32 x FlowModule(\n",
       "            (act_norm): ActNorm()\n",
       "            (inv_conv): InvConvLU()\n",
       "            (affine_coupling): AffineCouplingLayer(\n",
       "              (nn): Sequential(\n",
       "                (0): Conv2d(12, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (1): ReLU()\n",
       "                (2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (3): ReLU()\n",
       "                (4): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (split): Split(\n",
       "          (conv_net): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (squeeze): Squeeze()\n",
       "        (flow_step): ModuleList(\n",
       "          (0-31): 32 x FlowModule(\n",
       "            (act_norm): ActNorm()\n",
       "            (inv_conv): InvConvLU()\n",
       "            (affine_coupling): AffineCouplingLayer(\n",
       "              (nn): Sequential(\n",
       "                (0): Conv2d(24, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (1): ReLU()\n",
       "                (2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (3): ReLU()\n",
       "                (4): Conv2d(512, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (split): Split(\n",
       "          (conv_net): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dequantize): Dequantization()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NFModel(num_flows=32, num_blocks=3)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[138., 148., 148.,  ...,  37., 199., 213.],\n",
       "          [219., 178.,  91.,  ...,  38., 149., 154.],\n",
       "          [209., 185., 146.,  ...,  80., 192.,  67.],\n",
       "          ...,\n",
       "          [130., 155.,  99.,  ..., 197.,  52.,  83.],\n",
       "          [ 86.,  24., 118.,  ..., 119., 165.,  97.],\n",
       "          [ 77., 176., 192.,  ...,  20., 128., 146.]],\n",
       "\n",
       "         [[102., 147.,  60.,  ..., 192., 209.,  67.],\n",
       "          [200., 174., 110.,  ...,  42.,  77., 145.],\n",
       "          [ 42., 128., 208.,  ...,  98., 221., 211.],\n",
       "          ...,\n",
       "          [101.,  95., 163.,  ..., 150., 161., 134.],\n",
       "          [ 98., 108., 219.,  ..., 198., 208., 150.],\n",
       "          [118., 114., 171.,  ...,  41., 147.,  28.]],\n",
       "\n",
       "         [[149., 241., 132.,  ...,  77.,  96., 106.],\n",
       "          [227., 105., 135.,  ..., 159.,  52., 103.],\n",
       "          [197., 181., 128.,  ...,  96., 121., 109.],\n",
       "          ...,\n",
       "          [121.,  90., 152.,  ..., 136., 206.,  99.],\n",
       "          [202., 213.,  23.,  ..., 213., 105.,  99.],\n",
       "          [ 27.,  88.,  48.,  ...,  52., 184., 220.]]]], device='cuda:0',\n",
       "       grad_fn=<ClampBackward1>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reverse(model(img.cuda())[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader=None, epochs=0, savepath=None):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)    \n",
    "        self.savepath = savepath\n",
    "        if self.savepath is not None:\n",
    "            os.makedirs(self.savepath, exist_ok=True)\n",
    "\n",
    "    def fit(self, epoch, loader):\n",
    "        losses = dict(log_p=[], log_det=[], loss=[])\n",
    "        for step, img in enumerate(loader):\n",
    "            out, z_list, log_p, log_det = self.model(img.cuda())\n",
    "            nll = -(log_det + log_p) / np.prod(img.shape[1:])\n",
    "\n",
    "            loss = nll.mean()\n",
    "            \n",
    "            if torch.isnan(loss).item():\n",
    "                continue\n",
    "            \n",
    "            losses[\"log_p\"].append(log_p.mean().item())\n",
    "            losses[\"log_det\"].append(log_det.mean().item())\n",
    "            losses[\"loss\"].append(loss.item())\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10.)\n",
    "\n",
    "            if step % 5 == 0:\n",
    "                print(f\"Epoch: {epoch}, step: {step}, loss: {np.mean(losses['loss'])}\")\n",
    "        \n",
    "        return losses\n",
    "\n",
    "    def sample_img(self, z):\n",
    "        with torch.no_grad():\n",
    "            x = self.model.reverse(z)\n",
    "            x = x.permute((0, 2, 3, 1))\n",
    "            x = x.cpu().numpy().astype(np.uint8)\n",
    "        return x\n",
    "\n",
    "    def train(self):\n",
    "        self.train_losses = dict()\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            losses = self.fit(epoch, train_loader)\n",
    "            for k, loss_vals in losses.items():\n",
    "                self.train_losses.setdefault(k, []).extend(loss_vals)\n",
    "            if epoch % 10 == 0:\n",
    "                self.evaluate(epoch)\n",
    "        return self.train_losses\n",
    "    \n",
    "    def evaluate(self, epoch=None):\n",
    "        if self.val_loader is None:\n",
    "            return\n",
    "        if epoch is None:\n",
    "            epoch = 0\n",
    "        savepath = os.path.join(self.savepath, f\"{epoch}\")\n",
    "        os.makedirs(savepath, exist_ok=True)\n",
    "        img = next(iter(self.val_loader))\n",
    "        with torch.no_grad():\n",
    "            out, z_list, log_p, log_det = self.model(img.cuda())\n",
    "            z = z_list[-1]\n",
    "            imgs = self.sample_img(z)\n",
    "            self.save_img(imgs, savepath)\n",
    "    \n",
    "    def save_img(self, imgs, savepath):\n",
    "        for i, img in enumerate(imgs, 1):\n",
    "            img = Image.fromarray(img)\n",
    "            img.save(os.path.join(savepath, f\"img{i:03d}.jpeg\"))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_loader, val_loader, 1000, savepath=\"/mnt/dl/generation/glow/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, step: 0, loss: 6.547210693359375\n",
      "Epoch: 1, step: 5, loss: 6.431819518407186\n",
      "Epoch: 1, step: 10, loss: 6.34410979531028\n",
      "Epoch: 1, step: 15, loss: 6.259847581386566\n",
      "Epoch: 1, step: 20, loss: 6.187811510903495\n",
      "Epoch: 1, step: 25, loss: 6.137885533846342\n",
      "Epoch: 1, step: 30, loss: 6.092843486416724\n",
      "Epoch: 2, step: 0, loss: 5.617392539978027\n",
      "Epoch: 2, step: 5, loss: 5.7079700628916425\n",
      "Epoch: 2, step: 15, loss: 5.664781125386556\n",
      "Epoch: 2, step: 20, loss: 5.623752140998841\n",
      "Epoch: 2, step: 25, loss: 5.585287551879883\n",
      "Epoch: 2, step: 30, loss: 5.554923184712728\n",
      "Epoch: 3, step: 0, loss: 5.342949867248535\n",
      "Epoch: 3, step: 5, loss: 5.368049144744873\n",
      "Epoch: 3, step: 10, loss: 5.3433637619018555\n",
      "Epoch: 3, step: 15, loss: 5.327048480510712\n",
      "Epoch: 3, step: 20, loss: 5.306049233391171\n",
      "Epoch: 3, step: 25, loss: 5.289342495111319\n",
      "Epoch: 3, step: 30, loss: 5.269463892905943\n",
      "Epoch: 4, step: 5, loss: 5.1367090225219725\n",
      "Epoch: 4, step: 10, loss: 5.129312038421631\n",
      "Epoch: 4, step: 15, loss: 5.115601317087809\n",
      "Epoch: 4, step: 20, loss: 5.105250358581543\n",
      "Epoch: 4, step: 25, loss: 5.0925868797302245\n",
      "Epoch: 4, step: 30, loss: 5.080731709798177\n",
      "Epoch: 5, step: 0, loss: 5.032576560974121\n",
      "Epoch: 5, step: 5, loss: 4.978180090586345\n",
      "Epoch: 5, step: 10, loss: 4.990213047374379\n",
      "Epoch: 5, step: 15, loss: 4.979970574378967\n",
      "Epoch: 5, step: 20, loss: 4.974262124016171\n",
      "Epoch: 5, step: 25, loss: 4.962555408477783\n",
      "Epoch: 5, step: 30, loss: 4.9559367020924885\n",
      "Epoch: 6, step: 0, loss: 4.846535682678223\n",
      "Epoch: 6, step: 5, loss: 4.914065361022949\n",
      "Epoch: 6, step: 10, loss: 4.892788150093772\n",
      "Epoch: 6, step: 15, loss: 4.870598137378693\n",
      "Epoch: 6, step: 20, loss: 4.865118072146461\n",
      "Epoch: 6, step: 25, loss: 4.8658686601198635\n",
      "Epoch: 6, step: 30, loss: 4.866625739682105\n",
      "Epoch: 7, step: 0, loss: 4.8948974609375\n",
      "Epoch: 7, step: 5, loss: 4.84897764523824\n",
      "Epoch: 7, step: 10, loss: 4.817695617675781\n",
      "Epoch: 7, step: 15, loss: 4.807577729225159\n",
      "Epoch: 7, step: 20, loss: 4.811803136553083\n",
      "Epoch: 7, step: 25, loss: 4.801663637161255\n",
      "Epoch: 7, step: 30, loss: 4.7969593232677825\n",
      "Epoch: 8, step: 0, loss: 4.818520545959473\n",
      "Epoch: 8, step: 5, loss: 4.740259250005086\n",
      "Epoch: 8, step: 10, loss: 4.748657096516002\n",
      "Epoch: 8, step: 15, loss: 4.74072390794754\n",
      "Epoch: 8, step: 20, loss: 4.738258112044561\n",
      "Epoch: 8, step: 25, loss: 4.745923794232882\n",
      "Epoch: 8, step: 30, loss: 4.740407574561335\n",
      "Epoch: 9, step: 0, loss: 4.607674598693848\n",
      "Epoch: 9, step: 5, loss: 4.704263846079509\n",
      "Epoch: 9, step: 10, loss: 4.713839184154164\n",
      "Epoch: 9, step: 15, loss: 4.714037239551544\n",
      "Epoch: 9, step: 20, loss: 4.703807263147263\n",
      "Epoch: 9, step: 25, loss: 4.692234736222487\n",
      "Epoch: 9, step: 30, loss: 4.692460413902037\n",
      "Epoch: 10, step: 5, loss: 4.6688131332397464\n",
      "Epoch: 10, step: 10, loss: 4.670827150344849\n",
      "Epoch: 10, step: 15, loss: 4.663671875\n",
      "Epoch: 10, step: 20, loss: 4.665245723724365\n",
      "Epoch: 10, step: 25, loss: 4.656845512390137\n",
      "Epoch: 10, step: 30, loss: 4.656462446848551\n",
      "Epoch: 11, step: 0, loss: 4.7357282638549805\n",
      "Epoch: 11, step: 5, loss: 4.662136713663737\n",
      "Epoch: 11, step: 10, loss: 4.65239108692516\n",
      "Epoch: 11, step: 15, loss: 4.648691385984421\n",
      "Epoch: 11, step: 20, loss: 4.639051164899554\n",
      "Epoch: 11, step: 25, loss: 4.63152735049908\n",
      "Epoch: 11, step: 30, loss: 4.625209469949046\n",
      "Epoch: 12, step: 0, loss: 4.537430763244629\n",
      "Epoch: 12, step: 5, loss: 4.577166477839152\n",
      "Epoch: 12, step: 10, loss: 4.585186958312988\n",
      "Epoch: 12, step: 15, loss: 4.589295506477356\n",
      "Epoch: 12, step: 20, loss: 4.592703160785494\n",
      "Epoch: 12, step: 25, loss: 4.589643551753118\n",
      "Epoch: 12, step: 30, loss: 4.5922801571507605\n",
      "Epoch: 13, step: 0, loss: 4.526933670043945\n",
      "Epoch: 13, step: 5, loss: 4.59381365776062\n",
      "Epoch: 13, step: 10, loss: 4.575792980194092\n",
      "Epoch: 13, step: 15, loss: 4.582679144541422\n",
      "Epoch: 13, step: 20, loss: 4.585050201416015\n",
      "Epoch: 13, step: 25, loss: 4.580121479034424\n",
      "Epoch: 13, step: 30, loss: 4.572676022847493\n",
      "Epoch: 14, step: 0, loss: 4.700572967529297\n",
      "Epoch: 14, step: 5, loss: 4.5524139404296875\n",
      "Epoch: 14, step: 10, loss: 4.527616457505659\n",
      "Epoch: 14, step: 15, loss: 4.524975895881653\n",
      "Epoch: 14, step: 20, loss: 4.541808082943871\n",
      "Epoch: 14, step: 25, loss: 4.539009626095112\n",
      "Epoch: 14, step: 30, loss: 4.546973812964655\n",
      "Epoch: 15, step: 0, loss: 4.582645416259766\n",
      "Epoch: 15, step: 5, loss: 4.549452781677246\n",
      "Epoch: 15, step: 10, loss: 4.535222443667325\n",
      "Epoch: 15, step: 15, loss: 4.536195993423462\n",
      "Epoch: 15, step: 20, loss: 4.531601565224784\n",
      "Epoch: 15, step: 25, loss: 4.524633425932664\n",
      "Epoch: 15, step: 30, loss: 4.526374063184185\n",
      "Epoch: 16, step: 0, loss: 4.548795700073242\n",
      "Epoch: 16, step: 5, loss: 4.5270107587178545\n",
      "Epoch: 16, step: 10, loss: 4.503707148812034\n",
      "Epoch: 16, step: 15, loss: 4.500234842300415\n",
      "Epoch: 16, step: 20, loss: 4.504198823656354\n",
      "Epoch: 16, step: 25, loss: 4.503836356676542\n",
      "Epoch: 16, step: 30, loss: 4.503711685057609\n",
      "Epoch: 17, step: 0, loss: 4.487031936645508\n",
      "Epoch: 17, step: 5, loss: 4.4778828620910645\n",
      "Epoch: 17, step: 10, loss: 4.476292523470792\n",
      "Epoch: 17, step: 15, loss: 4.492170333862305\n",
      "Epoch: 17, step: 20, loss: 4.48231451851981\n",
      "Epoch: 17, step: 25, loss: 4.48407349219689\n",
      "Epoch: 17, step: 30, loss: 4.486268874137632\n",
      "Epoch: 18, step: 0, loss: 4.427244663238525\n",
      "Epoch: 18, step: 5, loss: 4.4536934693654375\n",
      "Epoch: 18, step: 10, loss: 4.460618452592329\n",
      "Epoch: 18, step: 15, loss: 4.476694077253342\n",
      "Epoch: 18, step: 20, loss: 4.467838060288202\n",
      "Epoch: 18, step: 25, loss: 4.465025938474215\n",
      "Epoch: 18, step: 30, loss: 4.468687657387026\n",
      "Epoch: 19, step: 0, loss: 4.353730201721191\n",
      "Epoch: 19, step: 5, loss: 4.425354878107707\n",
      "Epoch: 19, step: 10, loss: 4.44185547395186\n",
      "Epoch: 19, step: 15, loss: 4.440606504678726\n",
      "Epoch: 19, step: 20, loss: 4.449982552301316\n",
      "Epoch: 19, step: 25, loss: 4.461593774648813\n",
      "Epoch: 19, step: 30, loss: 4.453638722819667\n",
      "Epoch: 20, step: 0, loss: 4.382090091705322\n",
      "Epoch: 20, step: 5, loss: 4.431425015131633\n",
      "Epoch: 20, step: 10, loss: 4.4247071092778985\n",
      "Epoch: 20, step: 15, loss: 4.44039049744606\n",
      "Epoch: 20, step: 20, loss: 4.451309022449312\n",
      "Epoch: 20, step: 25, loss: 4.437336499874409\n",
      "Epoch: 20, step: 30, loss: 4.435624821980794\n",
      "Epoch: 21, step: 0, loss: 4.5103912353515625\n",
      "Epoch: 21, step: 5, loss: 4.427143812179565\n",
      "Epoch: 21, step: 10, loss: 4.4333367781205615\n",
      "Epoch: 21, step: 15, loss: 4.4306976199150085\n",
      "Epoch: 21, step: 20, loss: 4.423204558236258\n",
      "Epoch: 21, step: 25, loss: 4.421876357151912\n",
      "Epoch: 21, step: 30, loss: 4.418711785347231\n",
      "Epoch: 22, step: 0, loss: 4.401561737060547\n",
      "Epoch: 22, step: 5, loss: 4.418476740519206\n",
      "Epoch: 22, step: 10, loss: 4.42664198441939\n",
      "Epoch: 22, step: 15, loss: 4.397139459848404\n",
      "Epoch: 22, step: 20, loss: 4.396567912328811\n",
      "Epoch: 22, step: 25, loss: 4.394244249050434\n",
      "Epoch: 22, step: 30, loss: 4.3982727604527625\n",
      "Epoch: 23, step: 0, loss: 4.384758472442627\n",
      "Epoch: 23, step: 5, loss: 4.417205731074016\n",
      "Epoch: 23, step: 10, loss: 4.413709943944758\n",
      "Epoch: 23, step: 15, loss: 4.401123017072678\n",
      "Epoch: 23, step: 20, loss: 4.395161946614583\n",
      "Epoch: 23, step: 25, loss: 4.381174325942993\n",
      "Epoch: 23, step: 30, loss: 4.384042324558381\n",
      "Epoch: 24, step: 0, loss: 4.261902809143066\n",
      "Epoch: 24, step: 5, loss: 4.377643982569377\n",
      "Epoch: 24, step: 10, loss: 4.3845203139565205\n",
      "Epoch: 24, step: 15, loss: 4.374221831560135\n",
      "Epoch: 24, step: 20, loss: 4.372340225038075\n",
      "Epoch: 24, step: 25, loss: 4.369353386072012\n",
      "Epoch: 24, step: 30, loss: 4.365106367295788\n",
      "Epoch: 25, step: 0, loss: 4.425210952758789\n",
      "Epoch: 25, step: 5, loss: 4.38227121035258\n",
      "Epoch: 25, step: 10, loss: 4.361270904541016\n",
      "Epoch: 25, step: 15, loss: 4.341923840840658\n",
      "Epoch: 25, step: 20, loss: 4.337948441505432\n",
      "Epoch: 25, step: 25, loss: 4.335164756774902\n",
      "Epoch: 25, step: 30, loss: 4.346777153015137\n",
      "Epoch: 26, step: 0, loss: 4.399543762207031\n",
      "Epoch: 26, step: 5, loss: 4.344483455022176\n",
      "Epoch: 26, step: 10, loss: 4.349455096504905\n",
      "Epoch: 26, step: 15, loss: 4.33457088470459\n",
      "Epoch: 26, step: 25, loss: 4.33227617263794\n",
      "Epoch: 26, step: 30, loss: 4.32861590385437\n",
      "Epoch: 27, step: 0, loss: 4.440014839172363\n",
      "Epoch: 27, step: 5, loss: 4.341127316157023\n",
      "Epoch: 27, step: 10, loss: 4.320350473577326\n",
      "Epoch: 27, step: 15, loss: 4.315680295228958\n",
      "Epoch: 27, step: 20, loss: 4.322071120852516\n",
      "Epoch: 27, step: 25, loss: 4.317280989426833\n",
      "Epoch: 27, step: 30, loss: 4.306695645855319\n",
      "Epoch: 28, step: 0, loss: 4.318606853485107\n",
      "Epoch: 28, step: 10, loss: 4.295990991592407\n",
      "Epoch: 28, step: 15, loss: 4.292465623219808\n",
      "Epoch: 28, step: 20, loss: 4.286653280258179\n",
      "Epoch: 28, step: 25, loss: 4.290041866302491\n",
      "Epoch: 29, step: 0, loss: 4.295118808746338\n",
      "Epoch: 29, step: 5, loss: 4.246941804885864\n",
      "Epoch: 29, step: 10, loss: 4.27055684002963\n",
      "Epoch: 29, step: 15, loss: 4.25615981221199\n",
      "Epoch: 29, step: 20, loss: 4.2508860769725985\n",
      "Epoch: 29, step: 25, loss: 4.252684574860793\n",
      "Epoch: 29, step: 30, loss: 4.246163245170347\n",
      "Epoch: 30, step: 0, loss: 4.252253532409668\n",
      "Epoch: 30, step: 5, loss: 4.2133154074351\n",
      "Epoch: 30, step: 10, loss: 4.223074609583074\n",
      "Epoch: 30, step: 15, loss: 4.208710283041\n",
      "Epoch: 30, step: 20, loss: 4.208338237944103\n",
      "Epoch: 30, step: 25, loss: 4.213514456382165\n",
      "Epoch: 30, step: 30, loss: 4.212486082507718\n",
      "Epoch: 31, step: 0, loss: 4.146242618560791\n",
      "Epoch: 31, step: 5, loss: 4.2107462882995605\n",
      "Epoch: 31, step: 10, loss: 4.195814436132258\n",
      "Epoch: 31, step: 15, loss: 4.192419856786728\n",
      "Epoch: 31, step: 20, loss: 4.198321683066232\n",
      "Epoch: 31, step: 25, loss: 4.1979361314039965\n",
      "Epoch: 31, step: 30, loss: 4.187685335836103\n",
      "Epoch: 32, step: 0, loss: 4.103456497192383\n",
      "Epoch: 32, step: 5, loss: 4.1055301030476885\n",
      "Epoch: 32, step: 10, loss: 4.120684667067095\n",
      "Epoch: 32, step: 15, loss: 4.1438756585121155\n",
      "Epoch: 32, step: 20, loss: 4.1493160384041925\n",
      "Epoch: 32, step: 25, loss: 4.149582532735971\n",
      "Epoch: 32, step: 30, loss: 4.151370925288046\n",
      "Epoch: 33, step: 0, loss: 4.181584358215332\n",
      "Epoch: 33, step: 5, loss: 4.163533846537272\n",
      "Epoch: 33, step: 10, loss: 4.177759777415883\n",
      "Epoch: 33, step: 15, loss: 4.161840707063675\n",
      "Epoch: 33, step: 20, loss: 4.153828666323707\n",
      "Epoch: 33, step: 30, loss: 4.124595514933268\n",
      "Epoch: 34, step: 0, loss: 4.13352108001709\n",
      "Epoch: 34, step: 5, loss: 4.078378518422444\n",
      "Epoch: 34, step: 10, loss: 4.097421602769331\n",
      "Epoch: 34, step: 15, loss: 4.103819966316223\n",
      "Epoch: 34, step: 20, loss: 4.096635443823678\n",
      "Epoch: 34, step: 25, loss: 4.0969213614097\n",
      "Epoch: 34, step: 30, loss: 4.096513925060149\n",
      "Epoch: 35, step: 0, loss: 3.9857802391052246\n",
      "Epoch: 35, step: 5, loss: 4.031144102414449\n",
      "Epoch: 35, step: 10, loss: 4.054699182510376\n",
      "Epoch: 35, step: 15, loss: 4.067825570702553\n",
      "Epoch: 35, step: 20, loss: 4.060913755780175\n",
      "Epoch: 35, step: 25, loss: 4.066644586049593\n",
      "Epoch: 35, step: 30, loss: 4.07149455624242\n",
      "Epoch: 36, step: 0, loss: 4.161187171936035\n",
      "Epoch: 36, step: 5, loss: 4.048131187756856\n",
      "Epoch: 36, step: 10, loss: 4.0605102669108994\n",
      "Epoch: 36, step: 15, loss: 4.065113469958305\n",
      "Epoch: 36, step: 20, loss: 4.063838402430217\n",
      "Epoch: 36, step: 25, loss: 4.060590166311997\n",
      "Epoch: 36, step: 30, loss: 4.0547235858055855\n",
      "Epoch: 37, step: 0, loss: 4.1099395751953125\n",
      "Epoch: 37, step: 5, loss: 4.077557524045308\n",
      "Epoch: 37, step: 10, loss: 4.050842870365489\n",
      "Epoch: 37, step: 15, loss: 4.031393125653267\n",
      "Epoch: 37, step: 25, loss: 4.035382013320923\n",
      "Epoch: 37, step: 30, loss: 4.030367477734884\n",
      "Epoch: 38, step: 0, loss: 4.130087852478027\n",
      "Epoch: 38, step: 5, loss: 4.011348684628804\n",
      "Epoch: 38, step: 10, loss: 4.0252672758969394\n",
      "Epoch: 38, step: 15, loss: 4.027523398399353\n",
      "Epoch: 38, step: 20, loss: 4.013231572650728\n",
      "Epoch: 38, step: 25, loss: 4.013361811637878\n",
      "Epoch: 38, step: 30, loss: 4.017817958708732\n",
      "Epoch: 39, step: 0, loss: 4.060812950134277\n",
      "Epoch: 39, step: 5, loss: 3.9694800774256387\n",
      "Epoch: 39, step: 10, loss: 4.005036917599765\n",
      "Epoch: 39, step: 15, loss: 4.027315318584442\n",
      "Epoch: 39, step: 20, loss: 4.02160104115804\n",
      "Epoch: 39, step: 25, loss: 4.014969688195449\n",
      "Epoch: 39, step: 30, loss: 3.99883828163147\n",
      "Epoch: 40, step: 0, loss: 3.9007833003997803\n",
      "Epoch: 40, step: 5, loss: 3.926084804534912\n",
      "Epoch: 40, step: 10, loss: 3.9656768798828126\n",
      "Epoch: 40, step: 15, loss: 3.971853510538737\n",
      "Epoch: 40, step: 20, loss: 3.9887726306915283\n",
      "Epoch: 40, step: 25, loss: 3.983974657058716\n",
      "Epoch: 40, step: 30, loss: 3.9883391936620076\n",
      "Epoch: 41, step: 0, loss: 3.9617204666137695\n",
      "Epoch: 41, step: 5, loss: 3.9764546950658164\n",
      "Epoch: 41, step: 10, loss: 3.9874014854431152\n",
      "Epoch: 41, step: 15, loss: 3.9841731786727905\n",
      "Epoch: 41, step: 20, loss: 3.9752265498751687\n",
      "Epoch: 41, step: 25, loss: 3.977894434562096\n",
      "Epoch: 41, step: 30, loss: 3.9725001089034544\n",
      "Epoch: 42, step: 0, loss: 3.9091033935546875\n",
      "Epoch: 42, step: 5, loss: 3.9963047901789346\n",
      "Epoch: 42, step: 10, loss: 3.9667130383578213\n",
      "Epoch: 42, step: 15, loss: 3.959733560681343\n",
      "Epoch: 42, step: 20, loss: 3.9495328608013334\n",
      "Epoch: 42, step: 25, loss: 3.9543852439293494\n",
      "Epoch: 42, step: 30, loss: 3.957159234631446\n",
      "Epoch: 43, step: 0, loss: 3.959282875061035\n",
      "Epoch: 43, step: 5, loss: 3.9494698524475096\n",
      "Epoch: 43, step: 10, loss: 3.9507044315338136\n",
      "Epoch: 43, step: 15, loss: 3.9500201384226483\n",
      "Epoch: 43, step: 20, loss: 3.953397274017334\n",
      "Epoch: 43, step: 25, loss: 3.9532721424102784\n",
      "Epoch: 43, step: 30, loss: 3.9558568318684895\n",
      "Epoch: 44, step: 0, loss: 3.9850850105285645\n",
      "Epoch: 44, step: 5, loss: 3.980828801790873\n",
      "Epoch: 44, step: 10, loss: 3.9429045373743232\n",
      "Epoch: 44, step: 15, loss: 3.941933289170265\n",
      "Epoch: 44, step: 20, loss: 3.9299826622009277\n",
      "Epoch: 44, step: 25, loss: 3.933102616896996\n",
      "Epoch: 44, step: 30, loss: 3.937822218864195\n",
      "Epoch: 45, step: 0, loss: 3.9125919342041016\n",
      "Epoch: 45, step: 5, loss: 3.911859671274821\n",
      "Epoch: 45, step: 10, loss: 3.918652209368619\n",
      "Epoch: 45, step: 15, loss: 3.923163577914238\n",
      "Epoch: 45, step: 20, loss: 3.9228968620300293\n",
      "Epoch: 45, step: 25, loss: 3.9252977187816915\n",
      "Epoch: 45, step: 30, loss: 3.9280690916122927\n",
      "Epoch: 46, step: 0, loss: 3.9285593032836914\n",
      "Epoch: 46, step: 5, loss: 3.925232768058777\n",
      "Epoch: 46, step: 10, loss: 3.911680590022694\n",
      "Epoch: 46, step: 15, loss: 3.9179261177778244\n",
      "Epoch: 46, step: 20, loss: 3.9212526934487477\n",
      "Epoch: 46, step: 25, loss: 3.916413353039668\n",
      "Epoch: 46, step: 30, loss: 3.914625706211213\n",
      "Epoch: 47, step: 0, loss: 3.8644614219665527\n",
      "Epoch: 47, step: 5, loss: 3.877063512802124\n",
      "Epoch: 47, step: 10, loss: 3.8980886069211094\n",
      "Epoch: 47, step: 15, loss: 3.9121985137462616\n",
      "Epoch: 47, step: 20, loss: 3.9114741597856795\n",
      "Epoch: 47, step: 25, loss: 3.912243907268231\n",
      "Epoch: 47, step: 30, loss: 3.9042370934640207\n",
      "Epoch: 48, step: 0, loss: 3.8654446601867676\n",
      "Epoch: 48, step: 5, loss: 3.8350114822387695\n",
      "Epoch: 48, step: 10, loss: 3.87541372125799\n",
      "Epoch: 48, step: 15, loss: 3.885987773537636\n",
      "Epoch: 48, step: 20, loss: 3.8951689969925654\n",
      "Epoch: 48, step: 25, loss: 3.9005325665840735\n",
      "Epoch: 48, step: 30, loss: 3.903453319303451\n",
      "Epoch: 49, step: 0, loss: 3.893198013305664\n",
      "Epoch: 49, step: 5, loss: 3.883536418279012\n",
      "Epoch: 49, step: 10, loss: 3.8786445530978115\n",
      "Epoch: 49, step: 15, loss: 3.8853198140859604\n",
      "Epoch: 49, step: 20, loss: 3.8867013567969915\n",
      "Epoch: 49, step: 25, loss: 3.889679486934955\n",
      "Epoch: 49, step: 30, loss: 3.885603166395618\n",
      "Epoch: 50, step: 0, loss: 3.8535051345825195\n",
      "Epoch: 50, step: 5, loss: 3.8629923661549888\n",
      "Epoch: 50, step: 10, loss: 3.8714859268882056\n",
      "Epoch: 50, step: 15, loss: 3.8644840866327286\n",
      "Epoch: 50, step: 20, loss: 3.877278975078038\n",
      "Epoch: 50, step: 25, loss: 3.876526740881113\n",
      "Epoch: 50, step: 30, loss: 3.8759855378058647\n",
      "Epoch: 51, step: 0, loss: 3.966092824935913\n",
      "Epoch: 51, step: 5, loss: 3.8625088135401406\n",
      "Epoch: 51, step: 10, loss: 3.865313855084506\n",
      "Epoch: 51, step: 15, loss: 3.860205739736557\n",
      "Epoch: 51, step: 20, loss: 3.8605489957900274\n",
      "Epoch: 51, step: 25, loss: 3.858204887463496\n",
      "Epoch: 51, step: 30, loss: 3.87162826138158\n",
      "Epoch: 52, step: 0, loss: 3.929370403289795\n",
      "Epoch: 52, step: 5, loss: 3.855287472407023\n",
      "Epoch: 52, step: 10, loss: 3.8689782185987993\n",
      "Epoch: 52, step: 15, loss: 3.8736654072999954\n",
      "Epoch: 52, step: 20, loss: 3.879377217519851\n",
      "Epoch: 52, step: 25, loss: 3.87857330762423\n",
      "Epoch: 52, step: 30, loss: 3.8613226721363683\n",
      "Epoch: 53, step: 0, loss: 3.8544466495513916\n",
      "Epoch: 53, step: 5, loss: 3.841851751009623\n",
      "Epoch: 53, step: 10, loss: 3.8653013706207275\n",
      "Epoch: 53, step: 15, loss: 3.8587453961372375\n",
      "Epoch: 53, step: 20, loss: 3.855636891864595\n",
      "Epoch: 53, step: 25, loss: 3.856606355080238\n",
      "Epoch: 53, step: 30, loss: 3.851943585180467\n",
      "Epoch: 54, step: 0, loss: 3.7974164485931396\n",
      "Epoch: 54, step: 5, loss: 3.864392797152201\n",
      "Epoch: 54, step: 10, loss: 3.844242724505338\n",
      "Epoch: 54, step: 15, loss: 3.847020775079727\n",
      "Epoch: 54, step: 20, loss: 3.8415109997703913\n",
      "Epoch: 54, step: 25, loss: 3.843719931749197\n",
      "Epoch: 54, step: 30, loss: 3.8431139299946446\n",
      "Epoch: 55, step: 0, loss: 3.9484028816223145\n",
      "Epoch: 55, step: 5, loss: 3.8636093139648438\n",
      "Epoch: 55, step: 10, loss: 3.8314939628947866\n",
      "Epoch: 55, step: 15, loss: 3.841444820165634\n",
      "Epoch: 55, step: 20, loss: 3.8459839934394475\n",
      "Epoch: 55, step: 25, loss: 3.8366182033832255\n",
      "Epoch: 55, step: 30, loss: 3.836160429062382\n",
      "Epoch: 56, step: 0, loss: 3.902945041656494\n",
      "Epoch: 56, step: 5, loss: 3.825183947881063\n",
      "Epoch: 56, step: 10, loss: 3.8183947801589966\n",
      "Epoch: 56, step: 15, loss: 3.8335222085316976\n",
      "Epoch: 56, step: 20, loss: 3.8318996906280516\n",
      "Epoch: 56, step: 25, loss: 3.830456380844116\n",
      "Epoch: 56, step: 30, loss: 3.8331223646799724\n",
      "Epoch: 57, step: 0, loss: 3.6735148429870605\n",
      "Epoch: 57, step: 5, loss: 3.778056263923645\n",
      "Epoch: 57, step: 10, loss: 3.7952499172904273\n",
      "Epoch: 57, step: 15, loss: 3.803866922855377\n",
      "Epoch: 57, step: 20, loss: 3.8157690366109214\n",
      "Epoch: 57, step: 25, loss: 3.8243264968578634\n",
      "Epoch: 57, step: 30, loss: 3.8235093931997977\n",
      "Epoch: 58, step: 0, loss: 3.90134334564209\n",
      "Epoch: 58, step: 5, loss: 3.8360506296157837\n",
      "Epoch: 58, step: 10, loss: 3.8349504709243774\n",
      "Epoch: 58, step: 15, loss: 3.830448547999064\n",
      "Epoch: 58, step: 20, loss: 3.8268548250198364\n",
      "Epoch: 58, step: 25, loss: 3.8227281093597414\n",
      "Epoch: 58, step: 30, loss: 3.813967808087667\n",
      "Epoch: 59, step: 0, loss: 3.780844211578369\n",
      "Epoch: 59, step: 5, loss: 3.7637224992116294\n",
      "Epoch: 59, step: 10, loss: 3.779743021184748\n",
      "Epoch: 59, step: 15, loss: 3.794923409819603\n",
      "Epoch: 59, step: 20, loss: 3.8062676815759566\n",
      "Epoch: 59, step: 25, loss: 3.8037230418278623\n",
      "Epoch: 59, step: 30, loss: 3.8112454645095335\n",
      "Epoch: 60, step: 0, loss: 3.821439027786255\n",
      "Epoch: 60, step: 5, loss: 3.807902733484904\n",
      "Epoch: 60, step: 10, loss: 3.821808403188532\n",
      "Epoch: 60, step: 15, loss: 3.8010610938072205\n",
      "Epoch: 60, step: 20, loss: 3.7960397515978133\n",
      "Epoch: 60, step: 25, loss: 3.805075865525466\n",
      "Epoch: 60, step: 30, loss: 3.8007768507926696\n",
      "Epoch: 61, step: 0, loss: 3.817089557647705\n",
      "Epoch: 61, step: 5, loss: 3.770830583572388\n",
      "Epoch: 61, step: 10, loss: 3.8081780672073364\n",
      "Epoch: 61, step: 15, loss: 3.806336784362793\n",
      "Epoch: 61, step: 20, loss: 3.7976765751838686\n",
      "Epoch: 61, step: 25, loss: 3.7900180053710937\n",
      "Epoch: 61, step: 30, loss: 3.793491506576538\n",
      "Epoch: 62, step: 0, loss: 3.851950168609619\n",
      "Epoch: 62, step: 5, loss: 3.814560850461324\n",
      "Epoch: 62, step: 10, loss: 3.7938455668362705\n",
      "Epoch: 62, step: 15, loss: 3.808513194322586\n",
      "Epoch: 62, step: 20, loss: 3.8098995327949523\n",
      "Epoch: 62, step: 25, loss: 3.7968343544006347\n",
      "Epoch: 62, step: 30, loss: 3.7935678482055666\n",
      "Epoch: 63, step: 0, loss: 3.837141990661621\n",
      "Epoch: 63, step: 5, loss: 3.7757771015167236\n",
      "Epoch: 63, step: 10, loss: 3.7902515801516445\n",
      "Epoch: 63, step: 15, loss: 3.7860089391469955\n",
      "Epoch: 63, step: 20, loss: 3.787540015720186\n",
      "Epoch: 63, step: 25, loss: 3.7813094762655406\n",
      "Epoch: 63, step: 30, loss: 3.7820197766827\n",
      "Epoch: 64, step: 0, loss: 3.671180486679077\n",
      "Epoch: 64, step: 5, loss: 3.7706812222798667\n",
      "Epoch: 64, step: 10, loss: 3.7862744331359863\n",
      "Epoch: 64, step: 15, loss: 3.7789063453674316\n",
      "Epoch: 64, step: 20, loss: 3.781352917353312\n",
      "Epoch: 64, step: 25, loss: 3.7754838191545925\n",
      "Epoch: 64, step: 30, loss: 3.7792248956618772\n",
      "Epoch: 65, step: 0, loss: 3.754237651824951\n",
      "Epoch: 65, step: 5, loss: 3.705314119656881\n",
      "Epoch: 65, step: 10, loss: 3.740517941388217\n",
      "Epoch: 65, step: 15, loss: 3.755533367395401\n",
      "Epoch: 65, step: 20, loss: 3.7746795813242593\n",
      "Epoch: 65, step: 25, loss: 3.7756367279933047\n",
      "Epoch: 65, step: 30, loss: 3.769507308160105\n",
      "Epoch: 66, step: 0, loss: 3.759243965148926\n",
      "Epoch: 66, step: 5, loss: 3.7577457825342813\n",
      "Epoch: 66, step: 10, loss: 3.7595854239030317\n",
      "Epoch: 66, step: 15, loss: 3.761911988258362\n",
      "Epoch: 66, step: 20, loss: 3.761242775689988\n",
      "Epoch: 66, step: 25, loss: 3.7705999337709866\n",
      "Epoch: 66, step: 30, loss: 3.7615772370369203\n",
      "Epoch: 67, step: 0, loss: 3.6382527351379395\n",
      "Epoch: 67, step: 5, loss: 3.7606842517852783\n",
      "Epoch: 67, step: 10, loss: 3.7829923196272417\n",
      "Epoch: 67, step: 15, loss: 3.7817674726247787\n",
      "Epoch: 67, step: 20, loss: 3.768245469956171\n",
      "Epoch: 67, step: 25, loss: 3.7694337368011475\n",
      "Epoch: 67, step: 30, loss: 3.762073770646126\n",
      "Epoch: 68, step: 0, loss: 3.7163636684417725\n",
      "Epoch: 68, step: 5, loss: 3.6733283599217734\n",
      "Epoch: 68, step: 10, loss: 3.732670393857089\n",
      "Epoch: 68, step: 15, loss: 3.731182247400284\n",
      "Epoch: 68, step: 20, loss: 3.744104657854353\n",
      "Epoch: 68, step: 25, loss: 3.7600346436867347\n",
      "Epoch: 68, step: 30, loss: 3.754938348647087\n",
      "Epoch: 69, step: 0, loss: 3.823850154876709\n",
      "Epoch: 69, step: 5, loss: 3.7486348152160645\n",
      "Epoch: 69, step: 10, loss: 3.7423865361647173\n",
      "Epoch: 69, step: 15, loss: 3.742800459265709\n",
      "Epoch: 69, step: 20, loss: 3.739801111675444\n",
      "Epoch: 69, step: 25, loss: 3.743105622438284\n",
      "Epoch: 69, step: 30, loss: 3.750687422290925\n",
      "Epoch: 70, step: 0, loss: 3.887958526611328\n",
      "Epoch: 70, step: 5, loss: 3.819418986638387\n",
      "Epoch: 70, step: 10, loss: 3.777299490841952\n",
      "Epoch: 70, step: 15, loss: 3.765994042158127\n",
      "Epoch: 70, step: 20, loss: 3.758438689368112\n",
      "Epoch: 70, step: 25, loss: 3.759447556275588\n",
      "Epoch: 70, step: 30, loss: 3.745627826259982\n",
      "Epoch: 71, step: 0, loss: 3.662492275238037\n",
      "Epoch: 71, step: 5, loss: 3.7778770526250205\n",
      "Epoch: 71, step: 15, loss: 3.7347780068715415\n",
      "Epoch: 71, step: 20, loss: 3.7500365018844604\n",
      "Epoch: 71, step: 25, loss: 3.7408262634277345\n",
      "Epoch: 71, step: 30, loss: 3.7432720184326174\n",
      "Epoch: 72, step: 0, loss: 3.731290817260742\n",
      "Epoch: 72, step: 5, loss: 3.75155246257782\n",
      "Epoch: 72, step: 10, loss: 3.747283762151545\n",
      "Epoch: 72, step: 15, loss: 3.73947910964489\n",
      "Epoch: 72, step: 20, loss: 3.7382790815262568\n",
      "Epoch: 72, step: 25, loss: 3.743584935481732\n",
      "Epoch: 72, step: 30, loss: 3.7396874735432286\n",
      "Epoch: 73, step: 0, loss: 3.7694458961486816\n",
      "Epoch: 73, step: 5, loss: 3.7565788427988687\n",
      "Epoch: 73, step: 10, loss: 3.758167787031694\n",
      "Epoch: 73, step: 15, loss: 3.7421538829803467\n",
      "Epoch: 73, step: 20, loss: 3.7404189904530845\n",
      "Epoch: 73, step: 25, loss: 3.7340561610001783\n",
      "Epoch: 73, step: 30, loss: 3.7264581341897287\n",
      "Epoch: 74, step: 0, loss: 3.616255760192871\n",
      "Epoch: 74, step: 5, loss: 3.7345596154530845\n",
      "Epoch: 74, step: 10, loss: 3.729584823955189\n",
      "Epoch: 74, step: 15, loss: 3.7318551391363144\n",
      "Epoch: 74, step: 20, loss: 3.731780529022217\n",
      "Epoch: 74, step: 25, loss: 3.728607205244211\n",
      "Epoch: 74, step: 30, loss: 3.7274735666090444\n",
      "Epoch: 75, step: 0, loss: 3.611234188079834\n",
      "Epoch: 75, step: 5, loss: 3.721047560373942\n",
      "Epoch: 75, step: 10, loss: 3.7254961837421763\n",
      "Epoch: 75, step: 15, loss: 3.721343442797661\n",
      "Epoch: 75, step: 20, loss: 3.7154548054649714\n",
      "Epoch: 75, step: 25, loss: 3.725330242743859\n",
      "Epoch: 75, step: 30, loss: 3.725191262460524\n",
      "Epoch: 76, step: 0, loss: 3.8380236625671387\n",
      "Epoch: 76, step: 5, loss: 3.766379197438558\n",
      "Epoch: 76, step: 10, loss: 3.755499146201394\n",
      "Epoch: 76, step: 15, loss: 3.7340497225522995\n",
      "Epoch: 76, step: 20, loss: 3.7190481140500022\n",
      "Epoch: 76, step: 25, loss: 3.7325555361234226\n",
      "Epoch: 76, step: 30, loss: 3.7217673486278904\n",
      "Epoch: 77, step: 0, loss: 3.854008674621582\n",
      "Epoch: 77, step: 5, loss: 3.7073238293329873\n",
      "Epoch: 77, step: 10, loss: 3.710604277524081\n",
      "Epoch: 77, step: 15, loss: 3.713789224624634\n",
      "Epoch: 77, step: 20, loss: 3.7140662443070185\n",
      "Epoch: 77, step: 25, loss: 3.7101708008692813\n",
      "Epoch: 77, step: 30, loss: 3.7120303184755388\n",
      "Epoch: 78, step: 0, loss: 3.7151918411254883\n",
      "Epoch: 78, step: 5, loss: 3.677901268005371\n",
      "Epoch: 78, step: 10, loss: 3.677727374163541\n",
      "Epoch: 78, step: 15, loss: 3.6965566724538803\n",
      "Epoch: 78, step: 20, loss: 3.705186650866554\n",
      "Epoch: 78, step: 25, loss: 3.711882609587449\n",
      "Epoch: 78, step: 30, loss: 3.711577638503044\n",
      "Epoch: 79, step: 0, loss: 3.7538981437683105\n",
      "Epoch: 79, step: 5, loss: 3.6883463859558105\n",
      "Epoch: 79, step: 10, loss: 3.6898572011427446\n",
      "Epoch: 79, step: 15, loss: 3.7072481513023376\n",
      "Epoch: 79, step: 20, loss: 3.7040395623161677\n",
      "Epoch: 79, step: 25, loss: 3.711049896020156\n",
      "Epoch: 79, step: 30, loss: 3.7101379902132097\n",
      "Epoch: 80, step: 0, loss: 3.6508517265319824\n",
      "Epoch: 80, step: 5, loss: 3.676231861114502\n",
      "Epoch: 80, step: 10, loss: 3.685500665144487\n",
      "Epoch: 80, step: 15, loss: 3.7004427164793015\n",
      "Epoch: 80, step: 20, loss: 3.6992157186780656\n",
      "Epoch: 80, step: 25, loss: 3.712221246499282\n",
      "Epoch: 80, step: 30, loss: 3.701862189077562\n",
      "Epoch: 81, step: 0, loss: 3.722360134124756\n",
      "Epoch: 81, step: 5, loss: 3.7102756102879844\n",
      "Epoch: 81, step: 10, loss: 3.7161485715345903\n",
      "Epoch: 81, step: 15, loss: 3.708890601992607\n",
      "Epoch: 81, step: 20, loss: 3.7112700939178467\n",
      "Epoch: 81, step: 25, loss: 3.699413143671476\n",
      "Epoch: 81, step: 30, loss: 3.697507699330648\n",
      "Epoch: 82, step: 0, loss: 3.8470029830932617\n",
      "Epoch: 82, step: 5, loss: 3.720639149347941\n",
      "Epoch: 82, step: 10, loss: 3.6925112334164707\n",
      "Epoch: 82, step: 15, loss: 3.709096923470497\n",
      "Epoch: 82, step: 20, loss: 3.69010059038798\n",
      "Epoch: 82, step: 25, loss: 3.698382487663856\n",
      "Epoch: 82, step: 30, loss: 3.695352054411365\n",
      "Epoch: 83, step: 0, loss: 3.554422378540039\n",
      "Epoch: 83, step: 5, loss: 3.6813031832377114\n",
      "Epoch: 83, step: 10, loss: 3.6873940988020464\n",
      "Epoch: 83, step: 15, loss: 3.6816795021295547\n",
      "Epoch: 83, step: 20, loss: 3.671327057338896\n",
      "Epoch: 83, step: 25, loss: 3.6861833333969116\n",
      "Epoch: 83, step: 30, loss: 3.689324717367849\n",
      "Epoch: 84, step: 0, loss: 3.853717088699341\n",
      "Epoch: 84, step: 5, loss: 3.7068224350611367\n",
      "Epoch: 84, step: 10, loss: 3.7029718485745517\n",
      "Epoch: 84, step: 15, loss: 3.67500901222229\n",
      "Epoch: 84, step: 20, loss: 3.6836137431008473\n",
      "Epoch: 84, step: 25, loss: 3.689457113926227\n",
      "Epoch: 84, step: 30, loss: 3.6897438649208314\n",
      "Epoch: 85, step: 0, loss: 3.6913137435913086\n",
      "Epoch: 85, step: 5, loss: 3.692174752553304\n",
      "Epoch: 85, step: 10, loss: 3.672254952517423\n",
      "Epoch: 85, step: 15, loss: 3.6941462010145187\n",
      "Epoch: 85, step: 20, loss: 3.6937853268214633\n",
      "Epoch: 85, step: 25, loss: 3.6951635434077335\n",
      "Epoch: 85, step: 30, loss: 3.68872401022142\n",
      "Epoch: 86, step: 0, loss: 3.7517638206481934\n",
      "Epoch: 86, step: 5, loss: 3.719364802042643\n",
      "Epoch: 86, step: 10, loss: 3.709234833717346\n",
      "Epoch: 86, step: 15, loss: 3.702616326014201\n",
      "Epoch: 86, step: 20, loss: 3.692044758796692\n",
      "Epoch: 86, step: 25, loss: 3.685041884581248\n",
      "Epoch: 86, step: 30, loss: 3.674929454408843\n",
      "Epoch: 87, step: 0, loss: 3.768486738204956\n",
      "Epoch: 87, step: 5, loss: 3.6443148056666055\n",
      "Epoch: 87, step: 10, loss: 3.659780589017001\n",
      "Epoch: 87, step: 15, loss: 3.6558489948511124\n",
      "Epoch: 87, step: 20, loss: 3.6545857588450112\n",
      "Epoch: 87, step: 25, loss: 3.667043456664452\n",
      "Epoch: 87, step: 30, loss: 3.677921087511124\n",
      "Epoch: 88, step: 0, loss: 3.7213563919067383\n",
      "Epoch: 88, step: 5, loss: 3.7115435202916465\n",
      "Epoch: 88, step: 10, loss: 3.679518309506503\n",
      "Epoch: 88, step: 15, loss: 3.66952982544899\n",
      "Epoch: 88, step: 20, loss: 3.669597648438953\n",
      "Epoch: 88, step: 25, loss: 3.6827015601671658\n",
      "Epoch: 88, step: 30, loss: 3.6765026892385175\n",
      "Epoch: 89, step: 0, loss: 3.632584571838379\n",
      "Epoch: 89, step: 5, loss: 3.6435482104619346\n",
      "Epoch: 89, step: 10, loss: 3.66146766055714\n",
      "Epoch: 89, step: 15, loss: 3.6828167885541916\n",
      "Epoch: 89, step: 20, loss: 3.68611825080145\n",
      "Epoch: 89, step: 25, loss: 3.679446743084834\n",
      "Epoch: 89, step: 30, loss: 3.673443063612907\n",
      "Epoch: 90, step: 0, loss: 3.4751198291778564\n",
      "Epoch: 90, step: 5, loss: 3.630687157313029\n",
      "Epoch: 90, step: 10, loss: 3.6625100699338047\n",
      "Epoch: 90, step: 15, loss: 3.6559162586927414\n",
      "Epoch: 90, step: 20, loss: 3.6629135268075124\n",
      "Epoch: 90, step: 25, loss: 3.6677125233870287\n",
      "Epoch: 90, step: 30, loss: 3.669377919166319\n",
      "Epoch: 91, step: 0, loss: 3.7372090816497803\n",
      "Epoch: 91, step: 5, loss: 3.6837750673294067\n",
      "Epoch: 91, step: 10, loss: 3.6531031781976875\n",
      "Epoch: 91, step: 15, loss: 3.652624726295471\n",
      "Epoch: 91, step: 20, loss: 3.6733338594436646\n",
      "Epoch: 91, step: 30, loss: 3.663184503029133\n",
      "Epoch: 92, step: 0, loss: 3.653064727783203\n",
      "Epoch: 92, step: 5, loss: 3.732623259226481\n",
      "Epoch: 92, step: 10, loss: 3.7180718291889536\n",
      "Epoch: 92, step: 15, loss: 3.6962494254112244\n",
      "Epoch: 92, step: 20, loss: 3.686599856331235\n",
      "Epoch: 92, step: 25, loss: 3.6697474809793325\n",
      "Epoch: 92, step: 30, loss: 3.6669100792177263\n",
      "Epoch: 93, step: 0, loss: 3.786850929260254\n",
      "Epoch: 93, step: 5, loss: 3.6981641054153442\n",
      "Epoch: 93, step: 10, loss: 3.6649019067937676\n",
      "Epoch: 93, step: 15, loss: 3.6637712568044662\n",
      "Epoch: 93, step: 20, loss: 3.653750419616699\n",
      "Epoch: 93, step: 25, loss: 3.658412273113544\n",
      "Epoch: 93, step: 30, loss: 3.6603265962293072\n",
      "Epoch: 94, step: 0, loss: 3.737781524658203\n",
      "Epoch: 94, step: 5, loss: 3.6052565574645996\n",
      "Epoch: 94, step: 10, loss: 3.612617167559537\n",
      "Epoch: 94, step: 15, loss: 3.6441388577222824\n",
      "Epoch: 94, step: 20, loss: 3.6452756382170177\n",
      "Epoch: 94, step: 25, loss: 3.6531161344968357\n",
      "Epoch: 94, step: 30, loss: 3.6591595757392144\n",
      "Epoch: 95, step: 0, loss: 3.574800968170166\n",
      "Epoch: 95, step: 5, loss: 3.636269211769104\n",
      "Epoch: 95, step: 10, loss: 3.6384572332555596\n",
      "Epoch: 95, step: 15, loss: 3.648225650191307\n",
      "Epoch: 95, step: 20, loss: 3.6440908114115396\n",
      "Epoch: 95, step: 25, loss: 3.655677547821632\n",
      "Epoch: 95, step: 30, loss: 3.6529284831016295\n",
      "Epoch: 96, step: 0, loss: 3.676571846008301\n",
      "Epoch: 96, step: 5, loss: 3.6870266596476235\n",
      "Epoch: 96, step: 10, loss: 3.6820309162139893\n",
      "Epoch: 96, step: 15, loss: 3.6666805744171143\n",
      "Epoch: 96, step: 20, loss: 3.656016224906558\n",
      "Epoch: 96, step: 25, loss: 3.6537589843456564\n",
      "Epoch: 96, step: 30, loss: 3.6496328769191617\n",
      "Epoch: 97, step: 0, loss: 3.620438814163208\n",
      "Epoch: 97, step: 5, loss: 3.6567867199579873\n",
      "Epoch: 97, step: 10, loss: 3.674370137127963\n",
      "Epoch: 97, step: 15, loss: 3.656446263194084\n",
      "Epoch: 97, step: 20, loss: 3.6419736317225864\n",
      "Epoch: 97, step: 25, loss: 3.6444351856525126\n",
      "Epoch: 97, step: 30, loss: 3.6475209959091677\n",
      "Epoch: 98, step: 0, loss: 3.7499725818634033\n",
      "Epoch: 98, step: 5, loss: 3.676373759905497\n",
      "Epoch: 98, step: 10, loss: 3.652560689232566\n",
      "Epoch: 98, step: 15, loss: 3.6520683467388153\n",
      "Epoch: 98, step: 20, loss: 3.6399150235312328\n",
      "Epoch: 98, step: 25, loss: 3.648482790360084\n",
      "Epoch: 98, step: 30, loss: 3.6479336369422173\n",
      "Epoch: 99, step: 0, loss: 3.5540308952331543\n",
      "Epoch: 99, step: 5, loss: 3.6535520950953164\n",
      "Epoch: 99, step: 10, loss: 3.6359977960586547\n",
      "Epoch: 99, step: 15, loss: 3.6336761951446532\n",
      "Epoch: 99, step: 20, loss: 3.636601150035858\n",
      "Epoch: 99, step: 25, loss: 3.6317460250854494\n",
      "Epoch: 99, step: 30, loss: 3.633878556887309\n",
      "Epoch: 100, step: 0, loss: 3.575953483581543\n",
      "Epoch: 100, step: 5, loss: 3.64921502272288\n",
      "Epoch: 100, step: 10, loss: 3.6484453678131104\n",
      "Epoch: 100, step: 15, loss: 3.643846720457077\n",
      "Epoch: 100, step: 20, loss: 3.6504488104865667\n",
      "Epoch: 100, step: 25, loss: 3.6504069841825046\n",
      "Epoch: 100, step: 30, loss: 3.6420287778300624\n",
      "Epoch: 101, step: 0, loss: 3.547175645828247\n",
      "Epoch: 101, step: 5, loss: 3.6160001357396445\n",
      "Epoch: 101, step: 10, loss: 3.6466251503337515\n",
      "Epoch: 101, step: 15, loss: 3.640422970056534\n",
      "Epoch: 101, step: 20, loss: 3.6400339489891413\n",
      "Epoch: 101, step: 25, loss: 3.6429381920741153\n",
      "Epoch: 101, step: 30, loss: 3.6387726030042096\n",
      "Epoch: 102, step: 0, loss: 3.6356139183044434\n",
      "Epoch: 102, step: 5, loss: 3.6534964640935264\n",
      "Epoch: 102, step: 10, loss: 3.648935404690829\n",
      "Epoch: 102, step: 15, loss: 3.646411567926407\n",
      "Epoch: 102, step: 20, loss: 3.641270013082595\n",
      "Epoch: 102, step: 25, loss: 3.638659807351919\n",
      "Epoch: 102, step: 30, loss: 3.6359332992184545\n",
      "Epoch: 103, step: 0, loss: 3.6293582916259766\n",
      "Epoch: 103, step: 5, loss: 3.681589365005493\n",
      "Epoch: 103, step: 10, loss: 3.666437127373435\n",
      "Epoch: 103, step: 15, loss: 3.6434744894504547\n",
      "Epoch: 103, step: 20, loss: 3.6402060872032527\n",
      "Epoch: 103, step: 25, loss: 3.6368566131591797\n",
      "Epoch: 103, step: 30, loss: 3.640344667434692\n",
      "Epoch: 104, step: 0, loss: 3.6411125659942627\n",
      "Epoch: 104, step: 5, loss: 3.6426521142323813\n",
      "Epoch: 104, step: 10, loss: 3.6498950611461294\n",
      "Epoch: 104, step: 15, loss: 3.6389722377061844\n",
      "Epoch: 104, step: 20, loss: 3.6416332608177546\n",
      "Epoch: 104, step: 25, loss: 3.64916861974276\n",
      "Epoch: 104, step: 30, loss: 3.635790886417512\n",
      "Epoch: 105, step: 0, loss: 3.6426968574523926\n",
      "Epoch: 105, step: 5, loss: 3.6308327118555703\n",
      "Epoch: 105, step: 10, loss: 3.649965221231634\n",
      "Epoch: 105, step: 15, loss: 3.632434755563736\n",
      "Epoch: 105, step: 20, loss: 3.639961662746611\n",
      "Epoch: 105, step: 25, loss: 3.6446070120884824\n",
      "Epoch: 105, step: 30, loss: 3.631679150366014\n",
      "Epoch: 106, step: 0, loss: 3.591238260269165\n",
      "Epoch: 106, step: 5, loss: 3.660317341486613\n",
      "Epoch: 106, step: 10, loss: 3.634647781198675\n",
      "Epoch: 106, step: 15, loss: 3.6394095569849014\n",
      "Epoch: 106, step: 20, loss: 3.624424923033941\n",
      "Epoch: 106, step: 25, loss: 3.626300124021677\n",
      "Epoch: 106, step: 30, loss: 3.6272389042762017\n",
      "Epoch: 107, step: 0, loss: 3.62719464302063\n",
      "Epoch: 107, step: 5, loss: 3.5351080894470215\n",
      "Epoch: 107, step: 10, loss: 3.5909932526675137\n",
      "Epoch: 107, step: 15, loss: 3.607762575149536\n",
      "Epoch: 107, step: 20, loss: 3.618077800387428\n",
      "Epoch: 107, step: 25, loss: 3.619095591398386\n",
      "Epoch: 107, step: 30, loss: 3.6288058219417447\n",
      "Epoch: 108, step: 0, loss: 3.595022201538086\n",
      "Epoch: 108, step: 5, loss: 3.593883752822876\n",
      "Epoch: 108, step: 10, loss: 3.5961174097928135\n",
      "Epoch: 108, step: 15, loss: 3.60655215382576\n",
      "Epoch: 108, step: 20, loss: 3.6132541611081077\n",
      "Epoch: 108, step: 25, loss: 3.6210072132257314\n",
      "Epoch: 108, step: 30, loss: 3.622540573919973\n",
      "Epoch: 109, step: 0, loss: 3.5647034645080566\n",
      "Epoch: 109, step: 5, loss: 3.625859101613363\n",
      "Epoch: 109, step: 10, loss: 3.640475273132324\n",
      "Epoch: 109, step: 15, loss: 3.6308834850788116\n",
      "Epoch: 109, step: 20, loss: 3.6213861987704323\n",
      "Epoch: 109, step: 25, loss: 3.6144792391703677\n",
      "Epoch: 109, step: 30, loss: 3.6222598475794636\n",
      "Epoch: 110, step: 0, loss: 3.5893921852111816\n",
      "Epoch: 110, step: 5, loss: 3.645549734433492\n",
      "Epoch: 110, step: 10, loss: 3.6406152681870894\n",
      "Epoch: 110, step: 15, loss: 3.623084396123886\n",
      "Epoch: 110, step: 20, loss: 3.6193147613888694\n",
      "Epoch: 110, step: 25, loss: 3.6261096459168654\n",
      "Epoch: 110, step: 30, loss: 3.624746730250697\n",
      "Epoch: 111, step: 0, loss: 3.4876861572265625\n",
      "Epoch: 111, step: 5, loss: 3.611216187477112\n",
      "Epoch: 111, step: 10, loss: 3.6136442964727227\n",
      "Epoch: 111, step: 15, loss: 3.6143314093351364\n",
      "Epoch: 111, step: 20, loss: 3.634252150853475\n",
      "Epoch: 111, step: 25, loss: 3.6144936084747314\n",
      "Epoch: 111, step: 30, loss: 3.6168319179165747\n",
      "Epoch: 112, step: 0, loss: 3.4634218215942383\n",
      "Epoch: 112, step: 5, loss: 3.594322601954142\n",
      "Epoch: 112, step: 10, loss: 3.6026981527155097\n",
      "Epoch: 112, step: 15, loss: 3.586412325501442\n",
      "Epoch: 112, step: 20, loss: 3.605861232394264\n",
      "Epoch: 112, step: 25, loss: 3.6040042547079234\n",
      "Epoch: 112, step: 30, loss: 3.613733237789523\n",
      "Epoch: 113, step: 0, loss: 3.710542678833008\n",
      "Epoch: 113, step: 5, loss: 3.5921784400939942\n",
      "Epoch: 113, step: 10, loss: 3.5809086084365847\n",
      "Epoch: 113, step: 15, loss: 3.6126082579294843\n",
      "Epoch: 113, step: 20, loss: 3.6175803899765016\n",
      "Epoch: 113, step: 25, loss: 3.614000692367554\n",
      "Epoch: 113, step: 30, loss: 3.6200488090515135\n",
      "Epoch: 114, step: 0, loss: 3.5210494995117188\n",
      "Epoch: 114, step: 5, loss: 3.5617347160975137\n",
      "Epoch: 114, step: 10, loss: 3.5936351255937056\n",
      "Epoch: 114, step: 15, loss: 3.6125524789094925\n",
      "Epoch: 114, step: 20, loss: 3.602163996015276\n",
      "Epoch: 114, step: 25, loss: 3.603794657267057\n",
      "Epoch: 114, step: 30, loss: 3.612033005683653\n",
      "Epoch: 115, step: 0, loss: 3.5186452865600586\n",
      "Epoch: 115, step: 5, loss: 3.582452376683553\n",
      "Epoch: 115, step: 10, loss: 3.5829345963217993\n",
      "Epoch: 115, step: 15, loss: 3.5827168226242065\n",
      "Epoch: 115, step: 20, loss: 3.6080522423698786\n",
      "Epoch: 115, step: 25, loss: 3.5996782779693604\n",
      "Epoch: 115, step: 30, loss: 3.609238616881832\n",
      "Epoch: 116, step: 0, loss: 3.624819278717041\n",
      "Epoch: 116, step: 5, loss: 3.616443951924642\n",
      "Epoch: 116, step: 10, loss: 3.60439996285872\n",
      "Epoch: 116, step: 15, loss: 3.595632940530777\n",
      "Epoch: 116, step: 20, loss: 3.608062676021031\n",
      "Epoch: 116, step: 25, loss: 3.6055310322688174\n",
      "Epoch: 116, step: 30, loss: 3.60389797149166\n",
      "Epoch: 117, step: 0, loss: 3.590846538543701\n",
      "Epoch: 117, step: 5, loss: 3.627671559651693\n",
      "Epoch: 117, step: 10, loss: 3.6300766468048096\n",
      "Epoch: 117, step: 15, loss: 3.6227206736803055\n",
      "Epoch: 117, step: 20, loss: 3.622607219786871\n",
      "Epoch: 117, step: 25, loss: 3.60904337809636\n",
      "Epoch: 117, step: 30, loss: 3.6049533582502797\n",
      "Epoch: 118, step: 0, loss: 3.544250011444092\n",
      "Epoch: 118, step: 5, loss: 3.5903445879618325\n",
      "Epoch: 118, step: 10, loss: 3.609425739808516\n",
      "Epoch: 118, step: 15, loss: 3.618910104036331\n",
      "Epoch: 118, step: 20, loss: 3.6080918652670726\n",
      "Epoch: 118, step: 25, loss: 3.6005072593688965\n",
      "Epoch: 118, step: 30, loss: 3.6048558065968175\n",
      "Epoch: 119, step: 0, loss: 3.489492893218994\n",
      "Epoch: 119, step: 5, loss: 3.640368858973185\n",
      "Epoch: 119, step: 10, loss: 3.6230850653214888\n",
      "Epoch: 119, step: 15, loss: 3.609032064676285\n",
      "Epoch: 119, step: 20, loss: 3.603563660667056\n",
      "Epoch: 119, step: 25, loss: 3.599400226886456\n",
      "Epoch: 119, step: 30, loss: 3.6018162465864614\n",
      "Epoch: 120, step: 0, loss: 3.5823066234588623\n",
      "Epoch: 120, step: 5, loss: 3.664177497227987\n",
      "Epoch: 120, step: 10, loss: 3.6296566182916816\n",
      "Epoch: 120, step: 15, loss: 3.61464262008667\n",
      "Epoch: 120, step: 20, loss: 3.614807344618298\n",
      "Epoch: 120, step: 25, loss: 3.6097886837445774\n",
      "Epoch: 120, step: 30, loss: 3.6052801378311647\n",
      "Epoch: 121, step: 0, loss: 3.563166618347168\n",
      "Epoch: 121, step: 5, loss: 3.6460830767949424\n",
      "Epoch: 121, step: 10, loss: 3.6238980943506416\n",
      "Epoch: 121, step: 15, loss: 3.613871768116951\n",
      "Epoch: 121, step: 20, loss: 3.6057510262443904\n",
      "Epoch: 121, step: 25, loss: 3.607196569442749\n",
      "Epoch: 121, step: 30, loss: 3.6057168822134695\n",
      "Epoch: 122, step: 0, loss: 3.5989761352539062\n",
      "Epoch: 122, step: 5, loss: 3.59146785736084\n",
      "Epoch: 122, step: 10, loss: 3.6051526069641113\n",
      "Epoch: 122, step: 15, loss: 3.6090081373850507\n",
      "Epoch: 122, step: 20, loss: 3.5991861462593078\n",
      "Epoch: 122, step: 25, loss: 3.6050233459472656\n",
      "Epoch: 122, step: 30, loss: 3.5975586970647178\n",
      "Epoch: 123, step: 0, loss: 3.6855762004852295\n",
      "Epoch: 123, step: 5, loss: 3.6185927788416543\n",
      "Epoch: 123, step: 10, loss: 3.614401405507868\n",
      "Epoch: 123, step: 15, loss: 3.602438911795616\n",
      "Epoch: 123, step: 20, loss: 3.5960463228679838\n",
      "Epoch: 123, step: 25, loss: 3.6040184681232157\n",
      "Epoch: 123, step: 30, loss: 3.597504908038724\n",
      "Epoch: 124, step: 0, loss: 3.664893627166748\n",
      "Epoch: 124, step: 5, loss: 3.5887734095255532\n",
      "Epoch: 124, step: 10, loss: 3.607143857262351\n",
      "Epoch: 124, step: 15, loss: 3.5888940542936325\n",
      "Epoch: 124, step: 20, loss: 3.5898501873016357\n",
      "Epoch: 124, step: 25, loss: 3.589911644275372\n",
      "Epoch: 124, step: 30, loss: 3.592782981934086\n",
      "Epoch: 125, step: 0, loss: 3.6504337787628174\n",
      "Epoch: 125, step: 5, loss: 3.5984081824620566\n",
      "Epoch: 125, step: 10, loss: 3.6099456657062876\n",
      "Epoch: 125, step: 15, loss: 3.606803148984909\n",
      "Epoch: 125, step: 20, loss: 3.6057102453140986\n",
      "Epoch: 125, step: 25, loss: 3.6018232840758104\n",
      "Epoch: 125, step: 30, loss: 3.5956782371767106\n",
      "Epoch: 126, step: 0, loss: 3.544811725616455\n",
      "Epoch: 126, step: 5, loss: 3.5913691918055215\n",
      "Epoch: 126, step: 10, loss: 3.5853084217418325\n",
      "Epoch: 126, step: 15, loss: 3.580747589468956\n",
      "Epoch: 126, step: 20, loss: 3.586716436204456\n",
      "Epoch: 126, step: 25, loss: 3.5977880404545712\n",
      "Epoch: 126, step: 30, loss: 3.5944541654279156\n",
      "Epoch: 127, step: 5, loss: 3.6103724002838136\n",
      "Epoch: 127, step: 10, loss: 3.6179073810577393\n",
      "Epoch: 127, step: 15, loss: 3.6067410628000895\n",
      "Epoch: 127, step: 20, loss: 3.6048632621765138\n",
      "Epoch: 127, step: 25, loss: 3.593916988372803\n",
      "Epoch: 128, step: 0, loss: 3.611400604248047\n",
      "Epoch: 128, step: 5, loss: 3.6153781016667685\n",
      "Epoch: 128, step: 10, loss: 3.6015122803774746\n",
      "Epoch: 128, step: 15, loss: 3.5986709594726562\n",
      "Epoch: 128, step: 20, loss: 3.588886033921015\n",
      "Epoch: 128, step: 25, loss: 3.5828669713093686\n",
      "Epoch: 128, step: 30, loss: 3.589354738112419\n",
      "Epoch: 129, step: 0, loss: 3.598268747329712\n",
      "Epoch: 129, step: 5, loss: 3.5684616565704346\n",
      "Epoch: 129, step: 10, loss: 3.575986450368708\n",
      "Epoch: 129, step: 15, loss: 3.570343554019928\n",
      "Epoch: 129, step: 20, loss: 3.583370333626157\n",
      "Epoch: 129, step: 25, loss: 3.579963885820829\n",
      "Epoch: 129, step: 30, loss: 3.583191979315973\n",
      "Epoch: 130, step: 0, loss: 3.648165702819824\n",
      "Epoch: 130, step: 5, loss: 3.6176697810490928\n",
      "Epoch: 130, step: 10, loss: 3.601623231714422\n",
      "Epoch: 130, step: 15, loss: 3.606089800596237\n",
      "Epoch: 130, step: 20, loss: 3.5919783796582903\n",
      "Epoch: 130, step: 25, loss: 3.5960727563271155\n",
      "Epoch: 130, step: 30, loss: 3.5830713548967914\n",
      "Epoch: 131, step: 0, loss: 3.4611687660217285\n",
      "Epoch: 131, step: 5, loss: 3.5845867395401\n",
      "Epoch: 131, step: 10, loss: 3.593928250399503\n",
      "Epoch: 131, step: 15, loss: 3.582965672016144\n",
      "Epoch: 131, step: 20, loss: 3.5869293326423284\n",
      "Epoch: 131, step: 25, loss: 3.58977735042572\n",
      "Epoch: 131, step: 30, loss: 3.584487315147154\n",
      "Epoch: 132, step: 0, loss: 3.4999217987060547\n",
      "Epoch: 132, step: 5, loss: 3.589591900507609\n",
      "Epoch: 132, step: 10, loss: 3.598372134295377\n",
      "Epoch: 132, step: 15, loss: 3.5835484713315964\n",
      "Epoch: 132, step: 20, loss: 3.5837177991867066\n",
      "Epoch: 132, step: 25, loss: 3.588788366317749\n",
      "Epoch: 132, step: 30, loss: 3.583656175931295\n",
      "Epoch: 133, step: 0, loss: 3.4882874488830566\n",
      "Epoch: 133, step: 5, loss: 3.6185067892074585\n",
      "Epoch: 133, step: 10, loss: 3.599667852575129\n",
      "Epoch: 133, step: 15, loss: 3.6004654467105865\n",
      "Epoch: 133, step: 20, loss: 3.5885059379395985\n",
      "Epoch: 133, step: 25, loss: 3.5791699244425845\n",
      "Epoch: 133, step: 30, loss: 3.5785477776681223\n",
      "Epoch: 134, step: 0, loss: 3.566009998321533\n",
      "Epoch: 134, step: 5, loss: 3.565429131189982\n",
      "Epoch: 134, step: 10, loss: 3.583170175552368\n",
      "Epoch: 134, step: 15, loss: 3.5764519423246384\n",
      "Epoch: 134, step: 20, loss: 3.5734251113164994\n",
      "Epoch: 134, step: 25, loss: 3.57593100804549\n",
      "Epoch: 134, step: 30, loss: 3.576822696193572\n",
      "Epoch: 135, step: 0, loss: 3.5614681243896484\n",
      "Epoch: 135, step: 5, loss: 3.5417426029841104\n",
      "Epoch: 135, step: 10, loss: 3.6045467420057817\n",
      "Epoch: 135, step: 15, loss: 3.591063618659973\n",
      "Epoch: 135, step: 20, loss: 3.5885657355898903\n",
      "Epoch: 135, step: 25, loss: 3.58263832789201\n",
      "Epoch: 135, step: 30, loss: 3.5795838679036787\n",
      "Epoch: 136, step: 0, loss: 3.6480278968811035\n",
      "Epoch: 136, step: 5, loss: 3.6411588986714682\n",
      "Epoch: 136, step: 15, loss: 3.56975892384847\n",
      "Epoch: 136, step: 20, loss: 3.573212540149689\n",
      "Epoch: 136, step: 25, loss: 3.5789493560791015\n",
      "Epoch: 136, step: 30, loss: 3.572882135709127\n",
      "Epoch: 137, step: 0, loss: 3.577610492706299\n",
      "Epoch: 137, step: 5, loss: 3.546909769376119\n",
      "Epoch: 137, step: 10, loss: 3.578395821831443\n",
      "Epoch: 137, step: 15, loss: 3.5815450698137283\n",
      "Epoch: 137, step: 20, loss: 3.5690423988160633\n",
      "Epoch: 137, step: 25, loss: 3.5710829404684215\n",
      "Epoch: 137, step: 30, loss: 3.5760432597129577\n",
      "Epoch: 138, step: 0, loss: 3.309817314147949\n",
      "Epoch: 138, step: 5, loss: 3.5295002460479736\n",
      "Epoch: 138, step: 10, loss: 3.5351721373471348\n",
      "Epoch: 138, step: 15, loss: 3.5521682649850845\n",
      "Epoch: 138, step: 20, loss: 3.5596536681765603\n",
      "Epoch: 138, step: 25, loss: 3.563340764779311\n",
      "Epoch: 138, step: 30, loss: 3.572733909853043\n",
      "Epoch: 139, step: 0, loss: 3.570427894592285\n",
      "Epoch: 139, step: 5, loss: 3.5737077395121255\n",
      "Epoch: 139, step: 10, loss: 3.5805592103437944\n",
      "Epoch: 139, step: 15, loss: 3.574714720249176\n",
      "Epoch: 139, step: 20, loss: 3.5678660301935103\n",
      "Epoch: 139, step: 25, loss: 3.567043478672321\n",
      "Epoch: 139, step: 30, loss: 3.574790785389562\n",
      "Epoch: 140, step: 0, loss: 3.3240771293640137\n",
      "Epoch: 140, step: 5, loss: 3.5306476751963296\n",
      "Epoch: 140, step: 10, loss: 3.562015923586759\n",
      "Epoch: 140, step: 15, loss: 3.5578349232673645\n",
      "Epoch: 140, step: 20, loss: 3.569137970606486\n",
      "Epoch: 140, step: 25, loss: 3.5665058814562283\n",
      "Epoch: 140, step: 30, loss: 3.5679815892250306\n",
      "Epoch: 141, step: 0, loss: 3.53322172164917\n",
      "Epoch: 141, step: 5, loss: 3.541457772254944\n",
      "Epoch: 141, step: 10, loss: 3.5365169915285977\n",
      "Epoch: 141, step: 15, loss: 3.5398168861865997\n",
      "Epoch: 141, step: 20, loss: 3.551236254828317\n",
      "Epoch: 141, step: 25, loss: 3.5618390028293314\n",
      "Epoch: 141, step: 30, loss: 3.569806014337847\n",
      "Epoch: 142, step: 0, loss: 3.509894609451294\n",
      "Epoch: 142, step: 5, loss: 3.5609565575917563\n",
      "Epoch: 142, step: 10, loss: 3.5444604700261895\n",
      "Epoch: 142, step: 15, loss: 3.5804813355207443\n",
      "Epoch: 142, step: 20, loss: 3.5762065592266263\n",
      "Epoch: 142, step: 25, loss: 3.5688112332270694\n",
      "Epoch: 142, step: 30, loss: 3.570937748878233\n",
      "Epoch: 143, step: 0, loss: 3.6429905891418457\n",
      "Epoch: 143, step: 5, loss: 3.5565251111984253\n",
      "Epoch: 143, step: 10, loss: 3.564438147978349\n",
      "Epoch: 143, step: 15, loss: 3.570466920733452\n",
      "Epoch: 143, step: 20, loss: 3.565702449707758\n",
      "Epoch: 143, step: 25, loss: 3.568813965870784\n",
      "Epoch: 143, step: 30, loss: 3.5664770141724618\n",
      "Epoch: 144, step: 0, loss: 3.607715606689453\n",
      "Epoch: 144, step: 5, loss: 3.6023995876312256\n",
      "Epoch: 144, step: 10, loss: 3.5651441270654853\n",
      "Epoch: 144, step: 15, loss: 3.561521753668785\n",
      "Epoch: 144, step: 20, loss: 3.548225028174264\n",
      "Epoch: 144, step: 25, loss: 3.561995873084435\n",
      "Epoch: 144, step: 30, loss: 3.568319897497854\n",
      "Epoch: 145, step: 0, loss: 3.589033603668213\n",
      "Epoch: 145, step: 5, loss: 3.624276598294576\n",
      "Epoch: 145, step: 10, loss: 3.572302753275091\n",
      "Epoch: 145, step: 15, loss: 3.5484637767076492\n",
      "Epoch: 145, step: 20, loss: 3.5563626970563615\n",
      "Epoch: 145, step: 25, loss: 3.5594386137448826\n",
      "Epoch: 145, step: 30, loss: 3.5652728926750923\n",
      "Epoch: 146, step: 0, loss: 3.6133177280426025\n",
      "Epoch: 146, step: 5, loss: 3.544134815533956\n",
      "Epoch: 146, step: 10, loss: 3.537264021960172\n",
      "Epoch: 146, step: 15, loss: 3.5338133424520493\n",
      "Epoch: 146, step: 20, loss: 3.551227467400687\n",
      "Epoch: 146, step: 25, loss: 3.5545974159240723\n",
      "Epoch: 146, step: 30, loss: 3.560300898551941\n",
      "Epoch: 147, step: 0, loss: 3.60414981842041\n",
      "Epoch: 147, step: 5, loss: 3.5355470180511475\n",
      "Epoch: 147, step: 10, loss: 3.5624749660491943\n",
      "Epoch: 147, step: 15, loss: 3.549084395170212\n",
      "Epoch: 147, step: 20, loss: 3.5459747314453125\n",
      "Epoch: 147, step: 25, loss: 3.567586614535405\n",
      "Epoch: 147, step: 30, loss: 3.5622877074826147\n",
      "Epoch: 148, step: 0, loss: 3.596721887588501\n",
      "Epoch: 148, step: 5, loss: 3.5894585053126016\n",
      "Epoch: 148, step: 10, loss: 3.5572009736841377\n",
      "Epoch: 148, step: 15, loss: 3.5613579005002975\n",
      "Epoch: 148, step: 20, loss: 3.570412136259533\n",
      "Epoch: 148, step: 25, loss: 3.569450323398297\n",
      "Epoch: 148, step: 30, loss: 3.5662595610464773\n",
      "Epoch: 149, step: 0, loss: 3.451392650604248\n",
      "Epoch: 149, step: 5, loss: 3.5373849471410117\n",
      "Epoch: 149, step: 10, loss: 3.5489955598657783\n",
      "Epoch: 149, step: 15, loss: 3.5476378202438354\n",
      "Epoch: 149, step: 20, loss: 3.5443507376171293\n",
      "Epoch: 149, step: 25, loss: 3.54812897168673\n",
      "Epoch: 149, step: 30, loss: 3.5622363398152013\n",
      "Epoch: 150, step: 0, loss: 3.516418933868408\n",
      "Epoch: 150, step: 5, loss: 3.5024821360905967\n",
      "Epoch: 150, step: 10, loss: 3.527725263075395\n",
      "Epoch: 150, step: 15, loss: 3.553659051656723\n",
      "Epoch: 150, step: 20, loss: 3.583645570845831\n",
      "Epoch: 150, step: 25, loss: 3.5752158348376932\n",
      "Epoch: 150, step: 30, loss: 3.561860376788724\n",
      "Epoch: 151, step: 0, loss: 3.6251301765441895\n",
      "Epoch: 151, step: 5, loss: 3.606854796409607\n",
      "Epoch: 151, step: 10, loss: 3.583666107871316\n",
      "Epoch: 151, step: 15, loss: 3.55579437315464\n",
      "Epoch: 151, step: 20, loss: 3.5514462675367082\n",
      "Epoch: 151, step: 25, loss: 3.5557661342620848\n",
      "Epoch: 151, step: 30, loss: 3.5580711285273234\n",
      "Epoch: 152, step: 0, loss: 3.6587281227111816\n",
      "Epoch: 152, step: 5, loss: 3.5853700637817383\n",
      "Epoch: 152, step: 10, loss: 3.564783204685558\n",
      "Epoch: 152, step: 15, loss: 3.564592957496643\n",
      "Epoch: 152, step: 20, loss: 3.5789826143355596\n",
      "Epoch: 152, step: 25, loss: 3.5591525114499607\n",
      "Epoch: 152, step: 30, loss: 3.558945578913535\n",
      "Epoch: 153, step: 0, loss: 3.5317487716674805\n",
      "Epoch: 153, step: 5, loss: 3.534780224164327\n",
      "Epoch: 153, step: 10, loss: 3.5433977300470527\n",
      "Epoch: 153, step: 15, loss: 3.5617037415504456\n",
      "Epoch: 153, step: 20, loss: 3.5609113375345864\n",
      "Epoch: 153, step: 25, loss: 3.551341781249413\n",
      "Epoch: 153, step: 30, loss: 3.5572570139361965\n",
      "Epoch: 154, step: 0, loss: 3.36179780960083\n",
      "Epoch: 154, step: 5, loss: 3.5212153593699136\n",
      "Epoch: 154, step: 10, loss: 3.537332209673795\n",
      "Epoch: 154, step: 15, loss: 3.5379423648118973\n",
      "Epoch: 154, step: 20, loss: 3.546195007505871\n",
      "Epoch: 154, step: 25, loss: 3.54367182804988\n",
      "Epoch: 154, step: 30, loss: 3.5526720477688696\n",
      "Epoch: 155, step: 0, loss: 3.398101329803467\n",
      "Epoch: 155, step: 5, loss: 3.547330896059672\n",
      "Epoch: 155, step: 10, loss: 3.5388514562086626\n",
      "Epoch: 155, step: 15, loss: 3.5433848798274994\n",
      "Epoch: 155, step: 20, loss: 3.5372904595874606\n",
      "Epoch: 155, step: 25, loss: 3.547047440822308\n",
      "Epoch: 155, step: 30, loss: 3.5487119920792116\n",
      "Epoch: 156, step: 0, loss: 3.602532386779785\n",
      "Epoch: 156, step: 5, loss: 3.5579150120417276\n",
      "Epoch: 156, step: 10, loss: 3.559654387560758\n",
      "Epoch: 156, step: 15, loss: 3.5516139417886734\n",
      "Epoch: 156, step: 20, loss: 3.5539268879663375\n",
      "Epoch: 156, step: 25, loss: 3.548668769689707\n",
      "Epoch: 156, step: 30, loss: 3.5476674341386363\n",
      "Epoch: 157, step: 0, loss: 3.47996187210083\n",
      "Epoch: 157, step: 5, loss: 3.5569310585657754\n",
      "Epoch: 157, step: 10, loss: 3.5509018898010254\n",
      "Epoch: 157, step: 15, loss: 3.5460802167654037\n",
      "Epoch: 157, step: 20, loss: 3.5527896767570857\n",
      "Epoch: 157, step: 25, loss: 3.559027616794293\n",
      "Epoch: 157, step: 30, loss: 3.5533453341453307\n",
      "Epoch: 158, step: 0, loss: 3.601901054382324\n",
      "Epoch: 158, step: 5, loss: 3.579730828603109\n",
      "Epoch: 158, step: 10, loss: 3.5889437198638916\n",
      "Epoch: 158, step: 15, loss: 3.5794637501239777\n",
      "Epoch: 158, step: 20, loss: 3.5675257728213357\n",
      "Epoch: 158, step: 25, loss: 3.5566238715098453\n",
      "Epoch: 159, step: 0, loss: 3.4512391090393066\n",
      "Epoch: 159, step: 5, loss: 3.5607897440592446\n",
      "Epoch: 159, step: 10, loss: 3.57812255079096\n",
      "Epoch: 159, step: 15, loss: 3.5565115958452225\n",
      "Epoch: 159, step: 20, loss: 3.5601622944786433\n",
      "Epoch: 159, step: 25, loss: 3.557762485284072\n",
      "Epoch: 159, step: 30, loss: 3.552160455334571\n",
      "Epoch: 160, step: 0, loss: 3.4751641750335693\n",
      "Epoch: 160, step: 5, loss: 3.5401471058527627\n",
      "Epoch: 160, step: 10, loss: 3.5458283857865767\n",
      "Epoch: 160, step: 15, loss: 3.5437790155410767\n",
      "Epoch: 160, step: 20, loss: 3.5393503166380382\n",
      "Epoch: 160, step: 25, loss: 3.5479789055310764\n",
      "Epoch: 160, step: 30, loss: 3.5482333090997513\n",
      "Epoch: 161, step: 0, loss: 3.4618148803710938\n",
      "Epoch: 161, step: 5, loss: 3.5537909269332886\n",
      "Epoch: 161, step: 10, loss: 3.5278492624109443\n",
      "Epoch: 161, step: 15, loss: 3.528258830308914\n",
      "Epoch: 161, step: 20, loss: 3.530681394395374\n",
      "Epoch: 161, step: 25, loss: 3.5445915643985453\n",
      "Epoch: 161, step: 30, loss: 3.541963807741801\n",
      "Epoch: 162, step: 0, loss: 3.453343391418457\n",
      "Epoch: 162, step: 5, loss: 3.4544942378997803\n",
      "Epoch: 162, step: 10, loss: 3.5135214762254194\n",
      "Epoch: 162, step: 15, loss: 3.541943296790123\n",
      "Epoch: 162, step: 20, loss: 3.539069959095546\n",
      "Epoch: 162, step: 25, loss: 3.53340888940371\n",
      "Epoch: 162, step: 30, loss: 3.5466678373275267\n",
      "Epoch: 163, step: 0, loss: 3.528012275695801\n",
      "Epoch: 163, step: 5, loss: 3.4827494621276855\n",
      "Epoch: 163, step: 10, loss: 3.5045458316802978\n",
      "Epoch: 163, step: 15, loss: 3.524967972437541\n",
      "Epoch: 163, step: 20, loss: 3.52726491689682\n",
      "Epoch: 163, step: 30, loss: 3.545548521239182\n",
      "Epoch: 164, step: 0, loss: 3.5748348236083984\n",
      "Epoch: 164, step: 5, loss: 3.5329980850219727\n",
      "Epoch: 164, step: 10, loss: 3.5376766594973477\n",
      "Epoch: 164, step: 15, loss: 3.528225749731064\n",
      "Epoch: 164, step: 20, loss: 3.534606729234968\n",
      "Epoch: 164, step: 25, loss: 3.5470859362528873\n",
      "Epoch: 164, step: 30, loss: 3.543143972273796\n",
      "Epoch: 165, step: 0, loss: 3.5663063526153564\n",
      "Epoch: 165, step: 5, loss: 3.5049533446629844\n",
      "Epoch: 165, step: 10, loss: 3.5134443803267046\n",
      "Epoch: 165, step: 15, loss: 3.5196518152952194\n",
      "Epoch: 165, step: 20, loss: 3.528620731262934\n",
      "Epoch: 165, step: 25, loss: 3.532832613358131\n",
      "Epoch: 165, step: 30, loss: 3.5415656643529094\n",
      "Epoch: 166, step: 0, loss: 3.270739793777466\n",
      "Epoch: 166, step: 5, loss: 3.4424842596054077\n",
      "Epoch: 166, step: 10, loss: 3.478566191413186\n",
      "Epoch: 166, step: 15, loss: 3.5116320699453354\n",
      "Epoch: 166, step: 20, loss: 3.5249706222897483\n",
      "Epoch: 166, step: 25, loss: 3.524730453124413\n",
      "Epoch: 166, step: 30, loss: 3.5392246092519453\n",
      "Epoch: 167, step: 0, loss: 3.6743927001953125\n",
      "Epoch: 167, step: 5, loss: 3.5711437861124673\n",
      "Epoch: 167, step: 10, loss: 3.53578764742071\n",
      "Epoch: 167, step: 15, loss: 3.5298226177692413\n",
      "Epoch: 167, step: 20, loss: 3.5461679867335727\n",
      "Epoch: 167, step: 25, loss: 3.536048549872178\n",
      "Epoch: 167, step: 30, loss: 3.546954539514357\n",
      "Epoch: 168, step: 0, loss: 3.4384305477142334\n",
      "Epoch: 168, step: 5, loss: 3.477371414502462\n",
      "Epoch: 168, step: 10, loss: 3.533082962036133\n",
      "Epoch: 168, step: 15, loss: 3.5238791406154633\n",
      "Epoch: 168, step: 20, loss: 3.529912312825521\n",
      "Epoch: 168, step: 25, loss: 3.5375780692467322\n",
      "Epoch: 168, step: 30, loss: 3.5376332267638175\n",
      "Epoch: 169, step: 0, loss: 3.561614990234375\n",
      "Epoch: 169, step: 5, loss: 3.534385085105896\n",
      "Epoch: 169, step: 10, loss: 3.5198458108035\n",
      "Epoch: 169, step: 15, loss: 3.526290848851204\n",
      "Epoch: 169, step: 20, loss: 3.527912730262393\n",
      "Epoch: 169, step: 25, loss: 3.5332253987972555\n",
      "Epoch: 169, step: 30, loss: 3.5359381398847027\n",
      "Epoch: 170, step: 0, loss: 3.518468141555786\n",
      "Epoch: 170, step: 5, loss: 3.5263986587524414\n",
      "Epoch: 170, step: 10, loss: 3.5281997593966397\n",
      "Epoch: 170, step: 15, loss: 3.5237555354833603\n",
      "Epoch: 170, step: 20, loss: 3.52631646110898\n",
      "Epoch: 170, step: 25, loss: 3.5320344062951894\n",
      "Epoch: 170, step: 30, loss: 3.536488486874488\n",
      "Epoch: 171, step: 0, loss: 3.579864501953125\n",
      "Epoch: 171, step: 5, loss: 3.4887125492095947\n",
      "Epoch: 171, step: 10, loss: 3.5041212385351006\n",
      "Epoch: 171, step: 15, loss: 3.536531239748001\n",
      "Epoch: 171, step: 20, loss: 3.537524177914574\n",
      "Epoch: 171, step: 25, loss: 3.533759016257066\n",
      "Epoch: 171, step: 30, loss: 3.5346024574772006\n",
      "Epoch: 172, step: 0, loss: 3.5887184143066406\n",
      "Epoch: 172, step: 5, loss: 3.5630796353022256\n",
      "Epoch: 172, step: 10, loss: 3.536146965893832\n",
      "Epoch: 172, step: 15, loss: 3.5319623947143555\n",
      "Epoch: 172, step: 20, loss: 3.531470400946481\n",
      "Epoch: 172, step: 25, loss: 3.5271636064235983\n",
      "Epoch: 172, step: 30, loss: 3.5316894592777377\n",
      "Epoch: 173, step: 0, loss: 3.564936637878418\n",
      "Epoch: 173, step: 5, loss: 3.576582153638204\n",
      "Epoch: 173, step: 10, loss: 3.567385955290361\n",
      "Epoch: 173, step: 15, loss: 3.5483617037534714\n",
      "Epoch: 173, step: 20, loss: 3.543898332686651\n",
      "Epoch: 173, step: 25, loss: 3.5325950109041653\n",
      "Epoch: 173, step: 30, loss: 3.532672466770295\n",
      "Epoch: 174, step: 0, loss: 3.565471649169922\n",
      "Epoch: 174, step: 5, loss: 3.554793397585551\n",
      "Epoch: 174, step: 10, loss: 3.5355713800950483\n",
      "Epoch: 174, step: 15, loss: 3.547430455684662\n",
      "Epoch: 174, step: 20, loss: 3.54273551986331\n",
      "Epoch: 174, step: 25, loss: 3.5273404671595645\n",
      "Epoch: 174, step: 30, loss: 3.5311352898997646\n",
      "Epoch: 175, step: 0, loss: 3.570383071899414\n",
      "Epoch: 175, step: 5, loss: 3.54229728380839\n",
      "Epoch: 175, step: 10, loss: 3.5298008051785557\n",
      "Epoch: 175, step: 15, loss: 3.5296702533960342\n",
      "Epoch: 175, step: 20, loss: 3.529424201874506\n",
      "Epoch: 175, step: 25, loss: 3.530509563592764\n",
      "Epoch: 175, step: 30, loss: 3.535917544364929\n",
      "Epoch: 176, step: 0, loss: 3.578205108642578\n",
      "Epoch: 176, step: 5, loss: 3.5700341860453286\n",
      "Epoch: 176, step: 10, loss: 3.5401232025840064\n",
      "Epoch: 176, step: 15, loss: 3.5410559326410294\n",
      "Epoch: 176, step: 20, loss: 3.5392725467681885\n",
      "Epoch: 176, step: 25, loss: 3.540257903245779\n",
      "Epoch: 176, step: 30, loss: 3.534023815585721\n",
      "Epoch: 177, step: 0, loss: 3.39139723777771\n",
      "Epoch: 177, step: 5, loss: 3.540934681892395\n",
      "Epoch: 177, step: 15, loss: 3.540929094950358\n",
      "Epoch: 177, step: 20, loss: 3.546879231929779\n",
      "Epoch: 177, step: 25, loss: 3.524690933227539\n",
      "Epoch: 177, step: 30, loss: 3.5332868496576944\n",
      "Epoch: 178, step: 0, loss: 3.4411396980285645\n",
      "Epoch: 178, step: 5, loss: 3.4605690638224282\n",
      "Epoch: 178, step: 10, loss: 3.527813499624079\n",
      "Epoch: 178, step: 15, loss: 3.538898289203644\n",
      "Epoch: 178, step: 20, loss: 3.5325317609877813\n",
      "Epoch: 178, step: 25, loss: 3.5381773801950307\n",
      "Epoch: 178, step: 30, loss: 3.53477303981781\n",
      "Epoch: 179, step: 0, loss: 3.469421863555908\n",
      "Epoch: 179, step: 5, loss: 3.51151712735494\n",
      "Epoch: 179, step: 10, loss: 3.504255273125388\n",
      "Epoch: 179, step: 15, loss: 3.497047245502472\n",
      "Epoch: 179, step: 20, loss: 3.5197657403491793\n",
      "Epoch: 179, step: 25, loss: 3.5197914288594174\n",
      "Epoch: 179, step: 30, loss: 3.528302954089257\n",
      "Epoch: 180, step: 0, loss: 3.465402603149414\n",
      "Epoch: 180, step: 5, loss: 3.5117194652557373\n",
      "Epoch: 180, step: 10, loss: 3.5081147714094683\n",
      "Epoch: 180, step: 15, loss: 3.5097195208072662\n",
      "Epoch: 180, step: 20, loss: 3.5239205473945256\n",
      "Epoch: 180, step: 25, loss: 3.5261146013553324\n",
      "Epoch: 180, step: 30, loss: 3.528816061635171\n",
      "Epoch: 181, step: 0, loss: 3.522737503051758\n",
      "Epoch: 181, step: 5, loss: 3.554530302683512\n",
      "Epoch: 181, step: 10, loss: 3.5345462452281606\n",
      "Epoch: 181, step: 15, loss: 3.5269616097211838\n",
      "Epoch: 181, step: 20, loss: 3.5336813586098805\n",
      "Epoch: 181, step: 25, loss: 3.5221495720056386\n",
      "Epoch: 181, step: 30, loss: 3.527309202378796\n",
      "Epoch: 182, step: 0, loss: 3.551840305328369\n",
      "Epoch: 182, step: 5, loss: 3.5306135018666587\n",
      "Epoch: 182, step: 10, loss: 3.5293012098832564\n",
      "Epoch: 182, step: 15, loss: 3.533261761069298\n",
      "Epoch: 182, step: 20, loss: 3.53936082976205\n",
      "Epoch: 182, step: 25, loss: 3.5277309876221876\n",
      "Epoch: 182, step: 30, loss: 3.5269408149103962\n",
      "Epoch: 183, step: 0, loss: 3.418238639831543\n",
      "Epoch: 183, step: 5, loss: 3.5046345392862954\n",
      "Epoch: 183, step: 10, loss: 3.5141958106647837\n",
      "Epoch: 183, step: 15, loss: 3.5162978023290634\n",
      "Epoch: 183, step: 20, loss: 3.5143067155565535\n",
      "Epoch: 183, step: 25, loss: 3.5211703869012685\n",
      "Epoch: 183, step: 30, loss: 3.5217808523485736\n",
      "Epoch: 184, step: 0, loss: 3.4599785804748535\n",
      "Epoch: 184, step: 5, loss: 3.49780281384786\n",
      "Epoch: 184, step: 10, loss: 3.4996645884080366\n",
      "Epoch: 184, step: 15, loss: 3.5084567815065384\n",
      "Epoch: 184, step: 20, loss: 3.5295921053205217\n",
      "Epoch: 184, step: 25, loss: 3.527555777476384\n",
      "Epoch: 184, step: 30, loss: 3.528751596327751\n",
      "Epoch: 185, step: 0, loss: 3.5521390438079834\n",
      "Epoch: 185, step: 5, loss: 3.5521673361460366\n",
      "Epoch: 185, step: 10, loss: 3.5320359360088003\n",
      "Epoch: 185, step: 15, loss: 3.531860053539276\n",
      "Epoch: 185, step: 20, loss: 3.5186932314009893\n",
      "Epoch: 185, step: 25, loss: 3.5283333154825063\n",
      "Epoch: 185, step: 30, loss: 3.5267554944561375\n",
      "Epoch: 186, step: 0, loss: 3.4549736976623535\n",
      "Epoch: 186, step: 5, loss: 3.554228146870931\n",
      "Epoch: 186, step: 10, loss: 3.5301027514717798\n",
      "Epoch: 186, step: 15, loss: 3.530071586370468\n",
      "Epoch: 186, step: 20, loss: 3.512194894608997\n",
      "Epoch: 186, step: 25, loss: 3.513315796852112\n",
      "Epoch: 186, step: 30, loss: 3.5230056085894184\n",
      "Epoch: 187, step: 0, loss: 3.554488182067871\n",
      "Epoch: 187, step: 5, loss: 3.5472156604131064\n",
      "Epoch: 187, step: 10, loss: 3.53712983564897\n",
      "Epoch: 187, step: 15, loss: 3.537388324737549\n",
      "Epoch: 187, step: 20, loss: 3.531145629428682\n",
      "Epoch: 187, step: 25, loss: 3.5270530719023485\n",
      "Epoch: 187, step: 30, loss: 3.5265482856381323\n",
      "Epoch: 188, step: 0, loss: 3.5267751216888428\n",
      "Epoch: 188, step: 5, loss: 3.524673422177633\n",
      "Epoch: 188, step: 10, loss: 3.526369419964877\n",
      "Epoch: 188, step: 15, loss: 3.534451887011528\n",
      "Epoch: 188, step: 20, loss: 3.5292461599622453\n",
      "Epoch: 188, step: 25, loss: 3.5237297736681423\n",
      "Epoch: 188, step: 30, loss: 3.517487064484627\n",
      "Epoch: 189, step: 0, loss: 3.5405173301696777\n",
      "Epoch: 189, step: 5, loss: 3.532569726308187\n",
      "Epoch: 189, step: 10, loss: 3.530113935470581\n",
      "Epoch: 189, step: 15, loss: 3.514309599995613\n",
      "Epoch: 189, step: 20, loss: 3.5220370406196233\n",
      "Epoch: 189, step: 25, loss: 3.523530198977544\n",
      "Epoch: 189, step: 30, loss: 3.524964770963115\n",
      "Epoch: 190, step: 0, loss: 3.593667984008789\n",
      "Epoch: 190, step: 5, loss: 3.550885796546936\n",
      "Epoch: 190, step: 10, loss: 3.56002740426497\n",
      "Epoch: 190, step: 15, loss: 3.5380436331033707\n",
      "Epoch: 190, step: 20, loss: 3.521939958844866\n",
      "Epoch: 190, step: 25, loss: 3.519332161316505\n",
      "Epoch: 190, step: 30, loss: 3.5183511241789787\n",
      "Epoch: 191, step: 0, loss: 3.4723243713378906\n",
      "Epoch: 191, step: 5, loss: 3.526102622350057\n",
      "Epoch: 191, step: 10, loss: 3.497609008442272\n",
      "Epoch: 191, step: 15, loss: 3.499836578965187\n",
      "Epoch: 191, step: 20, loss: 3.509433376789093\n",
      "Epoch: 191, step: 25, loss: 3.5161768054962157\n",
      "Epoch: 191, step: 30, loss: 3.517555340131124\n",
      "Epoch: 192, step: 0, loss: 3.525516986846924\n",
      "Epoch: 192, step: 5, loss: 3.5570606787999473\n",
      "Epoch: 192, step: 10, loss: 3.527213287353516\n",
      "Epoch: 192, step: 15, loss: 3.5210286299387614\n",
      "Epoch: 192, step: 20, loss: 3.5277499675750734\n",
      "Epoch: 192, step: 25, loss: 3.5204020500183106\n",
      "Epoch: 192, step: 30, loss: 3.5117836952209474\n",
      "Epoch: 193, step: 0, loss: 3.525564193725586\n",
      "Epoch: 193, step: 5, loss: 3.5019477208455405\n",
      "Epoch: 193, step: 10, loss: 3.5121532678604126\n",
      "Epoch: 193, step: 15, loss: 3.5271147727966308\n",
      "Epoch: 193, step: 20, loss: 3.521286427974701\n",
      "Epoch: 193, step: 25, loss: 3.5188840007781983\n",
      "Epoch: 193, step: 30, loss: 3.5179043134053547\n",
      "Epoch: 194, step: 0, loss: 3.5821874141693115\n",
      "Epoch: 194, step: 5, loss: 3.532283584276835\n",
      "Epoch: 194, step: 10, loss: 3.540075800635598\n",
      "Epoch: 194, step: 15, loss: 3.522848978638649\n",
      "Epoch: 194, step: 20, loss: 3.5139976796649752\n",
      "Epoch: 194, step: 25, loss: 3.5176231219218326\n",
      "Epoch: 194, step: 30, loss: 3.516154966046733\n",
      "Epoch: 195, step: 0, loss: 3.5584287643432617\n",
      "Epoch: 195, step: 5, loss: 3.549724737803141\n",
      "Epoch: 195, step: 10, loss: 3.5194061669436367\n",
      "Epoch: 195, step: 15, loss: 3.519265964627266\n",
      "Epoch: 195, step: 20, loss: 3.516171546209426\n",
      "Epoch: 195, step: 25, loss: 3.5072539953085093\n",
      "Epoch: 195, step: 30, loss: 3.5140678574962\n",
      "Epoch: 196, step: 0, loss: 3.472219944000244\n",
      "Epoch: 196, step: 5, loss: 3.512208183606466\n",
      "Epoch: 196, step: 10, loss: 3.5354410951787774\n",
      "Epoch: 196, step: 15, loss: 3.5245892703533173\n",
      "Epoch: 196, step: 20, loss: 3.5230958348228816\n",
      "Epoch: 196, step: 25, loss: 3.5210057497024536\n",
      "Epoch: 196, step: 30, loss: 3.515270963791878\n",
      "Epoch: 197, step: 0, loss: 3.5925514698028564\n",
      "Epoch: 197, step: 5, loss: 3.541526436805725\n",
      "Epoch: 197, step: 10, loss: 3.551930557597767\n",
      "Epoch: 197, step: 15, loss: 3.5446458011865616\n",
      "Epoch: 197, step: 20, loss: 3.535848299662272\n",
      "Epoch: 197, step: 25, loss: 3.5265848361528835\n",
      "Epoch: 197, step: 30, loss: 3.5148457250287457\n",
      "Epoch: 198, step: 0, loss: 3.5618579387664795\n",
      "Epoch: 198, step: 5, loss: 3.504759669303894\n",
      "Epoch: 198, step: 10, loss: 3.498559431596236\n",
      "Epoch: 198, step: 15, loss: 3.513932481408119\n",
      "Epoch: 198, step: 20, loss: 3.5215414819263278\n",
      "Epoch: 198, step: 25, loss: 3.517668549831097\n",
      "Epoch: 198, step: 30, loss: 3.5109102110708914\n",
      "Epoch: 199, step: 0, loss: 3.469161033630371\n",
      "Epoch: 199, step: 5, loss: 3.5480886697769165\n",
      "Epoch: 199, step: 10, loss: 3.5192787647247314\n",
      "Epoch: 199, step: 15, loss: 3.523522302508354\n",
      "Epoch: 199, step: 20, loss: 3.513397534688314\n",
      "Epoch: 199, step: 25, loss: 3.5118597984313964\n",
      "Epoch: 199, step: 30, loss: 3.5121551752090454\n",
      "Epoch: 200, step: 0, loss: 3.500565528869629\n",
      "Epoch: 200, step: 5, loss: 3.4556565284729004\n",
      "Epoch: 200, step: 10, loss: 3.485661571676081\n",
      "Epoch: 200, step: 15, loss: 3.4938096702098846\n",
      "Epoch: 200, step: 20, loss: 3.504687854221889\n",
      "Epoch: 200, step: 25, loss: 3.5077008925951443\n",
      "Epoch: 200, step: 30, loss: 3.507034355594266\n",
      "Epoch: 201, step: 0, loss: 3.4148108959198\n",
      "Epoch: 201, step: 5, loss: 3.4920096000035605\n",
      "Epoch: 201, step: 10, loss: 3.5084410797465932\n",
      "Epoch: 201, step: 15, loss: 3.491251453757286\n",
      "Epoch: 201, step: 20, loss: 3.5011554786137173\n",
      "Epoch: 201, step: 25, loss: 3.501138687133789\n",
      "Epoch: 201, step: 30, loss: 3.507052498479043\n",
      "Epoch: 202, step: 0, loss: 3.6616055965423584\n",
      "Epoch: 202, step: 5, loss: 3.5511677265167236\n",
      "Epoch: 202, step: 10, loss: 3.5222777886824175\n",
      "Epoch: 202, step: 15, loss: 3.5069961100816727\n",
      "Epoch: 202, step: 20, loss: 3.5041065897260393\n",
      "Epoch: 202, step: 25, loss: 3.5043510657090406\n",
      "Epoch: 202, step: 30, loss: 3.5106661396641887\n",
      "Epoch: 203, step: 0, loss: 3.4380950927734375\n",
      "Epoch: 203, step: 5, loss: 3.51747989654541\n",
      "Epoch: 203, step: 10, loss: 3.5356162894855845\n",
      "Epoch: 203, step: 20, loss: 3.5282244086265564\n",
      "Epoch: 203, step: 25, loss: 3.517354202270508\n",
      "Epoch: 203, step: 30, loss: 3.5032798449198403\n",
      "Epoch: 204, step: 0, loss: 3.3898746967315674\n",
      "Epoch: 204, step: 5, loss: 3.468133727709452\n",
      "Epoch: 204, step: 10, loss: 3.4901092919436367\n",
      "Epoch: 204, step: 15, loss: 3.5030193626880646\n",
      "Epoch: 204, step: 20, loss: 3.504161312466576\n",
      "Epoch: 204, step: 25, loss: 3.511708011993995\n",
      "Epoch: 204, step: 30, loss: 3.5079688410605154\n",
      "Epoch: 205, step: 0, loss: 3.3447017669677734\n",
      "Epoch: 205, step: 5, loss: 3.50142240524292\n",
      "Epoch: 205, step: 10, loss: 3.489792238582264\n",
      "Epoch: 205, step: 15, loss: 3.5033591389656067\n",
      "Epoch: 205, step: 20, loss: 3.5029506456284296\n",
      "Epoch: 205, step: 25, loss: 3.4996006855597863\n",
      "Epoch: 205, step: 30, loss: 3.5045119254819808\n",
      "Epoch: 206, step: 0, loss: 3.374361991882324\n",
      "Epoch: 206, step: 5, loss: 3.431232293446859\n",
      "Epoch: 206, step: 10, loss: 3.484869523481889\n",
      "Epoch: 206, step: 15, loss: 3.485952392220497\n",
      "Epoch: 206, step: 20, loss: 3.5060456480298723\n",
      "Epoch: 206, step: 25, loss: 3.5051082372665405\n",
      "Epoch: 206, step: 30, loss: 3.5072091933219665\n",
      "Epoch: 207, step: 0, loss: 3.6264266967773438\n",
      "Epoch: 207, step: 5, loss: 3.5191126267115274\n",
      "Epoch: 207, step: 10, loss: 3.5357577150518242\n",
      "Epoch: 207, step: 15, loss: 3.531345248222351\n",
      "Epoch: 207, step: 20, loss: 3.5400583176385787\n",
      "Epoch: 207, step: 25, loss: 3.5084135073881884\n",
      "Epoch: 207, step: 30, loss: 3.507335347513999\n",
      "Epoch: 208, step: 0, loss: 3.5284018516540527\n",
      "Epoch: 208, step: 5, loss: 3.501809557278951\n",
      "Epoch: 208, step: 10, loss: 3.479764526540583\n",
      "Epoch: 208, step: 15, loss: 3.503301218152046\n",
      "Epoch: 208, step: 20, loss: 3.4883166948954263\n",
      "Epoch: 208, step: 25, loss: 3.4958496002050548\n",
      "Epoch: 208, step: 30, loss: 3.5013404430881625\n",
      "Epoch: 209, step: 0, loss: 3.5603699684143066\n",
      "Epoch: 209, step: 5, loss: 3.536163846651713\n",
      "Epoch: 209, step: 10, loss: 3.487076759338379\n",
      "Epoch: 209, step: 15, loss: 3.4873712062835693\n",
      "Epoch: 209, step: 20, loss: 3.5087527888161794\n",
      "Epoch: 209, step: 25, loss: 3.5022537524883566\n",
      "Epoch: 209, step: 30, loss: 3.5033793295583417\n",
      "Epoch: 210, step: 0, loss: 3.5262246131896973\n",
      "Epoch: 210, step: 5, loss: 3.5213162899017334\n",
      "Epoch: 210, step: 10, loss: 3.5250664234161375\n",
      "Epoch: 210, step: 15, loss: 3.519989093144735\n",
      "Epoch: 210, step: 20, loss: 3.5076363325119018\n",
      "Epoch: 210, step: 25, loss: 3.500908794403076\n",
      "Epoch: 210, step: 30, loss: 3.5058510382970174\n",
      "Epoch: 211, step: 0, loss: 3.446544647216797\n",
      "Epoch: 211, step: 5, loss: 3.5185221433639526\n",
      "Epoch: 211, step: 10, loss: 3.516327836296775\n",
      "Epoch: 211, step: 15, loss: 3.5130290538072586\n",
      "Epoch: 211, step: 20, loss: 3.5032297997247603\n",
      "Epoch: 211, step: 25, loss: 3.498722168115469\n",
      "Epoch: 211, step: 30, loss: 3.5028382808931413\n",
      "Epoch: 212, step: 0, loss: 3.4681739807128906\n",
      "Epoch: 212, step: 5, loss: 3.538110852241516\n",
      "Epoch: 212, step: 10, loss: 3.5440599484877153\n",
      "Epoch: 212, step: 15, loss: 3.5254115611314774\n",
      "Epoch: 212, step: 20, loss: 3.5147231419881186\n",
      "Epoch: 212, step: 25, loss: 3.5056314284984884\n",
      "Epoch: 212, step: 30, loss: 3.5042998021648777\n",
      "Epoch: 213, step: 0, loss: 3.416902542114258\n",
      "Epoch: 213, step: 5, loss: 3.496839682261149\n",
      "Epoch: 213, step: 10, loss: 3.4967451962557705\n",
      "Epoch: 213, step: 15, loss: 3.5094892382621765\n",
      "Epoch: 213, step: 20, loss: 3.4988057017326355\n",
      "Epoch: 213, step: 25, loss: 3.504236364364624\n",
      "Epoch: 213, step: 30, loss: 3.5072516043980917\n",
      "Epoch: 214, step: 0, loss: 3.4626457691192627\n",
      "Epoch: 214, step: 5, loss: 3.529297391573588\n",
      "Epoch: 214, step: 10, loss: 3.514246095310558\n",
      "Epoch: 214, step: 15, loss: 3.498883008956909\n",
      "Epoch: 214, step: 20, loss: 3.499739022482009\n",
      "Epoch: 214, step: 25, loss: 3.4962170674250674\n",
      "Epoch: 214, step: 30, loss: 3.502190997523646\n",
      "Epoch: 215, step: 0, loss: 3.5234336853027344\n",
      "Epoch: 215, step: 5, loss: 3.5050666332244873\n",
      "Epoch: 215, step: 10, loss: 3.5020687146620317\n",
      "Epoch: 215, step: 15, loss: 3.5090952664613724\n",
      "Epoch: 215, step: 20, loss: 3.506485553014846\n",
      "Epoch: 215, step: 25, loss: 3.5124869438318105\n",
      "Epoch: 215, step: 30, loss: 3.5002893478639665\n",
      "Epoch: 216, step: 0, loss: 3.4370787143707275\n",
      "Epoch: 216, step: 5, loss: 3.4838976860046387\n",
      "Epoch: 216, step: 10, loss: 3.4671970063989814\n",
      "Epoch: 216, step: 15, loss: 3.4768251925706863\n",
      "Epoch: 216, step: 20, loss: 3.478444292431786\n",
      "Epoch: 216, step: 25, loss: 3.489556156671964\n",
      "Epoch: 216, step: 30, loss: 3.4975848505573888\n",
      "Epoch: 217, step: 0, loss: 3.525188446044922\n",
      "Epoch: 217, step: 5, loss: 3.5258476734161377\n",
      "Epoch: 217, step: 10, loss: 3.4982306241989134\n",
      "Epoch: 217, step: 15, loss: 3.4834144751230878\n",
      "Epoch: 217, step: 20, loss: 3.4883622765541076\n",
      "Epoch: 217, step: 25, loss: 3.4925654125213623\n",
      "Epoch: 217, step: 30, loss: 3.490247956911723\n",
      "Epoch: 218, step: 0, loss: 3.352633237838745\n",
      "Epoch: 218, step: 5, loss: 3.51758881409963\n",
      "Epoch: 218, step: 10, loss: 3.5011309493671763\n",
      "Epoch: 218, step: 15, loss: 3.507315903902054\n",
      "Epoch: 218, step: 20, loss: 3.5023306097303117\n",
      "Epoch: 218, step: 25, loss: 3.5018271666306715\n",
      "Epoch: 218, step: 30, loss: 3.4973246743602138\n",
      "Epoch: 219, step: 0, loss: 3.560795545578003\n",
      "Epoch: 219, step: 5, loss: 3.4721844593683877\n",
      "Epoch: 219, step: 10, loss: 3.4668335914611816\n",
      "Epoch: 219, step: 15, loss: 3.4873881936073303\n",
      "Epoch: 219, step: 20, loss: 3.4922043823060536\n",
      "Epoch: 219, step: 25, loss: 3.4997568955788245\n",
      "Epoch: 219, step: 30, loss: 3.4953754255848546\n",
      "Epoch: 220, step: 0, loss: 3.4509897232055664\n",
      "Epoch: 220, step: 5, loss: 3.476508617401123\n",
      "Epoch: 220, step: 10, loss: 3.465664365074851\n",
      "Epoch: 220, step: 15, loss: 3.473105728626251\n",
      "Epoch: 220, step: 20, loss: 3.4711637724013555\n",
      "Epoch: 220, step: 25, loss: 3.494296825849093\n",
      "Epoch: 220, step: 30, loss: 3.4922969725824173\n",
      "Epoch: 221, step: 0, loss: 3.4653983116149902\n",
      "Epoch: 221, step: 5, loss: 3.463060903549194\n",
      "Epoch: 221, step: 10, loss: 3.486795735359192\n",
      "Epoch: 221, step: 15, loss: 3.4968835353851317\n",
      "Epoch: 221, step: 20, loss: 3.5121296644210815\n",
      "Epoch: 221, step: 25, loss: 3.5124249935150145\n",
      "Epoch: 221, step: 30, loss: 3.4971570491790773\n",
      "Epoch: 222, step: 0, loss: 3.5995020866394043\n",
      "Epoch: 222, step: 5, loss: 3.5735354820887246\n",
      "Epoch: 222, step: 10, loss: 3.5515091852708296\n",
      "Epoch: 222, step: 15, loss: 3.5291529446840286\n",
      "Epoch: 222, step: 20, loss: 3.5130698340279713\n",
      "Epoch: 222, step: 25, loss: 3.500844258528489\n",
      "Epoch: 222, step: 30, loss: 3.4942090588231243\n",
      "Epoch: 223, step: 0, loss: 3.5848770141601562\n",
      "Epoch: 223, step: 5, loss: 3.4951117833455405\n",
      "Epoch: 223, step: 10, loss: 3.49728326363997\n",
      "Epoch: 223, step: 15, loss: 3.50479356944561\n",
      "Epoch: 223, step: 20, loss: 3.499268656685239\n",
      "Epoch: 223, step: 25, loss: 3.4911183210519643\n",
      "Epoch: 223, step: 30, loss: 3.4947395170888593\n",
      "Epoch: 224, step: 0, loss: 3.571821689605713\n",
      "Epoch: 224, step: 5, loss: 3.493669350941976\n",
      "Epoch: 224, step: 10, loss: 3.470312010158192\n",
      "Epoch: 224, step: 15, loss: 3.4756479263305664\n",
      "Epoch: 224, step: 20, loss: 3.489442450659616\n",
      "Epoch: 224, step: 25, loss: 3.4828228033505955\n",
      "Epoch: 224, step: 30, loss: 3.4909229509292112\n",
      "Epoch: 225, step: 0, loss: 3.6281793117523193\n",
      "Epoch: 225, step: 5, loss: 3.553727308909098\n",
      "Epoch: 225, step: 10, loss: 3.5272241939197886\n",
      "Epoch: 225, step: 15, loss: 3.502464160323143\n",
      "Epoch: 225, step: 20, loss: 3.5000637412071227\n",
      "Epoch: 225, step: 25, loss: 3.492210578918457\n",
      "Epoch: 225, step: 30, loss: 3.4913029013008905\n",
      "Epoch: 226, step: 0, loss: 3.451615571975708\n",
      "Epoch: 226, step: 5, loss: 3.5035541454950967\n",
      "Epoch: 226, step: 10, loss: 3.483748110857877\n",
      "Epoch: 226, step: 15, loss: 3.479319989681244\n",
      "Epoch: 226, step: 20, loss: 3.4783636388324557\n",
      "Epoch: 226, step: 25, loss: 3.4918845158356886\n",
      "Epoch: 226, step: 30, loss: 3.489211989987281\n",
      "Epoch: 227, step: 0, loss: 3.4736971855163574\n",
      "Epoch: 227, step: 5, loss: 3.505244334538778\n",
      "Epoch: 227, step: 10, loss: 3.505265864458951\n",
      "Epoch: 227, step: 15, loss: 3.500430017709732\n",
      "Epoch: 227, step: 20, loss: 3.502194035053253\n",
      "Epoch: 227, step: 25, loss: 3.4978177547454834\n",
      "Epoch: 227, step: 30, loss: 3.4922476689020794\n",
      "Epoch: 228, step: 0, loss: 3.616243362426758\n",
      "Epoch: 228, step: 5, loss: 3.5593185822168985\n",
      "Epoch: 228, step: 10, loss: 3.535608941858465\n",
      "Epoch: 228, step: 15, loss: 3.4951988756656647\n",
      "Epoch: 228, step: 20, loss: 3.4986304879188537\n",
      "Epoch: 228, step: 25, loss: 3.4850611209869387\n",
      "Epoch: 228, step: 30, loss: 3.484401893615723\n",
      "Epoch: 229, step: 5, loss: 3.417027711868286\n",
      "Epoch: 229, step: 10, loss: 3.4420583486557006\n",
      "Epoch: 229, step: 15, loss: 3.4667949517567953\n",
      "Epoch: 229, step: 20, loss: 3.472841238975525\n",
      "Epoch: 229, step: 25, loss: 3.4796338844299317\n",
      "Epoch: 229, step: 30, loss: 3.490750026702881\n",
      "Epoch: 230, step: 0, loss: 3.460879325866699\n",
      "Epoch: 230, step: 5, loss: 3.4926299254099527\n",
      "Epoch: 230, step: 10, loss: 3.457252719185569\n",
      "Epoch: 230, step: 15, loss: 3.4770465344190598\n",
      "Epoch: 230, step: 20, loss: 3.4862102553957985\n",
      "Epoch: 230, step: 25, loss: 3.4994590007341824\n",
      "Epoch: 230, step: 30, loss: 3.490685970552506\n",
      "Epoch: 231, step: 0, loss: 3.4057674407958984\n",
      "Epoch: 231, step: 5, loss: 3.4494307041168213\n",
      "Epoch: 231, step: 10, loss: 3.468143333088268\n",
      "Epoch: 231, step: 15, loss: 3.4669872522354126\n",
      "Epoch: 231, step: 20, loss: 3.4700801826658703\n",
      "Epoch: 231, step: 25, loss: 3.475877835200383\n",
      "Epoch: 231, step: 30, loss: 3.484439642198624\n",
      "Epoch: 232, step: 0, loss: 3.4837069511413574\n",
      "Epoch: 232, step: 5, loss: 3.4914697806040444\n",
      "Epoch: 232, step: 10, loss: 3.4945074211467397\n",
      "Epoch: 232, step: 15, loss: 3.490225300192833\n",
      "Epoch: 232, step: 20, loss: 3.485457215990339\n",
      "Epoch: 232, step: 25, loss: 3.4892498254776\n",
      "Epoch: 232, step: 30, loss: 3.4878460591839207\n",
      "Epoch: 233, step: 0, loss: 3.4644570350646973\n",
      "Epoch: 233, step: 5, loss: 3.476435383160909\n",
      "Epoch: 233, step: 10, loss: 3.5005710558457808\n",
      "Epoch: 233, step: 15, loss: 3.4762117117643356\n",
      "Epoch: 233, step: 20, loss: 3.4899502038955688\n",
      "Epoch: 233, step: 25, loss: 3.485136709213257\n",
      "Epoch: 233, step: 30, loss: 3.4879391113917033\n",
      "Epoch: 234, step: 0, loss: 3.4938464164733887\n",
      "Epoch: 234, step: 5, loss: 3.47303569316864\n",
      "Epoch: 234, step: 10, loss: 3.4795691100033848\n",
      "Epoch: 234, step: 15, loss: 3.490791603922844\n",
      "Epoch: 234, step: 20, loss: 3.475808166322254\n",
      "Epoch: 234, step: 25, loss: 3.485905876526466\n",
      "Epoch: 234, step: 30, loss: 3.487852265757899\n",
      "Epoch: 235, step: 0, loss: 3.5824129581451416\n",
      "Epoch: 235, step: 5, loss: 3.4651963313420615\n",
      "Epoch: 235, step: 10, loss: 3.470413077961315\n",
      "Epoch: 235, step: 15, loss: 3.498445004224777\n",
      "Epoch: 235, step: 20, loss: 3.4797329107920327\n",
      "Epoch: 235, step: 25, loss: 3.486820633594806\n",
      "Epoch: 235, step: 30, loss: 3.4853822108237975\n",
      "Epoch: 236, step: 0, loss: 3.417990207672119\n",
      "Epoch: 236, step: 5, loss: 3.4526398181915283\n",
      "Epoch: 236, step: 10, loss: 3.4952717911113393\n",
      "Epoch: 236, step: 15, loss: 3.4922451227903366\n",
      "Epoch: 236, step: 20, loss: 3.493433089483352\n",
      "Epoch: 236, step: 25, loss: 3.4860571714547963\n",
      "Epoch: 236, step: 30, loss: 3.4864680151785574\n",
      "Epoch: 237, step: 0, loss: 3.470155715942383\n",
      "Epoch: 237, step: 5, loss: 3.4729501803716025\n",
      "Epoch: 237, step: 10, loss: 3.454490531574596\n",
      "Epoch: 237, step: 15, loss: 3.4613043665885925\n",
      "Epoch: 237, step: 20, loss: 3.4683104356129966\n",
      "Epoch: 237, step: 25, loss: 3.486398302591764\n",
      "Epoch: 237, step: 30, loss: 3.4831046596650155\n",
      "Epoch: 238, step: 0, loss: 3.4974822998046875\n",
      "Epoch: 238, step: 5, loss: 3.4835151036580405\n",
      "Epoch: 238, step: 10, loss: 3.476194453239441\n",
      "Epoch: 238, step: 15, loss: 3.4602692127227783\n",
      "Epoch: 238, step: 20, loss: 3.4705411911010744\n",
      "Epoch: 238, step: 25, loss: 3.4803075981140137\n",
      "Epoch: 238, step: 30, loss: 3.4811657746632894\n",
      "Epoch: 239, step: 0, loss: 3.4397387504577637\n",
      "Epoch: 239, step: 5, loss: 3.484006325403849\n",
      "Epoch: 239, step: 10, loss: 3.483150005340576\n",
      "Epoch: 239, step: 15, loss: 3.4677656292915344\n",
      "Epoch: 239, step: 20, loss: 3.473311628614153\n",
      "Epoch: 239, step: 25, loss: 3.4753806591033936\n",
      "Epoch: 239, step: 30, loss: 3.4801089994369017\n",
      "Epoch: 240, step: 0, loss: 3.555536985397339\n",
      "Epoch: 240, step: 5, loss: 3.4014641841252646\n",
      "Epoch: 240, step: 10, loss: 3.46454778584567\n",
      "Epoch: 240, step: 15, loss: 3.4722040593624115\n",
      "Epoch: 240, step: 20, loss: 3.4588139284224737\n",
      "Epoch: 240, step: 25, loss: 3.475819569367629\n",
      "Epoch: 240, step: 30, loss: 3.478193483045024\n",
      "Epoch: 241, step: 0, loss: 3.405989170074463\n",
      "Epoch: 241, step: 5, loss: 3.4609702825546265\n",
      "Epoch: 241, step: 10, loss: 3.508975397456776\n",
      "Epoch: 241, step: 15, loss: 3.4974460005760193\n",
      "Epoch: 241, step: 20, loss: 3.491195099694388\n",
      "Epoch: 241, step: 25, loss: 3.4968110139553366\n",
      "Epoch: 241, step: 30, loss: 3.4820498727983042\n",
      "Epoch: 242, step: 0, loss: 3.5905508995056152\n",
      "Epoch: 242, step: 5, loss: 3.5180852810541787\n",
      "Epoch: 242, step: 15, loss: 3.503812869389852\n",
      "Epoch: 242, step: 20, loss: 3.497075629234314\n",
      "Epoch: 242, step: 25, loss: 3.4900496864318846\n",
      "Epoch: 242, step: 30, loss: 3.4810455004374186\n",
      "Epoch: 243, step: 0, loss: 3.492804765701294\n",
      "Epoch: 243, step: 5, loss: 3.528725743293762\n",
      "Epoch: 243, step: 10, loss: 3.4919856028123335\n",
      "Epoch: 243, step: 15, loss: 3.4832408726215363\n",
      "Epoch: 243, step: 20, loss: 3.4906505403064547\n",
      "Epoch: 243, step: 25, loss: 3.4902068926737857\n",
      "Epoch: 243, step: 30, loss: 3.4823620396275676\n",
      "Epoch: 244, step: 0, loss: 3.496364116668701\n",
      "Epoch: 244, step: 5, loss: 3.52163036664327\n",
      "Epoch: 244, step: 10, loss: 3.5025274753570557\n",
      "Epoch: 244, step: 15, loss: 3.4933033138513565\n",
      "Epoch: 244, step: 20, loss: 3.4773913678668795\n",
      "Epoch: 244, step: 25, loss: 3.470042109489441\n",
      "Epoch: 244, step: 30, loss: 3.4778309483681955\n",
      "Epoch: 245, step: 0, loss: 3.4573607444763184\n",
      "Epoch: 245, step: 5, loss: 3.460954507191976\n",
      "Epoch: 245, step: 10, loss: 3.455116618763317\n",
      "Epoch: 245, step: 15, loss: 3.4767817854881287\n",
      "Epoch: 245, step: 20, loss: 3.47605486143203\n",
      "Epoch: 245, step: 25, loss: 3.4777442125173716\n",
      "Epoch: 245, step: 30, loss: 3.4815133310133413\n",
      "Epoch: 246, step: 0, loss: 3.407985210418701\n",
      "Epoch: 246, step: 5, loss: 3.4554803371429443\n",
      "Epoch: 246, step: 10, loss: 3.444392269307917\n",
      "Epoch: 246, step: 15, loss: 3.4667962789535522\n",
      "Epoch: 246, step: 20, loss: 3.4749460106804255\n",
      "Epoch: 246, step: 25, loss: 3.4785557710207424\n",
      "Epoch: 246, step: 30, loss: 3.477113616081976\n",
      "Epoch: 247, step: 0, loss: 3.485138416290283\n",
      "Epoch: 247, step: 5, loss: 3.4659868478775024\n",
      "Epoch: 247, step: 10, loss: 3.4882891178131104\n",
      "Epoch: 247, step: 15, loss: 3.4977466613054276\n",
      "Epoch: 247, step: 20, loss: 3.488100290298462\n",
      "Epoch: 247, step: 25, loss: 3.484910167180575\n",
      "Epoch: 247, step: 30, loss: 3.4760425090789795\n",
      "Epoch: 248, step: 0, loss: 3.4643094539642334\n",
      "Epoch: 248, step: 5, loss: 3.4650240739186606\n",
      "Epoch: 248, step: 10, loss: 3.482940847223455\n",
      "Epoch: 248, step: 15, loss: 3.4842242747545242\n",
      "Epoch: 248, step: 20, loss: 3.500697817121233\n",
      "Epoch: 248, step: 25, loss: 3.4810020740215597\n",
      "Epoch: 248, step: 30, loss: 3.4778440306263585\n",
      "Epoch: 249, step: 0, loss: 3.4937679767608643\n",
      "Epoch: 249, step: 5, loss: 3.468858480453491\n",
      "Epoch: 249, step: 10, loss: 3.4734914086081763\n",
      "Epoch: 249, step: 15, loss: 3.465511754155159\n",
      "Epoch: 249, step: 20, loss: 3.469059785207113\n",
      "Epoch: 249, step: 25, loss: 3.470068234663743\n",
      "Epoch: 249, step: 30, loss: 3.476329380466092\n",
      "Epoch: 250, step: 0, loss: 3.5001094341278076\n",
      "Epoch: 250, step: 5, loss: 3.458989143371582\n",
      "Epoch: 250, step: 10, loss: 3.4702368432825264\n",
      "Epoch: 250, step: 15, loss: 3.4699788838624954\n",
      "Epoch: 250, step: 20, loss: 3.468673728761219\n",
      "Epoch: 250, step: 25, loss: 3.4655460119247437\n",
      "Epoch: 250, step: 30, loss: 3.4718212927541425\n",
      "Epoch: 251, step: 0, loss: 3.522225856781006\n",
      "Epoch: 251, step: 5, loss: 3.4897936582565308\n",
      "Epoch: 251, step: 10, loss: 3.4649069092490454\n",
      "Epoch: 251, step: 15, loss: 3.4768186807632446\n",
      "Epoch: 251, step: 20, loss: 3.47071403548831\n",
      "Epoch: 251, step: 25, loss: 3.4700783032637377\n",
      "Epoch: 251, step: 30, loss: 3.4743497602401243\n",
      "Epoch: 252, step: 0, loss: 3.5602598190307617\n",
      "Epoch: 252, step: 5, loss: 3.464978655179342\n",
      "Epoch: 252, step: 10, loss: 3.467487421902743\n",
      "Epoch: 252, step: 15, loss: 3.4718099534511566\n",
      "Epoch: 252, step: 20, loss: 3.475187142690023\n",
      "Epoch: 252, step: 25, loss: 3.475733784528879\n",
      "Epoch: 252, step: 30, loss: 3.4735435285875873\n",
      "Epoch: 253, step: 0, loss: 3.372087001800537\n",
      "Epoch: 253, step: 5, loss: 3.4419556856155396\n",
      "Epoch: 253, step: 10, loss: 3.4727671363136987\n",
      "Epoch: 253, step: 15, loss: 3.4892587661743164\n",
      "Epoch: 253, step: 20, loss: 3.4825671968005953\n",
      "Epoch: 253, step: 25, loss: 3.474412514613225\n",
      "Epoch: 253, step: 30, loss: 3.4775616661194833\n",
      "Epoch: 254, step: 0, loss: 3.43925142288208\n",
      "Epoch: 254, step: 5, loss: 3.4978157679239907\n",
      "Epoch: 254, step: 10, loss: 3.4944602142680776\n",
      "Epoch: 254, step: 15, loss: 3.489781156182289\n",
      "Epoch: 254, step: 20, loss: 3.479109037490118\n",
      "Epoch: 254, step: 25, loss: 3.474428596496582\n",
      "Epoch: 254, step: 30, loss: 3.474351493517558\n",
      "Epoch: 255, step: 0, loss: 3.565793037414551\n",
      "Epoch: 255, step: 5, loss: 3.4468405644098916\n",
      "Epoch: 255, step: 10, loss: 3.4672167734666304\n",
      "Epoch: 255, step: 15, loss: 3.4729723930358887\n",
      "Epoch: 255, step: 20, loss: 3.475946131206694\n",
      "Epoch: 255, step: 25, loss: 3.4787832773648777\n",
      "Epoch: 255, step: 30, loss: 3.4739765121090795\n",
      "Epoch: 256, step: 0, loss: 3.5152361392974854\n",
      "Epoch: 256, step: 5, loss: 3.482162594795227\n",
      "Epoch: 256, step: 10, loss: 3.494635885412043\n",
      "Epoch: 256, step: 15, loss: 3.4822509586811066\n",
      "Epoch: 256, step: 20, loss: 3.4826097034272694\n",
      "Epoch: 256, step: 25, loss: 3.4744928983541636\n",
      "Epoch: 256, step: 30, loss: 3.4712666773026988\n",
      "Epoch: 257, step: 0, loss: 3.5343782901763916\n",
      "Epoch: 257, step: 5, loss: 3.472031513849894\n",
      "Epoch: 257, step: 10, loss: 3.4826212146065454\n",
      "Epoch: 257, step: 15, loss: 3.481647416949272\n",
      "Epoch: 257, step: 20, loss: 3.472691853841146\n",
      "Epoch: 257, step: 25, loss: 3.466788337780879\n",
      "Epoch: 257, step: 30, loss: 3.4658036385813067\n",
      "Epoch: 258, step: 0, loss: 3.5536375045776367\n",
      "Epoch: 258, step: 5, loss: 3.467851916948954\n",
      "Epoch: 258, step: 10, loss: 3.4835428541356865\n",
      "Epoch: 258, step: 15, loss: 3.4820432513952255\n",
      "Epoch: 258, step: 20, loss: 3.480748244694301\n",
      "Epoch: 258, step: 25, loss: 3.4628086548585157\n",
      "Epoch: 258, step: 30, loss: 3.467634893232776\n",
      "Epoch: 259, step: 0, loss: 3.4255471229553223\n",
      "Epoch: 259, step: 5, loss: 3.436040242513021\n",
      "Epoch: 259, step: 10, loss: 3.4499839435924184\n",
      "Epoch: 259, step: 15, loss: 3.455733001232147\n",
      "Epoch: 259, step: 20, loss: 3.465271541050502\n",
      "Epoch: 259, step: 25, loss: 3.4681875705718994\n",
      "Epoch: 259, step: 30, loss: 3.465657026537003\n",
      "Epoch: 260, step: 0, loss: 3.420722723007202\n",
      "Epoch: 260, step: 5, loss: 3.4507166941960654\n",
      "Epoch: 260, step: 10, loss: 3.4789243828166616\n",
      "Epoch: 260, step: 15, loss: 3.4776075929403305\n",
      "Epoch: 260, step: 20, loss: 3.4706561792464483\n",
      "Epoch: 260, step: 25, loss: 3.4740545382866492\n",
      "Epoch: 260, step: 30, loss: 3.4740419849272697\n",
      "Epoch: 261, step: 0, loss: 3.6657907962799072\n",
      "Epoch: 261, step: 5, loss: 3.493347406387329\n",
      "Epoch: 261, step: 10, loss: 3.497708494013006\n",
      "Epoch: 261, step: 15, loss: 3.489249277114868\n",
      "Epoch: 261, step: 20, loss: 3.4736696004867555\n",
      "Epoch: 261, step: 25, loss: 3.4713861656188967\n",
      "Epoch: 261, step: 30, loss: 3.467573881149292\n",
      "Epoch: 262, step: 0, loss: 3.3794713020324707\n",
      "Epoch: 262, step: 5, loss: 3.453287442525228\n",
      "Epoch: 262, step: 10, loss: 3.4772548458792945\n",
      "Epoch: 262, step: 15, loss: 3.4737519323825836\n",
      "Epoch: 262, step: 20, loss: 3.4677148092360723\n",
      "Epoch: 262, step: 25, loss: 3.4678287964600782\n",
      "Epoch: 262, step: 30, loss: 3.4657497098368983\n",
      "Epoch: 263, step: 0, loss: 3.3905129432678223\n",
      "Epoch: 263, step: 5, loss: 3.3882498343785605\n",
      "Epoch: 263, step: 10, loss: 3.4334078051827173\n",
      "Epoch: 263, step: 15, loss: 3.451576814055443\n",
      "Epoch: 263, step: 20, loss: 3.4591650054568337\n",
      "Epoch: 263, step: 25, loss: 3.4641788647725034\n",
      "Epoch: 263, step: 30, loss: 3.4653980334599814\n",
      "Epoch: 264, step: 0, loss: 3.4234261512756348\n",
      "Epoch: 264, step: 5, loss: 3.4166900316874185\n",
      "Epoch: 264, step: 10, loss: 3.4632925553755327\n",
      "Epoch: 264, step: 15, loss: 3.4648512452840805\n",
      "Epoch: 264, step: 20, loss: 3.4639474664415633\n",
      "Epoch: 264, step: 25, loss: 3.4503986010184655\n",
      "Epoch: 264, step: 30, loss: 3.463445301978819\n",
      "Epoch: 265, step: 0, loss: 3.5211093425750732\n",
      "Epoch: 265, step: 5, loss: 3.4497992595036826\n",
      "Epoch: 265, step: 10, loss: 3.44774207201871\n",
      "Epoch: 265, step: 15, loss: 3.46036647160848\n",
      "Epoch: 265, step: 20, loss: 3.4588972091674806\n",
      "Epoch: 265, step: 25, loss: 3.4641908407211304\n",
      "Epoch: 265, step: 30, loss: 3.466218882593615\n",
      "Epoch: 266, step: 0, loss: 3.402226448059082\n",
      "Epoch: 266, step: 5, loss: 3.4792741934458413\n",
      "Epoch: 266, step: 10, loss: 3.4831149794838647\n",
      "Epoch: 266, step: 15, loss: 3.483474365870158\n",
      "Epoch: 266, step: 20, loss: 3.4806932687759398\n",
      "Epoch: 266, step: 25, loss: 3.464998331069946\n",
      "Epoch: 266, step: 30, loss: 3.4630178689956663\n",
      "Epoch: 267, step: 0, loss: 3.4652860164642334\n",
      "Epoch: 267, step: 5, loss: 3.438193202018738\n",
      "Epoch: 267, step: 10, loss: 3.4230548620223997\n",
      "Epoch: 267, step: 15, loss: 3.447591288884481\n",
      "Epoch: 267, step: 20, loss: 3.463264584541321\n",
      "Epoch: 267, step: 25, loss: 3.461296682357788\n",
      "Epoch: 267, step: 30, loss: 3.4664143244425456\n",
      "Epoch: 268, step: 0, loss: 3.5509471893310547\n",
      "Epoch: 268, step: 5, loss: 3.4858853816986084\n",
      "Epoch: 268, step: 10, loss: 3.5065576076507567\n",
      "Epoch: 268, step: 15, loss: 3.4945329348246257\n",
      "Epoch: 268, step: 20, loss: 3.467959225177765\n",
      "Epoch: 268, step: 25, loss: 3.4591045284271242\n",
      "Epoch: 268, step: 30, loss: 3.46082915465037\n",
      "Epoch: 269, step: 0, loss: 3.48903751373291\n",
      "Epoch: 269, step: 5, loss: 3.4467941522598267\n",
      "Epoch: 269, step: 10, loss: 3.449803590774536\n",
      "Epoch: 269, step: 15, loss: 3.454950968424479\n",
      "Epoch: 269, step: 20, loss: 3.444129168987274\n",
      "Epoch: 269, step: 25, loss: 3.454413480758667\n",
      "Epoch: 269, step: 30, loss: 3.4617390632629395\n",
      "Epoch: 270, step: 0, loss: 3.4114558696746826\n",
      "Epoch: 270, step: 5, loss: 3.4188582499821982\n",
      "Epoch: 270, step: 10, loss: 3.432156346061013\n",
      "Epoch: 270, step: 15, loss: 3.454078748822212\n",
      "Epoch: 270, step: 20, loss: 3.4633939379737493\n",
      "Epoch: 270, step: 25, loss: 3.468718941395099\n",
      "Epoch: 270, step: 30, loss: 3.4641297786466536\n",
      "Epoch: 271, step: 0, loss: 3.6439871788024902\n",
      "Epoch: 271, step: 5, loss: 3.503937840461731\n",
      "Epoch: 271, step: 10, loss: 3.4873698191209273\n",
      "Epoch: 271, step: 15, loss: 3.4849785566329956\n",
      "Epoch: 271, step: 25, loss: 3.458051472902298\n",
      "Epoch: 271, step: 30, loss: 3.4600060890460838\n",
      "Epoch: 272, step: 0, loss: 3.465433359146118\n",
      "Epoch: 272, step: 5, loss: 3.4895858764648438\n",
      "Epoch: 272, step: 10, loss: 3.477952480316162\n",
      "Epoch: 272, step: 15, loss: 3.473101422190666\n",
      "Epoch: 272, step: 20, loss: 3.469665697642735\n",
      "Epoch: 272, step: 25, loss: 3.469339682505681\n",
      "Epoch: 272, step: 30, loss: 3.462200187867688\n",
      "Epoch: 273, step: 0, loss: 3.5539026260375977\n",
      "Epoch: 273, step: 5, loss: 3.4507168531417847\n",
      "Epoch: 273, step: 10, loss: 3.462639180096713\n",
      "Epoch: 273, step: 15, loss: 3.4578894823789597\n",
      "Epoch: 273, step: 20, loss: 3.4526726484298704\n",
      "Epoch: 273, step: 25, loss: 3.4640048599243163\n",
      "Epoch: 273, step: 30, loss: 3.457325294100005\n",
      "Epoch: 274, step: 0, loss: 3.3837645053863525\n",
      "Epoch: 274, step: 5, loss: 3.452410022417704\n",
      "Epoch: 274, step: 10, loss: 3.4484619227322666\n",
      "Epoch: 274, step: 15, loss: 3.443650931119919\n",
      "Epoch: 274, step: 20, loss: 3.4582995006016324\n",
      "Epoch: 274, step: 25, loss: 3.4487204368297872\n",
      "Epoch: 274, step: 30, loss: 3.4607418583285425\n",
      "Epoch: 275, step: 0, loss: 3.3843581676483154\n",
      "Epoch: 275, step: 5, loss: 3.4450532595316568\n",
      "Epoch: 275, step: 10, loss: 3.466118162328547\n",
      "Epoch: 275, step: 15, loss: 3.4723439663648605\n",
      "Epoch: 275, step: 20, loss: 3.4593235424586704\n",
      "Epoch: 275, step: 25, loss: 3.459832237317012\n",
      "Epoch: 275, step: 30, loss: 3.458037507149481\n",
      "Epoch: 276, step: 0, loss: 3.499192953109741\n",
      "Epoch: 276, step: 5, loss: 3.421394944190979\n",
      "Epoch: 276, step: 10, loss: 3.4556221528486772\n",
      "Epoch: 276, step: 15, loss: 3.466316595673561\n",
      "Epoch: 276, step: 20, loss: 3.4620687620980397\n",
      "Epoch: 276, step: 25, loss: 3.4632975229850183\n",
      "Epoch: 276, step: 30, loss: 3.464764625795426\n",
      "Epoch: 277, step: 0, loss: 3.4867069721221924\n",
      "Epoch: 277, step: 5, loss: 3.4368762572606406\n",
      "Epoch: 277, step: 10, loss: 3.4461095333099365\n",
      "Epoch: 277, step: 15, loss: 3.4466840773820877\n",
      "Epoch: 277, step: 20, loss: 3.447211186091105\n",
      "Epoch: 277, step: 25, loss: 3.4653114722325253\n",
      "Epoch: 277, step: 30, loss: 3.461668760545792\n",
      "Epoch: 278, step: 0, loss: 3.5897881984710693\n",
      "Epoch: 278, step: 5, loss: 3.4631358782450357\n",
      "Epoch: 278, step: 10, loss: 3.4586544687097724\n",
      "Epoch: 278, step: 15, loss: 3.447605714201927\n",
      "Epoch: 278, step: 20, loss: 3.447468916575114\n",
      "Epoch: 278, step: 25, loss: 3.454501977333656\n",
      "Epoch: 278, step: 30, loss: 3.455616274187642\n",
      "Epoch: 279, step: 0, loss: 3.3777923583984375\n",
      "Epoch: 279, step: 5, loss: 3.412683288256327\n",
      "Epoch: 279, step: 10, loss: 3.4345342462713067\n",
      "Epoch: 279, step: 15, loss: 3.443439558148384\n",
      "Epoch: 279, step: 20, loss: 3.455921729405721\n",
      "Epoch: 279, step: 25, loss: 3.452135150249188\n",
      "Epoch: 279, step: 30, loss: 3.4585401473506803\n",
      "Epoch: 280, step: 0, loss: 3.4882121086120605\n",
      "Epoch: 280, step: 5, loss: 3.4696176846822104\n",
      "Epoch: 280, step: 10, loss: 3.500425837256692\n",
      "Epoch: 280, step: 15, loss: 3.4745388329029083\n",
      "Epoch: 280, step: 20, loss: 3.462548948469616\n",
      "Epoch: 280, step: 25, loss: 3.4563835767599254\n",
      "Epoch: 280, step: 30, loss: 3.456312125728976\n",
      "Epoch: 281, step: 0, loss: 3.5526747703552246\n",
      "Epoch: 281, step: 5, loss: 3.4138291676839194\n",
      "Epoch: 281, step: 10, loss: 3.4370667067441074\n",
      "Epoch: 281, step: 15, loss: 3.438871055841446\n",
      "Epoch: 281, step: 20, loss: 3.4358492351713634\n",
      "Epoch: 281, step: 25, loss: 3.44695023389963\n",
      "Epoch: 281, step: 30, loss: 3.4567383181664253\n",
      "Epoch: 282, step: 0, loss: 3.6110751628875732\n",
      "Epoch: 282, step: 5, loss: 3.4541097482045493\n",
      "Epoch: 282, step: 10, loss: 3.4776889194141734\n",
      "Epoch: 282, step: 15, loss: 3.481647625565529\n",
      "Epoch: 282, step: 20, loss: 3.463626918338594\n",
      "Epoch: 282, step: 25, loss: 3.455674180617699\n",
      "Epoch: 282, step: 30, loss: 3.4568626496099655\n",
      "Epoch: 283, step: 0, loss: 3.432572603225708\n",
      "Epoch: 283, step: 5, loss: 3.477809945742289\n",
      "Epoch: 283, step: 10, loss: 3.4694022698835894\n",
      "Epoch: 283, step: 15, loss: 3.466545954346657\n",
      "Epoch: 283, step: 20, loss: 3.458298478807722\n",
      "Epoch: 283, step: 25, loss: 3.4658221831688514\n",
      "Epoch: 283, step: 30, loss: 3.4545792918051443\n",
      "Epoch: 284, step: 0, loss: 3.4084742069244385\n",
      "Epoch: 284, step: 5, loss: 3.438502033551534\n",
      "Epoch: 284, step: 10, loss: 3.46254639192061\n",
      "Epoch: 284, step: 15, loss: 3.461355909705162\n",
      "Epoch: 284, step: 20, loss: 3.456840821674892\n",
      "Epoch: 284, step: 25, loss: 3.4617059597602258\n",
      "Epoch: 284, step: 30, loss: 3.454134464263916\n",
      "Epoch: 285, step: 0, loss: 3.570685386657715\n",
      "Epoch: 285, step: 5, loss: 3.4910009860992433\n",
      "Epoch: 285, step: 10, loss: 3.496786689758301\n",
      "Epoch: 285, step: 15, loss: 3.490221373240153\n",
      "Epoch: 285, step: 20, loss: 3.4620938658714295\n",
      "Epoch: 285, step: 25, loss: 3.4587160968780517\n",
      "Epoch: 285, step: 30, loss: 3.455336507161458\n",
      "Epoch: 286, step: 0, loss: 3.4998481273651123\n",
      "Epoch: 286, step: 5, loss: 3.4390505154927573\n",
      "Epoch: 286, step: 10, loss: 3.4505151835354892\n",
      "Epoch: 286, step: 15, loss: 3.455843046307564\n",
      "Epoch: 286, step: 20, loss: 3.4625198614029657\n",
      "Epoch: 286, step: 25, loss: 3.4584133625030518\n",
      "Epoch: 286, step: 30, loss: 3.4543914487285\n",
      "Epoch: 287, step: 0, loss: 3.4524989128112793\n",
      "Epoch: 287, step: 5, loss: 3.4301419655481973\n",
      "Epoch: 287, step: 10, loss: 3.433624419299039\n",
      "Epoch: 287, step: 15, loss: 3.4423428773880005\n",
      "Epoch: 287, step: 20, loss: 3.436239719390869\n",
      "Epoch: 287, step: 25, loss: 3.451430008961604\n",
      "Epoch: 287, step: 30, loss: 3.4550185280461467\n",
      "Epoch: 288, step: 0, loss: 3.289355993270874\n",
      "Epoch: 288, step: 5, loss: 3.4784220695495605\n",
      "Epoch: 288, step: 10, loss: 3.449227285385132\n",
      "Epoch: 288, step: 15, loss: 3.4631884892781577\n",
      "Epoch: 288, step: 20, loss: 3.4792497992515563\n",
      "Epoch: 288, step: 25, loss: 3.4554015731811525\n",
      "Epoch: 288, step: 30, loss: 3.4552616993586223\n",
      "Epoch: 289, step: 0, loss: 3.401549816131592\n",
      "Epoch: 289, step: 5, loss: 3.436172525087992\n",
      "Epoch: 289, step: 10, loss: 3.454522653059526\n",
      "Epoch: 289, step: 15, loss: 3.4740122109651566\n",
      "Epoch: 289, step: 20, loss: 3.4500035785493397\n",
      "Epoch: 289, step: 25, loss: 3.4559901035748997\n",
      "Epoch: 289, step: 30, loss: 3.452506196114325\n",
      "Epoch: 290, step: 0, loss: 3.4907026290893555\n",
      "Epoch: 290, step: 5, loss: 3.4057898124059043\n",
      "Epoch: 290, step: 10, loss: 3.4388963092457163\n",
      "Epoch: 290, step: 15, loss: 3.4480885565280914\n",
      "Epoch: 290, step: 20, loss: 3.4456067766462053\n",
      "Epoch: 290, step: 25, loss: 3.4497649302849402\n",
      "Epoch: 290, step: 30, loss: 3.454296096678703\n",
      "Epoch: 291, step: 0, loss: 3.4448256492614746\n",
      "Epoch: 291, step: 5, loss: 3.482656995455424\n",
      "Epoch: 291, step: 10, loss: 3.4352814717726274\n",
      "Epoch: 291, step: 15, loss: 3.4485737085342407\n",
      "Epoch: 291, step: 20, loss: 3.4502470266251337\n",
      "Epoch: 291, step: 25, loss: 3.4424895414939294\n",
      "Epoch: 291, step: 30, loss: 3.450190697946856\n",
      "Epoch: 292, step: 0, loss: 3.4062252044677734\n",
      "Epoch: 292, step: 5, loss: 3.4177942673365274\n",
      "Epoch: 292, step: 10, loss: 3.441747535358776\n",
      "Epoch: 292, step: 15, loss: 3.4484407901763916\n",
      "Epoch: 292, step: 20, loss: 3.4458841255732944\n",
      "Epoch: 292, step: 25, loss: 3.453742780685425\n",
      "Epoch: 292, step: 30, loss: 3.453419550259908\n",
      "Epoch: 293, step: 0, loss: 3.579049825668335\n",
      "Epoch: 293, step: 5, loss: 3.490464687347412\n",
      "Epoch: 293, step: 10, loss: 3.4624276377938012\n",
      "Epoch: 293, step: 15, loss: 3.44976469874382\n",
      "Epoch: 293, step: 20, loss: 3.441575674783616\n",
      "Epoch: 293, step: 25, loss: 3.4468631194188046\n",
      "Epoch: 293, step: 30, loss: 3.4524820543104604\n",
      "Epoch: 294, step: 0, loss: 3.467064619064331\n",
      "Epoch: 294, step: 5, loss: 3.4292694330215454\n",
      "Epoch: 294, step: 10, loss: 3.437114867297086\n",
      "Epoch: 294, step: 15, loss: 3.4420517683029175\n",
      "Epoch: 294, step: 20, loss: 3.4361648786635626\n",
      "Epoch: 294, step: 25, loss: 3.448738382412837\n",
      "Epoch: 294, step: 30, loss: 3.44676996046497\n",
      "Epoch: 295, step: 0, loss: 3.5077266693115234\n",
      "Epoch: 295, step: 5, loss: 3.463646094004313\n",
      "Epoch: 295, step: 10, loss: 3.4585416316986084\n",
      "Epoch: 295, step: 15, loss: 3.4414838552474976\n",
      "Epoch: 295, step: 20, loss: 3.44918798265003\n",
      "Epoch: 295, step: 25, loss: 3.4550992984038134\n",
      "Epoch: 295, step: 30, loss: 3.4481641784791024\n",
      "Epoch: 296, step: 0, loss: 3.450455665588379\n",
      "Epoch: 296, step: 5, loss: 3.48863955338796\n",
      "Epoch: 296, step: 10, loss: 3.4637082490054043\n",
      "Epoch: 296, step: 15, loss: 3.448081612586975\n",
      "Epoch: 296, step: 20, loss: 3.4404535520644415\n",
      "Epoch: 296, step: 25, loss: 3.446274839914762\n",
      "Epoch: 296, step: 30, loss: 3.445696730767527\n",
      "Epoch: 297, step: 0, loss: 3.4277870655059814\n",
      "Epoch: 297, step: 5, loss: 3.4313203493754068\n",
      "Epoch: 297, step: 10, loss: 3.4162134690718218\n",
      "Epoch: 297, step: 15, loss: 3.429828628897667\n",
      "Epoch: 297, step: 20, loss: 3.429043996901739\n",
      "Epoch: 297, step: 25, loss: 3.440006842980018\n",
      "Epoch: 297, step: 30, loss: 3.449861034270256\n",
      "Epoch: 298, step: 0, loss: 3.303293466567993\n",
      "Epoch: 298, step: 5, loss: 3.386285662651062\n",
      "Epoch: 298, step: 10, loss: 3.4146046855232934\n",
      "Epoch: 298, step: 15, loss: 3.4247097969055176\n",
      "Epoch: 298, step: 20, loss: 3.442959047499157\n",
      "Epoch: 298, step: 25, loss: 3.4512616854447584\n",
      "Epoch: 298, step: 30, loss: 3.4444025408837105\n",
      "Epoch: 299, step: 0, loss: 3.5356526374816895\n",
      "Epoch: 299, step: 5, loss: 3.4250324169794717\n",
      "Epoch: 299, step: 10, loss: 3.4252268184315073\n",
      "Epoch: 299, step: 15, loss: 3.439914807677269\n",
      "Epoch: 299, step: 20, loss: 3.442120563416254\n",
      "Epoch: 299, step: 25, loss: 3.4437495378347545\n",
      "Epoch: 299, step: 30, loss: 3.449621900435417\n",
      "Epoch: 300, step: 0, loss: 3.324411630630493\n",
      "Epoch: 300, step: 5, loss: 3.4126036961873374\n",
      "Epoch: 300, step: 10, loss: 3.4434779340570625\n",
      "Epoch: 300, step: 15, loss: 3.448455825448036\n",
      "Epoch: 300, step: 20, loss: 3.44811106863476\n",
      "Epoch: 300, step: 25, loss: 3.444769107378446\n",
      "Epoch: 300, step: 30, loss: 3.4511622305839293\n",
      "Epoch: 301, step: 0, loss: 3.529257297515869\n",
      "Epoch: 301, step: 5, loss: 3.4712464412053428\n",
      "Epoch: 301, step: 10, loss: 3.473356680436568\n",
      "Epoch: 301, step: 15, loss: 3.4630052596330643\n",
      "Epoch: 301, step: 20, loss: 3.4639819236028764\n",
      "Epoch: 301, step: 25, loss: 3.450491134936993\n",
      "Epoch: 301, step: 30, loss: 3.4483104213591544\n",
      "Epoch: 302, step: 0, loss: 3.5313358306884766\n",
      "Epoch: 302, step: 5, loss: 3.419556188583374\n",
      "Epoch: 302, step: 10, loss: 3.4485127210617064\n",
      "Epoch: 302, step: 15, loss: 3.4557278156280518\n",
      "Epoch: 302, step: 20, loss: 3.4558234572410584\n",
      "Epoch: 302, step: 25, loss: 3.442303171157837\n",
      "Epoch: 302, step: 30, loss: 3.4468372027079264\n",
      "Epoch: 303, step: 0, loss: 3.479483127593994\n",
      "Epoch: 303, step: 5, loss: 3.517150640487671\n",
      "Epoch: 303, step: 10, loss: 3.4390956271778452\n",
      "Epoch: 303, step: 15, loss: 3.448540136218071\n",
      "Epoch: 303, step: 20, loss: 3.447609606243315\n",
      "Epoch: 303, step: 25, loss: 3.451596003312331\n",
      "Epoch: 303, step: 30, loss: 3.4463104663356656\n",
      "Epoch: 304, step: 0, loss: 3.4826300144195557\n",
      "Epoch: 304, step: 5, loss: 3.4562977155049643\n",
      "Epoch: 304, step: 10, loss: 3.435979561372237\n",
      "Epoch: 304, step: 15, loss: 3.428384155035019\n",
      "Epoch: 304, step: 20, loss: 3.4460791860307967\n",
      "Epoch: 304, step: 25, loss: 3.4439224096444936\n",
      "Epoch: 304, step: 30, loss: 3.442480018061976\n",
      "Epoch: 305, step: 0, loss: 3.4215807914733887\n",
      "Epoch: 305, step: 5, loss: 3.431183417638143\n",
      "Epoch: 305, step: 10, loss: 3.438197439367121\n",
      "Epoch: 305, step: 15, loss: 3.4258655309677124\n",
      "Epoch: 305, step: 20, loss: 3.430322011311849\n",
      "Epoch: 305, step: 25, loss: 3.4333252906799316\n",
      "Epoch: 305, step: 30, loss: 3.442954740216655\n",
      "Epoch: 306, step: 0, loss: 3.3647918701171875\n",
      "Epoch: 306, step: 5, loss: 3.4797215461730957\n",
      "Epoch: 306, step: 10, loss: 3.4379163438623603\n",
      "Epoch: 306, step: 15, loss: 3.4388616383075714\n",
      "Epoch: 306, step: 20, loss: 3.434722207841419\n",
      "Epoch: 306, step: 25, loss: 3.43691662641672\n",
      "Epoch: 306, step: 30, loss: 3.4425353311723277\n",
      "Epoch: 307, step: 0, loss: 3.3833866119384766\n",
      "Epoch: 307, step: 5, loss: 3.4380542437235513\n",
      "Epoch: 307, step: 10, loss: 3.44560508294539\n",
      "Epoch: 307, step: 15, loss: 3.427725672721863\n",
      "Epoch: 307, step: 20, loss: 3.4379197529384067\n",
      "Epoch: 307, step: 25, loss: 3.436979137934171\n",
      "Epoch: 307, step: 30, loss: 3.4444674291918354\n",
      "Epoch: 308, step: 0, loss: 3.440507173538208\n",
      "Epoch: 308, step: 5, loss: 3.4385033448537192\n",
      "Epoch: 308, step: 10, loss: 3.4465327739715574\n",
      "Epoch: 308, step: 15, loss: 3.45234751701355\n",
      "Epoch: 308, step: 20, loss: 3.4428855538368226\n",
      "Epoch: 308, step: 25, loss: 3.4404136085510255\n",
      "Epoch: 308, step: 30, loss: 3.4434265454610187\n",
      "Epoch: 309, step: 0, loss: 3.318897247314453\n",
      "Epoch: 309, step: 5, loss: 3.4055380821228027\n",
      "Epoch: 309, step: 10, loss: 3.447851137681441\n",
      "Epoch: 309, step: 15, loss: 3.4307768493890762\n",
      "Epoch: 309, step: 20, loss: 3.4412659122830345\n",
      "Epoch: 309, step: 25, loss: 3.441436144021841\n",
      "Epoch: 309, step: 30, loss: 3.440165227459323\n",
      "Epoch: 310, step: 0, loss: 3.4540297985076904\n",
      "Epoch: 310, step: 5, loss: 3.418004115422567\n",
      "Epoch: 310, step: 10, loss: 3.420280196449973\n",
      "Epoch: 310, step: 15, loss: 3.4299440532922745\n",
      "Epoch: 310, step: 20, loss: 3.4413411163148426\n",
      "Epoch: 310, step: 25, loss: 3.4449952840805054\n",
      "Epoch: 310, step: 30, loss: 3.441753841215564\n",
      "Epoch: 311, step: 0, loss: 3.5062508583068848\n",
      "Epoch: 311, step: 5, loss: 3.439764897028605\n",
      "Epoch: 311, step: 10, loss: 3.435453111475164\n",
      "Epoch: 311, step: 15, loss: 3.435183674097061\n",
      "Epoch: 311, step: 20, loss: 3.4496429420652843\n",
      "Epoch: 311, step: 25, loss: 3.4554614562254686\n",
      "Epoch: 311, step: 30, loss: 3.4413934138513382\n",
      "Epoch: 312, step: 0, loss: 3.429682970046997\n",
      "Epoch: 312, step: 5, loss: 3.4283175071080527\n",
      "Epoch: 312, step: 10, loss: 3.4429430094632236\n",
      "Epoch: 312, step: 15, loss: 3.438092902302742\n",
      "Epoch: 312, step: 20, loss: 3.4404752935682024\n",
      "Epoch: 312, step: 25, loss: 3.4441087337640615\n",
      "Epoch: 312, step: 30, loss: 3.4357503096262616\n",
      "Epoch: 313, step: 0, loss: 3.5082075595855713\n",
      "Epoch: 313, step: 5, loss: 3.4375636180241904\n",
      "Epoch: 313, step: 10, loss: 3.428663904016668\n",
      "Epoch: 313, step: 15, loss: 3.4308922439813614\n",
      "Epoch: 313, step: 20, loss: 3.435467517375946\n",
      "Epoch: 313, step: 25, loss: 3.4303336715698243\n",
      "Epoch: 313, step: 30, loss: 3.4401345094045004\n",
      "Epoch: 314, step: 0, loss: 3.3994834423065186\n",
      "Epoch: 314, step: 5, loss: 3.4250771601994834\n",
      "Epoch: 314, step: 10, loss: 3.4381275827234443\n",
      "Epoch: 314, step: 15, loss: 3.4288086146116257\n",
      "Epoch: 314, step: 20, loss: 3.437622376850673\n",
      "Epoch: 314, step: 25, loss: 3.445382650081928\n",
      "Epoch: 314, step: 30, loss: 3.441497810425297\n",
      "Epoch: 315, step: 0, loss: 3.3115572929382324\n",
      "Epoch: 315, step: 5, loss: 3.3786805868148804\n",
      "Epoch: 315, step: 10, loss: 3.410963383587924\n",
      "Epoch: 315, step: 15, loss: 3.422335207462311\n",
      "Epoch: 315, step: 20, loss: 3.446749221710932\n",
      "Epoch: 315, step: 25, loss: 3.4447367649811964\n",
      "Epoch: 315, step: 30, loss: 3.442639212454519\n",
      "Epoch: 316, step: 0, loss: 3.5020813941955566\n",
      "Epoch: 316, step: 5, loss: 3.3854618867238364\n",
      "Epoch: 316, step: 10, loss: 3.393623048608953\n",
      "Epoch: 316, step: 15, loss: 3.4366005063056946\n",
      "Epoch: 316, step: 20, loss: 3.4159603232429143\n",
      "Epoch: 316, step: 25, loss: 3.4421659524624166\n",
      "Epoch: 316, step: 30, loss: 3.4395430934044624\n",
      "Epoch: 317, step: 0, loss: 3.4376885890960693\n",
      "Epoch: 317, step: 5, loss: 3.4405202070871987\n",
      "Epoch: 317, step: 10, loss: 3.429047172719782\n",
      "Epoch: 317, step: 15, loss: 3.4454139918088913\n",
      "Epoch: 317, step: 20, loss: 3.44117800394694\n",
      "Epoch: 317, step: 25, loss: 3.442931046852699\n",
      "Epoch: 317, step: 30, loss: 3.439710432483304\n",
      "Epoch: 318, step: 0, loss: 3.5169694423675537\n",
      "Epoch: 318, step: 5, loss: 3.4301719268163047\n",
      "Epoch: 318, step: 10, loss: 3.4138314723968506\n",
      "Epoch: 318, step: 15, loss: 3.419158026576042\n",
      "Epoch: 318, step: 20, loss: 3.4316130933307467\n",
      "Epoch: 318, step: 25, loss: 3.4288176756638746\n",
      "Epoch: 318, step: 30, loss: 3.4388979865658666\n",
      "Epoch: 319, step: 0, loss: 3.4890849590301514\n",
      "Epoch: 319, step: 5, loss: 3.4803806940714517\n",
      "Epoch: 319, step: 10, loss: 3.448414607481523\n",
      "Epoch: 319, step: 15, loss: 3.4303247332572937\n",
      "Epoch: 319, step: 20, loss: 3.4272885663168773\n",
      "Epoch: 319, step: 25, loss: 3.4350544030849752\n",
      "Epoch: 319, step: 30, loss: 3.4319120299431587\n",
      "Epoch: 320, step: 0, loss: 3.3756463527679443\n",
      "Epoch: 320, step: 5, loss: 3.4382466872533164\n",
      "Epoch: 320, step: 10, loss: 3.450752865184437\n",
      "Epoch: 320, step: 15, loss: 3.4392040371894836\n",
      "Epoch: 320, step: 20, loss: 3.4425015222458613\n",
      "Epoch: 320, step: 25, loss: 3.4393556118011475\n",
      "Epoch: 320, step: 30, loss: 3.4377533082039125\n",
      "Epoch: 321, step: 0, loss: 3.6233479976654053\n",
      "Epoch: 321, step: 5, loss: 3.4628872076670327\n",
      "Epoch: 321, step: 10, loss: 3.4633732492273506\n",
      "Epoch: 321, step: 15, loss: 3.4503699094057083\n",
      "Epoch: 321, step: 20, loss: 3.4435812632242837\n",
      "Epoch: 321, step: 25, loss: 3.440483515079205\n",
      "Epoch: 321, step: 30, loss: 3.43683288943383\n",
      "Epoch: 322, step: 0, loss: 3.3499865531921387\n",
      "Epoch: 322, step: 5, loss: 3.4351467291514077\n",
      "Epoch: 322, step: 10, loss: 3.4112742814150723\n",
      "Epoch: 322, step: 15, loss: 3.4374374747276306\n",
      "Epoch: 322, step: 20, loss: 3.4431853748503185\n",
      "Epoch: 322, step: 25, loss: 3.437767533155588\n",
      "Epoch: 322, step: 30, loss: 3.4382137790802987\n",
      "Epoch: 323, step: 0, loss: 3.3963966369628906\n",
      "Epoch: 323, step: 5, loss: 3.4504427909851074\n",
      "Epoch: 323, step: 10, loss: 3.4485556429082695\n",
      "Epoch: 323, step: 15, loss: 3.4293460845947266\n",
      "Epoch: 323, step: 20, loss: 3.436756451924642\n",
      "Epoch: 323, step: 25, loss: 3.4344124427208533\n",
      "Epoch: 323, step: 30, loss: 3.436348384426486\n",
      "Epoch: 324, step: 0, loss: 3.4765238761901855\n",
      "Epoch: 324, step: 5, loss: 3.463538980484009\n",
      "Epoch: 324, step: 10, loss: 3.467093586921692\n",
      "Epoch: 324, step: 20, loss: 3.4455557245957222\n",
      "Epoch: 324, step: 25, loss: 3.440730760494868\n",
      "Epoch: 324, step: 30, loss: 3.4401057424216437\n",
      "Epoch: 325, step: 0, loss: 3.5148849487304688\n",
      "Epoch: 325, step: 5, loss: 3.4414385159810386\n",
      "Epoch: 325, step: 10, loss: 3.4383406205610796\n",
      "Epoch: 325, step: 15, loss: 3.436286598443985\n",
      "Epoch: 325, step: 20, loss: 3.4363323620387485\n",
      "Epoch: 325, step: 25, loss: 3.4386481871971717\n",
      "Epoch: 325, step: 30, loss: 3.4369574592959498\n",
      "Epoch: 326, step: 0, loss: 3.4696621894836426\n",
      "Epoch: 326, step: 5, loss: 3.3922142585118613\n",
      "Epoch: 326, step: 10, loss: 3.413641994649714\n",
      "Epoch: 326, step: 15, loss: 3.428433135151863\n",
      "Epoch: 326, step: 20, loss: 3.4403060958499\n",
      "Epoch: 326, step: 25, loss: 3.43252341563885\n",
      "Epoch: 326, step: 30, loss: 3.4362087403574297\n",
      "Epoch: 327, step: 0, loss: 3.392611026763916\n",
      "Epoch: 327, step: 5, loss: 3.449545383453369\n",
      "Epoch: 327, step: 10, loss: 3.4250265034762295\n",
      "Epoch: 327, step: 15, loss: 3.4330083578824997\n",
      "Epoch: 327, step: 20, loss: 3.4381625652313232\n",
      "Epoch: 327, step: 25, loss: 3.435068598160377\n",
      "Epoch: 327, step: 30, loss: 3.4383767497154976\n",
      "Epoch: 328, step: 0, loss: 3.3672797679901123\n",
      "Epoch: 328, step: 5, loss: 3.3971898953119912\n",
      "Epoch: 328, step: 10, loss: 3.3969185569069604\n",
      "Epoch: 328, step: 15, loss: 3.410209998488426\n",
      "Epoch: 328, step: 20, loss: 3.413312832514445\n",
      "Epoch: 328, step: 25, loss: 3.4218546335513773\n",
      "Epoch: 328, step: 30, loss: 3.4345953003052743\n",
      "Epoch: 329, step: 0, loss: 3.5288479328155518\n",
      "Epoch: 329, step: 5, loss: 3.4332560300827026\n",
      "Epoch: 329, step: 10, loss: 3.424502871253274\n",
      "Epoch: 329, step: 15, loss: 3.4139883667230606\n",
      "Epoch: 329, step: 20, loss: 3.4143636226654053\n",
      "Epoch: 329, step: 25, loss: 3.4270004272460937\n",
      "Epoch: 329, step: 30, loss: 3.426322857538859\n",
      "Epoch: 330, step: 0, loss: 3.489867925643921\n",
      "Epoch: 330, step: 5, loss: 3.457170565923055\n",
      "Epoch: 330, step: 15, loss: 3.4394325574239093\n",
      "Epoch: 330, step: 20, loss: 3.441235363483429\n",
      "Epoch: 330, step: 25, loss: 3.4382122802734374\n",
      "Epoch: 330, step: 30, loss: 3.4278783400853476\n",
      "Epoch: 331, step: 0, loss: 3.548495054244995\n",
      "Epoch: 331, step: 5, loss: 3.4386444091796875\n",
      "Epoch: 331, step: 10, loss: 3.417506066235629\n",
      "Epoch: 331, step: 15, loss: 3.4331111907958984\n",
      "Epoch: 331, step: 20, loss: 3.4537129402160645\n",
      "Epoch: 331, step: 25, loss: 3.4412332681509166\n",
      "Epoch: 331, step: 30, loss: 3.432582401460217\n",
      "Epoch: 332, step: 0, loss: 3.4208478927612305\n",
      "Epoch: 332, step: 5, loss: 3.429816643397013\n",
      "Epoch: 332, step: 10, loss: 3.4670672199942847\n",
      "Epoch: 332, step: 15, loss: 3.463057115674019\n",
      "Epoch: 332, step: 20, loss: 3.448301973797026\n",
      "Epoch: 332, step: 25, loss: 3.431994548210731\n",
      "Epoch: 332, step: 30, loss: 3.4341971182054087\n",
      "Epoch: 333, step: 0, loss: 3.5844433307647705\n",
      "Epoch: 333, step: 5, loss: 3.4672518571217856\n",
      "Epoch: 333, step: 10, loss: 3.4499323584816675\n",
      "Epoch: 333, step: 15, loss: 3.4449832290410995\n",
      "Epoch: 333, step: 20, loss: 3.446438959666661\n",
      "Epoch: 333, step: 25, loss: 3.4389070730942946\n",
      "Epoch: 333, step: 30, loss: 3.4329664630274617\n",
      "Epoch: 334, step: 0, loss: 3.4559073448181152\n",
      "Epoch: 334, step: 5, loss: 3.436912775039673\n",
      "Epoch: 334, step: 10, loss: 3.4517400047995825\n",
      "Epoch: 334, step: 15, loss: 3.454531490802765\n",
      "Epoch: 334, step: 20, loss: 3.4424718675159274\n",
      "Epoch: 334, step: 25, loss: 3.4370778523958645\n",
      "Epoch: 334, step: 30, loss: 3.4323538541793823\n",
      "Epoch: 335, step: 0, loss: 3.5304102897644043\n",
      "Epoch: 335, step: 5, loss: 3.4484365383783975\n",
      "Epoch: 335, step: 10, loss: 3.438377033580433\n",
      "Epoch: 335, step: 15, loss: 3.441913589835167\n",
      "Epoch: 335, step: 20, loss: 3.4337827137538364\n",
      "Epoch: 335, step: 25, loss: 3.433562911473788\n",
      "Epoch: 335, step: 30, loss: 3.428295204716344\n",
      "Epoch: 336, step: 5, loss: 3.4551878452301024\n",
      "Epoch: 336, step: 10, loss: 3.4573814153671263\n",
      "Epoch: 336, step: 15, loss: 3.4393340746561685\n",
      "Epoch: 336, step: 20, loss: 3.4233319640159605\n",
      "Epoch: 336, step: 25, loss: 3.4182258319854735\n",
      "Epoch: 336, step: 30, loss: 3.4288535197575887\n",
      "Epoch: 337, step: 0, loss: 3.44735050201416\n",
      "Epoch: 337, step: 5, loss: 3.400949756304423\n",
      "Epoch: 337, step: 10, loss: 3.4123227379538794\n",
      "Epoch: 337, step: 15, loss: 3.4159710705280304\n",
      "Epoch: 337, step: 20, loss: 3.4154462927863714\n",
      "Epoch: 337, step: 25, loss: 3.4171794652938843\n",
      "Epoch: 337, step: 30, loss: 3.428134964358422\n",
      "Epoch: 338, step: 0, loss: 3.5560121536254883\n",
      "Epoch: 338, step: 5, loss: 3.4541569550832114\n",
      "Epoch: 338, step: 10, loss: 3.460495710372925\n",
      "Epoch: 338, step: 15, loss: 3.4305915385484695\n",
      "Epoch: 338, step: 20, loss: 3.424806935446603\n",
      "Epoch: 338, step: 25, loss: 3.430039323293246\n",
      "Epoch: 338, step: 30, loss: 3.4319214205588064\n",
      "Epoch: 339, step: 0, loss: 3.329700469970703\n",
      "Epoch: 339, step: 5, loss: 3.428245504697164\n",
      "Epoch: 339, step: 10, loss: 3.4222324544733222\n",
      "Epoch: 339, step: 15, loss: 3.444888800382614\n",
      "Epoch: 339, step: 20, loss: 3.4374574706667946\n",
      "Epoch: 339, step: 25, loss: 3.43728453379411\n",
      "Epoch: 339, step: 30, loss: 3.426640349049722\n",
      "Epoch: 340, step: 0, loss: 3.3883626461029053\n",
      "Epoch: 340, step: 5, loss: 3.4073163270950317\n",
      "Epoch: 340, step: 10, loss: 3.398392581939697\n",
      "Epoch: 340, step: 15, loss: 3.4189589341481526\n",
      "Epoch: 340, step: 20, loss: 3.419766342639923\n",
      "Epoch: 340, step: 25, loss: 3.4108399868011476\n",
      "Epoch: 340, step: 30, loss: 3.4259514729181926\n",
      "Epoch: 341, step: 0, loss: 3.537539005279541\n",
      "Epoch: 341, step: 5, loss: 3.4683725833892822\n",
      "Epoch: 341, step: 10, loss: 3.4498396786776455\n",
      "Epoch: 341, step: 15, loss: 3.4503964632749557\n",
      "Epoch: 341, step: 20, loss: 3.4356075695582797\n",
      "Epoch: 341, step: 25, loss: 3.424830601765559\n",
      "Epoch: 341, step: 30, loss: 3.4252296724627094\n",
      "Epoch: 342, step: 0, loss: 3.4828107357025146\n",
      "Epoch: 342, step: 5, loss: 3.448358416557312\n",
      "Epoch: 342, step: 10, loss: 3.4522421793504194\n",
      "Epoch: 342, step: 15, loss: 3.4322564005851746\n",
      "Epoch: 342, step: 20, loss: 3.439771118618193\n",
      "Epoch: 342, step: 25, loss: 3.427966310427739\n",
      "Epoch: 342, step: 30, loss: 3.4277153399682816\n",
      "Epoch: 343, step: 0, loss: 3.353391408920288\n",
      "Epoch: 343, step: 5, loss: 3.4039272467295327\n",
      "Epoch: 343, step: 10, loss: 3.4147070971402256\n",
      "Epoch: 343, step: 15, loss: 3.418784573674202\n",
      "Epoch: 343, step: 20, loss: 3.416433129991804\n",
      "Epoch: 343, step: 25, loss: 3.4295753974180956\n",
      "Epoch: 343, step: 30, loss: 3.426239029053719\n",
      "Epoch: 344, step: 0, loss: 3.4074344635009766\n",
      "Epoch: 344, step: 5, loss: 3.4979063669840493\n",
      "Epoch: 344, step: 10, loss: 3.4617671316320244\n",
      "Epoch: 344, step: 15, loss: 3.436415582895279\n",
      "Epoch: 344, step: 20, loss: 3.441749232155936\n",
      "Epoch: 344, step: 25, loss: 3.432116435124324\n",
      "Epoch: 344, step: 30, loss: 3.425006881836922\n",
      "Epoch: 345, step: 0, loss: 3.5372183322906494\n",
      "Epoch: 345, step: 5, loss: 3.4365233182907104\n",
      "Epoch: 345, step: 10, loss: 3.4454080841758032\n",
      "Epoch: 345, step: 15, loss: 3.432183548808098\n",
      "Epoch: 345, step: 20, loss: 3.4254931041172574\n",
      "Epoch: 345, step: 25, loss: 3.4219903579125037\n",
      "Epoch: 345, step: 30, loss: 3.428156852722168\n",
      "Epoch: 346, step: 0, loss: 3.4704439640045166\n",
      "Epoch: 346, step: 5, loss: 3.4192468325297036\n",
      "Epoch: 346, step: 10, loss: 3.4214830615303735\n",
      "Epoch: 346, step: 15, loss: 3.419268935918808\n",
      "Epoch: 346, step: 20, loss: 3.4230484508332752\n",
      "Epoch: 346, step: 25, loss: 3.4270900121101966\n",
      "Epoch: 346, step: 30, loss: 3.427355481732276\n",
      "Epoch: 347, step: 0, loss: 3.490994930267334\n",
      "Epoch: 347, step: 5, loss: 3.4653515021006265\n",
      "Epoch: 347, step: 10, loss: 3.4445496038957075\n",
      "Epoch: 347, step: 15, loss: 3.439645465215047\n",
      "Epoch: 347, step: 20, loss: 3.440426242351532\n",
      "Epoch: 347, step: 25, loss: 3.4323747158050537\n",
      "Epoch: 347, step: 30, loss: 3.4238733291625976\n",
      "Epoch: 348, step: 0, loss: 3.408430337905884\n",
      "Epoch: 348, step: 5, loss: 3.4233481884002686\n",
      "Epoch: 348, step: 10, loss: 3.4297638177871703\n",
      "Epoch: 348, step: 15, loss: 3.418361759185791\n",
      "Epoch: 348, step: 20, loss: 3.4296310544013977\n",
      "Epoch: 348, step: 25, loss: 3.431161308288574\n",
      "Epoch: 348, step: 30, loss: 3.427812385559082\n",
      "Epoch: 349, step: 0, loss: 3.3926258087158203\n",
      "Epoch: 349, step: 5, loss: 3.4346598386764526\n",
      "Epoch: 349, step: 10, loss: 3.4245065342296255\n",
      "Epoch: 349, step: 15, loss: 3.416218936443329\n",
      "Epoch: 349, step: 20, loss: 3.419142734436762\n",
      "Epoch: 349, step: 25, loss: 3.421493255175077\n",
      "Epoch: 349, step: 30, loss: 3.428651948128977\n",
      "Epoch: 350, step: 0, loss: 3.38185453414917\n",
      "Epoch: 350, step: 5, loss: 3.402142564455668\n",
      "Epoch: 350, step: 10, loss: 3.423674084923484\n",
      "Epoch: 350, step: 15, loss: 3.4194615930318832\n",
      "Epoch: 350, step: 20, loss: 3.4193189484732494\n",
      "Epoch: 350, step: 25, loss: 3.432895834629352\n",
      "Epoch: 350, step: 30, loss: 3.4265272155884774\n",
      "Epoch: 351, step: 0, loss: 3.3789899349212646\n",
      "Epoch: 351, step: 5, loss: 3.401896874109904\n",
      "Epoch: 351, step: 10, loss: 3.393403400074352\n",
      "Epoch: 351, step: 15, loss: 3.39899879693985\n",
      "Epoch: 351, step: 20, loss: 3.410929384685698\n",
      "Epoch: 351, step: 25, loss: 3.4155581272565403\n",
      "Epoch: 351, step: 30, loss: 3.4233384747659006\n",
      "Epoch: 352, step: 0, loss: 3.2764792442321777\n",
      "Epoch: 352, step: 5, loss: 3.3904590606689453\n",
      "Epoch: 352, step: 10, loss: 3.407506812702526\n",
      "Epoch: 352, step: 15, loss: 3.408690467476845\n",
      "Epoch: 352, step: 20, loss: 3.413458676565261\n",
      "Epoch: 352, step: 25, loss: 3.4188920167776256\n",
      "Epoch: 352, step: 30, loss: 3.4207140476472917\n",
      "Epoch: 353, step: 0, loss: 3.464728832244873\n",
      "Epoch: 353, step: 5, loss: 3.4394890864690146\n",
      "Epoch: 353, step: 10, loss: 3.4046077078038994\n",
      "Epoch: 353, step: 15, loss: 3.4246515184640884\n",
      "Epoch: 353, step: 20, loss: 3.42553626923334\n",
      "Epoch: 353, step: 25, loss: 3.4266062516432543\n",
      "Epoch: 353, step: 30, loss: 3.422768155733744\n",
      "Epoch: 354, step: 0, loss: 3.5261058807373047\n",
      "Epoch: 354, step: 5, loss: 3.4357864459355674\n",
      "Epoch: 354, step: 10, loss: 3.43146614594893\n",
      "Epoch: 354, step: 15, loss: 3.406697317957878\n",
      "Epoch: 354, step: 20, loss: 3.4267508415948775\n",
      "Epoch: 354, step: 25, loss: 3.4286533960929284\n",
      "Epoch: 354, step: 30, loss: 3.42896560699709\n",
      "Epoch: 355, step: 0, loss: 3.3176751136779785\n",
      "Epoch: 355, step: 5, loss: 3.4178088506062827\n",
      "Epoch: 355, step: 10, loss: 3.454792196100408\n",
      "Epoch: 355, step: 15, loss: 3.4252535104751587\n",
      "Epoch: 355, step: 20, loss: 3.425929921013968\n",
      "Epoch: 355, step: 25, loss: 3.427854299545288\n",
      "Epoch: 355, step: 30, loss: 3.4211720728105113\n",
      "Epoch: 356, step: 0, loss: 3.477383613586426\n",
      "Epoch: 356, step: 5, loss: 3.451801578203837\n",
      "Epoch: 356, step: 10, loss: 3.4603965932672676\n",
      "Epoch: 356, step: 15, loss: 3.45614817738533\n",
      "Epoch: 356, step: 20, loss: 3.4387942382267545\n",
      "Epoch: 356, step: 25, loss: 3.42778846850762\n",
      "Epoch: 356, step: 30, loss: 3.427890246914279\n",
      "Epoch: 357, step: 0, loss: 3.371100425720215\n",
      "Epoch: 357, step: 5, loss: 3.383222150802612\n",
      "Epoch: 357, step: 10, loss: 3.412673340903388\n",
      "Epoch: 357, step: 15, loss: 3.4183250835963657\n",
      "Epoch: 357, step: 20, loss: 3.419918097947773\n",
      "Epoch: 357, step: 25, loss: 3.4208375414212546\n",
      "Epoch: 357, step: 30, loss: 3.417766275077031\n",
      "Epoch: 358, step: 0, loss: 3.4061455726623535\n",
      "Epoch: 358, step: 5, loss: 3.370723533630371\n",
      "Epoch: 358, step: 10, loss: 3.409119987487793\n",
      "Epoch: 358, step: 15, loss: 3.392856550216675\n",
      "Epoch: 358, step: 20, loss: 3.4046347618103026\n",
      "Epoch: 358, step: 25, loss: 3.420636730194092\n",
      "Epoch: 358, step: 30, loss: 3.4209266106287637\n",
      "Epoch: 359, step: 0, loss: 3.3830604553222656\n",
      "Epoch: 359, step: 5, loss: 3.427589178085327\n",
      "Epoch: 359, step: 10, loss: 3.440301938490434\n",
      "Epoch: 359, step: 15, loss: 3.428440272808075\n",
      "Epoch: 359, step: 20, loss: 3.426954292115711\n",
      "Epoch: 359, step: 25, loss: 3.4133736078555765\n",
      "Epoch: 359, step: 30, loss: 3.4205130992397184\n",
      "Epoch: 360, step: 0, loss: 3.4921860694885254\n",
      "Epoch: 360, step: 5, loss: 3.390801469484965\n",
      "Epoch: 360, step: 10, loss: 3.398451176556674\n",
      "Epoch: 360, step: 15, loss: 3.404214322566986\n",
      "Epoch: 360, step: 20, loss: 3.421501795450846\n",
      "Epoch: 360, step: 25, loss: 3.4205353810237002\n",
      "Epoch: 360, step: 30, loss: 3.4166166628560712\n",
      "Epoch: 361, step: 0, loss: 3.439044952392578\n",
      "Epoch: 361, step: 5, loss: 3.4521988232930503\n",
      "Epoch: 361, step: 10, loss: 3.4324822425842285\n",
      "Epoch: 361, step: 15, loss: 3.4209703654050827\n",
      "Epoch: 361, step: 20, loss: 3.4271351950509206\n",
      "Epoch: 361, step: 25, loss: 3.422271957397461\n",
      "Epoch: 361, step: 30, loss: 3.4186328172683718\n",
      "Epoch: 362, step: 0, loss: 3.330890655517578\n",
      "Epoch: 362, step: 5, loss: 3.3725966612497964\n",
      "Epoch: 362, step: 10, loss: 3.37953199039806\n",
      "Epoch: 362, step: 15, loss: 3.3992701917886734\n",
      "Epoch: 362, step: 20, loss: 3.412839866819836\n",
      "Epoch: 362, step: 25, loss: 3.423315690113948\n",
      "Epoch: 362, step: 30, loss: 3.419762872880505\n",
      "Epoch: 363, step: 0, loss: 3.516270399093628\n",
      "Epoch: 363, step: 5, loss: 3.412934184074402\n",
      "Epoch: 363, step: 10, loss: 3.411437142979015\n",
      "Epoch: 363, step: 15, loss: 3.4298141598701477\n",
      "Epoch: 363, step: 20, loss: 3.420209668931507\n",
      "Epoch: 363, step: 25, loss: 3.420089602470398\n",
      "Epoch: 363, step: 30, loss: 3.4168991196540093\n",
      "Epoch: 364, step: 0, loss: 3.547980308532715\n",
      "Epoch: 364, step: 5, loss: 3.4547568957010903\n",
      "Epoch: 364, step: 10, loss: 3.4051454067230225\n",
      "Epoch: 364, step: 15, loss: 3.4146469682455063\n",
      "Epoch: 364, step: 20, loss: 3.4103008792513894\n",
      "Epoch: 364, step: 25, loss: 3.4153311802790713\n",
      "Epoch: 364, step: 30, loss: 3.4145576338614188\n",
      "Epoch: 365, step: 0, loss: 3.370756149291992\n",
      "Epoch: 365, step: 5, loss: 3.457639733950297\n",
      "Epoch: 365, step: 10, loss: 3.433860432017933\n",
      "Epoch: 365, step: 15, loss: 3.4253816455602646\n",
      "Epoch: 365, step: 20, loss: 3.4201608271825883\n",
      "Epoch: 365, step: 25, loss: 3.4227551313546987\n",
      "Epoch: 365, step: 30, loss: 3.4166847198240218\n",
      "Epoch: 366, step: 0, loss: 3.3814330101013184\n",
      "Epoch: 366, step: 5, loss: 3.3921162287394204\n",
      "Epoch: 366, step: 10, loss: 3.4212497797879307\n",
      "Epoch: 366, step: 15, loss: 3.4035162925720215\n",
      "Epoch: 366, step: 20, loss: 3.413463002159482\n",
      "Epoch: 366, step: 25, loss: 3.422305107116699\n",
      "Epoch: 366, step: 30, loss: 3.4243771491512174\n",
      "Epoch: 367, step: 0, loss: 3.4536306858062744\n",
      "Epoch: 367, step: 5, loss: 3.4267653624216714\n",
      "Epoch: 367, step: 10, loss: 3.417028773914684\n",
      "Epoch: 367, step: 15, loss: 3.4047426283359528\n",
      "Epoch: 367, step: 20, loss: 3.4127459866659984\n",
      "Epoch: 367, step: 25, loss: 3.4165031359745908\n",
      "Epoch: 367, step: 30, loss: 3.4170126145885837\n",
      "Epoch: 368, step: 0, loss: 3.451845169067383\n",
      "Epoch: 368, step: 5, loss: 3.443907459576925\n",
      "Epoch: 368, step: 10, loss: 3.4480366056615654\n",
      "Epoch: 368, step: 15, loss: 3.4304138720035553\n",
      "Epoch: 368, step: 20, loss: 3.4254342828478133\n",
      "Epoch: 368, step: 25, loss: 3.424233968441303\n",
      "Epoch: 368, step: 30, loss: 3.419632873227519\n",
      "Epoch: 369, step: 0, loss: 3.387580633163452\n",
      "Epoch: 369, step: 5, loss: 3.419046719868978\n",
      "Epoch: 369, step: 10, loss: 3.4096563946117056\n",
      "Epoch: 369, step: 15, loss: 3.4110411256551743\n",
      "Epoch: 369, step: 20, loss: 3.408651431401571\n",
      "Epoch: 369, step: 25, loss: 3.4152988287118764\n",
      "Epoch: 369, step: 30, loss: 3.4178505897521974\n",
      "Epoch: 370, step: 0, loss: 3.457085132598877\n",
      "Epoch: 370, step: 5, loss: 3.3987698952356973\n",
      "Epoch: 370, step: 10, loss: 3.41895790533586\n",
      "Epoch: 370, step: 15, loss: 3.4117236137390137\n",
      "Epoch: 370, step: 20, loss: 3.412695918764387\n",
      "Epoch: 370, step: 25, loss: 3.4087007522583006\n",
      "Epoch: 370, step: 30, loss: 3.4136516650517783\n",
      "Epoch: 371, step: 0, loss: 3.3647356033325195\n",
      "Epoch: 371, step: 5, loss: 3.4204659461975098\n",
      "Epoch: 371, step: 10, loss: 3.4069294062527744\n",
      "Epoch: 371, step: 15, loss: 3.4068145006895065\n",
      "Epoch: 371, step: 20, loss: 3.423324539547875\n",
      "Epoch: 371, step: 25, loss: 3.422451477784377\n",
      "Epoch: 371, step: 30, loss: 3.4195595172143753\n",
      "Epoch: 372, step: 0, loss: 3.344564914703369\n",
      "Epoch: 372, step: 5, loss: 3.4093618392944336\n",
      "Epoch: 372, step: 10, loss: 3.414414102380926\n",
      "Epoch: 372, step: 15, loss: 3.393558621406555\n",
      "Epoch: 372, step: 20, loss: 3.400677646909441\n",
      "Epoch: 372, step: 25, loss: 3.413114520219656\n",
      "Epoch: 372, step: 30, loss: 3.4171216103338424\n",
      "Epoch: 373, step: 0, loss: 3.538191795349121\n",
      "Epoch: 373, step: 5, loss: 3.422673980394999\n",
      "Epoch: 373, step: 10, loss: 3.426959579641169\n",
      "Epoch: 373, step: 15, loss: 3.4125226885080338\n",
      "Epoch: 373, step: 20, loss: 3.409892263866606\n",
      "Epoch: 373, step: 25, loss: 3.4133233565550585\n",
      "Epoch: 373, step: 30, loss: 3.413844362381966\n",
      "Epoch: 374, step: 0, loss: 3.372056007385254\n",
      "Epoch: 374, step: 5, loss: 3.43415101369222\n",
      "Epoch: 374, step: 10, loss: 3.4053396961905738\n",
      "Epoch: 374, step: 15, loss: 3.399927794933319\n",
      "Epoch: 374, step: 20, loss: 3.3959965024675642\n",
      "Epoch: 374, step: 25, loss: 3.407969969969529\n",
      "Epoch: 374, step: 30, loss: 3.415295654727567\n",
      "Epoch: 375, step: 0, loss: 3.4964654445648193\n",
      "Epoch: 375, step: 5, loss: 3.4860697587331138\n",
      "Epoch: 375, step: 10, loss: 3.455252690748735\n",
      "Epoch: 375, step: 15, loss: 3.433729887008667\n",
      "Epoch: 375, step: 20, loss: 3.416175401210785\n",
      "Epoch: 375, step: 25, loss: 3.3961213207244874\n",
      "Epoch: 375, step: 30, loss: 3.413519819577535\n",
      "Epoch: 376, step: 0, loss: 3.538516044616699\n",
      "Epoch: 376, step: 5, loss: 3.470352133115133\n",
      "Epoch: 376, step: 10, loss: 3.471854404969649\n",
      "Epoch: 376, step: 15, loss: 3.4295836091041565\n",
      "Epoch: 376, step: 20, loss: 3.409024408885411\n",
      "Epoch: 376, step: 25, loss: 3.4118345425679135\n",
      "Epoch: 376, step: 30, loss: 3.4177235841751097\n",
      "Epoch: 377, step: 0, loss: 3.279625177383423\n",
      "Epoch: 377, step: 5, loss: 3.355275273323059\n",
      "Epoch: 377, step: 10, loss: 3.3762528029355137\n",
      "Epoch: 377, step: 15, loss: 3.3850823044776917\n",
      "Epoch: 377, step: 20, loss: 3.4053840523674372\n",
      "Epoch: 377, step: 25, loss: 3.4125704306822557\n",
      "Epoch: 377, step: 30, loss: 3.4183452898456204\n",
      "Epoch: 378, step: 0, loss: 3.48235821723938\n",
      "Epoch: 378, step: 5, loss: 3.4333050727844237\n",
      "Epoch: 378, step: 10, loss: 3.4241876363754273\n",
      "Epoch: 378, step: 15, loss: 3.396938689549764\n",
      "Epoch: 378, step: 20, loss: 3.399336111545563\n",
      "Epoch: 378, step: 25, loss: 3.4105892467498777\n",
      "Epoch: 378, step: 30, loss: 3.4104445536931354\n",
      "Epoch: 379, step: 0, loss: 3.4207851886749268\n",
      "Epoch: 379, step: 5, loss: 3.4418641726175943\n",
      "Epoch: 379, step: 10, loss: 3.410806829279119\n",
      "Epoch: 379, step: 15, loss: 3.4244685620069504\n",
      "Epoch: 379, step: 20, loss: 3.422965651466733\n",
      "Epoch: 379, step: 25, loss: 3.436213465837332\n",
      "Epoch: 379, step: 30, loss: 3.4135998218290267\n",
      "Epoch: 380, step: 0, loss: 3.497499465942383\n",
      "Epoch: 380, step: 5, loss: 3.4382368326187134\n",
      "Epoch: 380, step: 10, loss: 3.413260351527821\n",
      "Epoch: 380, step: 15, loss: 3.428907409310341\n",
      "Epoch: 380, step: 20, loss: 3.4131201335362027\n",
      "Epoch: 380, step: 25, loss: 3.4196643370848436\n",
      "Epoch: 380, step: 30, loss: 3.4134685454830045\n",
      "Epoch: 381, step: 0, loss: 3.545459508895874\n",
      "Epoch: 381, step: 5, loss: 3.441204071044922\n",
      "Epoch: 381, step: 10, loss: 3.423445701599121\n",
      "Epoch: 381, step: 15, loss: 3.4251212924718857\n",
      "Epoch: 381, step: 20, loss: 3.4070539474487305\n",
      "Epoch: 381, step: 25, loss: 3.4045985020123997\n",
      "Epoch: 381, step: 30, loss: 3.409676444145941\n",
      "Epoch: 382, step: 0, loss: 3.396350622177124\n",
      "Epoch: 382, step: 5, loss: 3.3972043991088867\n",
      "Epoch: 382, step: 10, loss: 3.383255958557129\n",
      "Epoch: 382, step: 15, loss: 3.4107623994350433\n",
      "Epoch: 382, step: 20, loss: 3.416778882344564\n",
      "Epoch: 382, step: 25, loss: 3.410424379202036\n",
      "Epoch: 382, step: 30, loss: 3.4136784307418333\n",
      "Epoch: 383, step: 0, loss: 3.4828872680664062\n",
      "Epoch: 383, step: 5, loss: 3.4169938961664834\n",
      "Epoch: 383, step: 10, loss: 3.4040434143759986\n",
      "Epoch: 383, step: 15, loss: 3.395650029182434\n",
      "Epoch: 383, step: 20, loss: 3.414301100231352\n",
      "Epoch: 383, step: 30, loss: 3.4083073933919272\n",
      "Epoch: 384, step: 0, loss: 3.3338260650634766\n",
      "Epoch: 384, step: 5, loss: 3.355422019958496\n",
      "Epoch: 384, step: 10, loss: 3.3861781467090952\n",
      "Epoch: 384, step: 15, loss: 3.382372125983238\n",
      "Epoch: 384, step: 20, loss: 3.397091331936064\n",
      "Epoch: 384, step: 25, loss: 3.415621491578909\n",
      "Epoch: 384, step: 30, loss: 3.4074314948051208\n",
      "Epoch: 385, step: 0, loss: 3.379438877105713\n",
      "Epoch: 385, step: 5, loss: 3.4383390744527182\n",
      "Epoch: 385, step: 10, loss: 3.436507961966775\n",
      "Epoch: 385, step: 15, loss: 3.4366483241319656\n",
      "Epoch: 385, step: 20, loss: 3.4359325000217984\n",
      "Epoch: 385, step: 25, loss: 3.423138040762681\n",
      "Epoch: 385, step: 30, loss: 3.4137435497776156\n",
      "Epoch: 386, step: 0, loss: 3.4364075660705566\n",
      "Epoch: 386, step: 5, loss: 3.4040271441141763\n",
      "Epoch: 386, step: 10, loss: 3.394340146671642\n",
      "Epoch: 386, step: 15, loss: 3.40609909594059\n",
      "Epoch: 386, step: 20, loss: 3.402683961959112\n",
      "Epoch: 386, step: 25, loss: 3.410644347851093\n",
      "Epoch: 386, step: 30, loss: 3.4084514494865172\n",
      "Epoch: 387, step: 0, loss: 3.395608425140381\n",
      "Epoch: 387, step: 5, loss: 3.440258264541626\n",
      "Epoch: 387, step: 10, loss: 3.4253460493954746\n",
      "Epoch: 387, step: 15, loss: 3.4163098484277725\n",
      "Epoch: 387, step: 20, loss: 3.4136522043318975\n",
      "Epoch: 387, step: 25, loss: 3.4127059624745297\n",
      "Epoch: 387, step: 30, loss: 3.410923188732516\n",
      "Epoch: 388, step: 0, loss: 3.2768030166625977\n",
      "Epoch: 388, step: 5, loss: 3.3870085080464682\n",
      "Epoch: 388, step: 10, loss: 3.4001783609390257\n",
      "Epoch: 388, step: 15, loss: 3.407112884521484\n",
      "Epoch: 388, step: 20, loss: 3.4156713843345643\n",
      "Epoch: 388, step: 25, loss: 3.4165922355651857\n",
      "Epoch: 388, step: 30, loss: 3.4106222709019978\n",
      "Epoch: 389, step: 0, loss: 3.349778652191162\n",
      "Epoch: 389, step: 5, loss: 3.3677615324656167\n",
      "Epoch: 389, step: 10, loss: 3.4271551262248647\n",
      "Epoch: 389, step: 15, loss: 3.416219100356102\n",
      "Epoch: 389, step: 20, loss: 3.4144153822036016\n",
      "Epoch: 389, step: 25, loss: 3.402054615020752\n",
      "Epoch: 389, step: 30, loss: 3.4093831698099772\n",
      "Epoch: 390, step: 0, loss: 3.619476795196533\n",
      "Epoch: 390, step: 5, loss: 3.443394899368286\n",
      "Epoch: 390, step: 10, loss: 3.4332375526428223\n",
      "Epoch: 390, step: 15, loss: 3.4127776324748993\n",
      "Epoch: 390, step: 20, loss: 3.4042516549428306\n",
      "Epoch: 390, step: 25, loss: 3.4061346420874963\n",
      "Epoch: 390, step: 30, loss: 3.409147101063882\n",
      "Epoch: 391, step: 0, loss: 3.3690497875213623\n",
      "Epoch: 391, step: 5, loss: 3.4213141600290933\n",
      "Epoch: 391, step: 10, loss: 3.4129468527707187\n",
      "Epoch: 391, step: 15, loss: 3.424284443259239\n",
      "Epoch: 391, step: 20, loss: 3.4124923206510998\n",
      "Epoch: 391, step: 25, loss: 3.402025397007282\n",
      "Epoch: 391, step: 30, loss: 3.4055610472156155\n",
      "Epoch: 392, step: 0, loss: 3.3407111167907715\n",
      "Epoch: 392, step: 5, loss: 3.4192911783854165\n",
      "Epoch: 392, step: 10, loss: 3.4255184043537485\n",
      "Epoch: 392, step: 15, loss: 3.408024936914444\n",
      "Epoch: 392, step: 20, loss: 3.407312415895008\n",
      "Epoch: 392, step: 25, loss: 3.4226424418962917\n",
      "Epoch: 392, step: 30, loss: 3.410203203078239\n",
      "Epoch: 393, step: 0, loss: 3.4545416831970215\n",
      "Epoch: 393, step: 5, loss: 3.4225892623265586\n",
      "Epoch: 393, step: 10, loss: 3.4168727181174536\n",
      "Epoch: 393, step: 15, loss: 3.4013005097707114\n",
      "Epoch: 393, step: 20, loss: 3.3970848321914673\n",
      "Epoch: 393, step: 25, loss: 3.411690502166748\n",
      "Epoch: 393, step: 30, loss: 3.409743626912435\n",
      "Epoch: 394, step: 0, loss: 3.4784600734710693\n",
      "Epoch: 394, step: 5, loss: 3.4083614349365234\n",
      "Epoch: 394, step: 10, loss: 3.3968326395208184\n",
      "Epoch: 394, step: 15, loss: 3.4106709212064743\n",
      "Epoch: 394, step: 20, loss: 3.4071946257636663\n",
      "Epoch: 394, step: 25, loss: 3.4086359647604136\n",
      "Epoch: 394, step: 30, loss: 3.4042017459869385\n",
      "Epoch: 395, step: 0, loss: 3.422490119934082\n",
      "Epoch: 395, step: 5, loss: 3.402121734619141\n",
      "Epoch: 395, step: 10, loss: 3.4130927085876466\n",
      "Epoch: 395, step: 15, loss: 3.4078606764475503\n",
      "Epoch: 395, step: 20, loss: 3.4095943570137024\n",
      "Epoch: 395, step: 25, loss: 3.4038358783721923\n",
      "Epoch: 395, step: 30, loss: 3.408572777112325\n",
      "Epoch: 396, step: 0, loss: 3.366483688354492\n",
      "Epoch: 396, step: 5, loss: 3.361915389696757\n",
      "Epoch: 396, step: 10, loss: 3.398055575110696\n",
      "Epoch: 396, step: 15, loss: 3.4109889417886734\n",
      "Epoch: 396, step: 20, loss: 3.4154316697801863\n",
      "Epoch: 396, step: 25, loss: 3.4084813594818115\n",
      "Epoch: 396, step: 30, loss: 3.405737938419465\n",
      "Epoch: 397, step: 0, loss: 3.3642940521240234\n",
      "Epoch: 397, step: 5, loss: 3.396904706954956\n",
      "Epoch: 397, step: 10, loss: 3.4074123339219526\n",
      "Epoch: 397, step: 15, loss: 3.402004688978195\n",
      "Epoch: 397, step: 20, loss: 3.412306070327759\n",
      "Epoch: 397, step: 25, loss: 3.4098750352859497\n",
      "Epoch: 397, step: 30, loss: 3.408686068750197\n",
      "Epoch: 398, step: 0, loss: 3.4315176010131836\n",
      "Epoch: 398, step: 5, loss: 3.377063790957133\n",
      "Epoch: 398, step: 10, loss: 3.402482509613037\n",
      "Epoch: 398, step: 15, loss: 3.4182310849428177\n",
      "Epoch: 398, step: 20, loss: 3.4021766639891124\n",
      "Epoch: 398, step: 25, loss: 3.39947261260106\n",
      "Epoch: 398, step: 30, loss: 3.406151033216907\n",
      "Epoch: 399, step: 0, loss: 3.381746292114258\n",
      "Epoch: 399, step: 5, loss: 3.424272974332174\n",
      "Epoch: 399, step: 10, loss: 3.41633263501254\n",
      "Epoch: 399, step: 15, loss: 3.391924723982811\n",
      "Epoch: 399, step: 20, loss: 3.399178845541818\n",
      "Epoch: 399, step: 25, loss: 3.4006074483578024\n",
      "Epoch: 399, step: 30, loss: 3.4074923607610885\n",
      "Epoch: 400, step: 0, loss: 3.3263723850250244\n",
      "Epoch: 400, step: 5, loss: 3.3864630858103433\n",
      "Epoch: 400, step: 10, loss: 3.399362954226407\n",
      "Epoch: 400, step: 15, loss: 3.403912454843521\n",
      "Epoch: 400, step: 20, loss: 3.398486296335856\n",
      "Epoch: 400, step: 25, loss: 3.4018413378642154\n",
      "Epoch: 400, step: 30, loss: 3.410632533411826\n",
      "Epoch: 401, step: 0, loss: 3.55137300491333\n",
      "Epoch: 401, step: 5, loss: 3.4297698736190796\n",
      "Epoch: 401, step: 10, loss: 3.410121896050193\n",
      "Epoch: 401, step: 15, loss: 3.4043406397104263\n",
      "Epoch: 401, step: 20, loss: 3.4159503891354515\n",
      "Epoch: 401, step: 25, loss: 3.408500918975243\n",
      "Epoch: 401, step: 30, loss: 3.4019455525182907\n",
      "Epoch: 402, step: 0, loss: 3.331111431121826\n",
      "Epoch: 402, step: 5, loss: 3.359232783317566\n",
      "Epoch: 402, step: 10, loss: 3.3626408793709497\n",
      "Epoch: 402, step: 15, loss: 3.3880452513694763\n",
      "Epoch: 402, step: 20, loss: 3.398754369644892\n",
      "Epoch: 402, step: 25, loss: 3.396356564301711\n",
      "Epoch: 402, step: 30, loss: 3.405811348269063\n",
      "Epoch: 403, step: 0, loss: 3.3932125568389893\n",
      "Epoch: 403, step: 5, loss: 3.427388858795166\n",
      "Epoch: 403, step: 10, loss: 3.4237274646759035\n",
      "Epoch: 403, step: 15, loss: 3.4079739093780517\n",
      "Epoch: 403, step: 25, loss: 3.417215794324875\n",
      "Epoch: 403, step: 30, loss: 3.4132389528998015\n",
      "Epoch: 404, step: 0, loss: 3.273794651031494\n",
      "Epoch: 404, step: 5, loss: 3.3796993494033813\n",
      "Epoch: 404, step: 10, loss: 3.3635191917419434\n",
      "Epoch: 404, step: 15, loss: 3.400479644536972\n",
      "Epoch: 404, step: 20, loss: 3.3910367602393743\n",
      "Epoch: 404, step: 25, loss: 3.3970614946805515\n",
      "Epoch: 404, step: 30, loss: 3.405086271224483\n",
      "Epoch: 405, step: 0, loss: 3.182028293609619\n",
      "Epoch: 405, step: 5, loss: 3.3880012035369873\n",
      "Epoch: 405, step: 10, loss: 3.393002820014954\n",
      "Epoch: 405, step: 15, loss: 3.3847606499989826\n",
      "Epoch: 405, step: 20, loss: 3.385155301345022\n",
      "Epoch: 405, step: 25, loss: 3.392064323027929\n",
      "Epoch: 405, step: 30, loss: 3.395901852640612\n",
      "Epoch: 406, step: 0, loss: 3.3956377506256104\n",
      "Epoch: 406, step: 5, loss: 3.352832476298014\n",
      "Epoch: 406, step: 10, loss: 3.3690272894772617\n",
      "Epoch: 406, step: 15, loss: 3.384728044271469\n",
      "Epoch: 406, step: 20, loss: 3.418629351116362\n",
      "Epoch: 406, step: 25, loss: 3.4080227063252377\n",
      "Epoch: 406, step: 30, loss: 3.4039130287785686\n",
      "Epoch: 407, step: 0, loss: 3.575101852416992\n",
      "Epoch: 407, step: 5, loss: 3.367457707722982\n",
      "Epoch: 407, step: 10, loss: 3.41872594573281\n",
      "Epoch: 407, step: 15, loss: 3.4141271263360977\n",
      "Epoch: 407, step: 20, loss: 3.4072744392213368\n",
      "Epoch: 407, step: 25, loss: 3.4104139621441183\n",
      "Epoch: 407, step: 30, loss: 3.4111339430655203\n",
      "Epoch: 408, step: 0, loss: 3.4305741786956787\n",
      "Epoch: 408, step: 5, loss: 3.4423373540242515\n",
      "Epoch: 408, step: 10, loss: 3.4111184857108374\n",
      "Epoch: 408, step: 15, loss: 3.403721332550049\n",
      "Epoch: 408, step: 20, loss: 3.3989348070962087\n",
      "Epoch: 408, step: 25, loss: 3.400393770291255\n",
      "Epoch: 408, step: 30, loss: 3.404316971378942\n",
      "Epoch: 409, step: 0, loss: 3.256714344024658\n",
      "Epoch: 409, step: 5, loss: 3.3558611075083413\n",
      "Epoch: 409, step: 10, loss: 3.396645524285056\n",
      "Epoch: 409, step: 15, loss: 3.4006894379854202\n",
      "Epoch: 409, step: 20, loss: 3.3941689332326255\n",
      "Epoch: 409, step: 25, loss: 3.394323101410499\n",
      "Epoch: 409, step: 30, loss: 3.3977616371647006\n",
      "Epoch: 410, step: 0, loss: 3.3984084129333496\n",
      "Epoch: 410, step: 5, loss: 3.389289458592733\n",
      "Epoch: 410, step: 10, loss: 3.42428457736969\n",
      "Epoch: 410, step: 15, loss: 3.4289217948913575\n",
      "Epoch: 410, step: 20, loss: 3.4170783162117004\n",
      "Epoch: 410, step: 25, loss: 3.406996479034424\n",
      "Epoch: 411, step: 0, loss: 3.3183345794677734\n",
      "Epoch: 411, step: 5, loss: 3.3814919789632163\n",
      "Epoch: 411, step: 10, loss: 3.382205442948775\n",
      "Epoch: 411, step: 15, loss: 3.375428482890129\n",
      "Epoch: 411, step: 20, loss: 3.3813594977060952\n",
      "Epoch: 411, step: 25, loss: 3.393931434704707\n",
      "Epoch: 411, step: 30, loss: 3.4007675570826374\n",
      "Epoch: 412, step: 0, loss: 3.4276156425476074\n",
      "Epoch: 412, step: 5, loss: 3.3988821109135947\n",
      "Epoch: 412, step: 10, loss: 3.405965209007263\n",
      "Epoch: 412, step: 15, loss: 3.4101855436960857\n",
      "Epoch: 412, step: 20, loss: 3.407683253288269\n",
      "Epoch: 412, step: 25, loss: 3.3931164169311523\n",
      "Epoch: 412, step: 30, loss: 3.3997551997502646\n",
      "Epoch: 413, step: 0, loss: 3.5039119720458984\n",
      "Epoch: 413, step: 5, loss: 3.4232227007548013\n",
      "Epoch: 413, step: 10, loss: 3.4176922711459072\n",
      "Epoch: 413, step: 15, loss: 3.416864827275276\n",
      "Epoch: 413, step: 20, loss: 3.4073215325673423\n",
      "Epoch: 413, step: 25, loss: 3.40476709145766\n",
      "Epoch: 413, step: 30, loss: 3.397851597878241\n",
      "Epoch: 414, step: 0, loss: 3.3440144062042236\n",
      "Epoch: 414, step: 5, loss: 3.3705782095591226\n",
      "Epoch: 414, step: 10, loss: 3.373179002241655\n",
      "Epoch: 414, step: 15, loss: 3.3632224053144455\n",
      "Epoch: 414, step: 20, loss: 3.398006166730608\n",
      "Epoch: 414, step: 25, loss: 3.3951805432637534\n",
      "Epoch: 414, step: 30, loss: 3.3923599123954773\n",
      "Epoch: 415, step: 0, loss: 3.3884031772613525\n",
      "Epoch: 415, step: 5, loss: 3.3824647665023804\n",
      "Epoch: 415, step: 10, loss: 3.393460815603083\n",
      "Epoch: 415, step: 15, loss: 3.3693935871124268\n",
      "Epoch: 415, step: 20, loss: 3.387826885495867\n",
      "Epoch: 415, step: 25, loss: 3.3835926422706017\n",
      "Epoch: 415, step: 30, loss: 3.399186095883769\n",
      "Epoch: 416, step: 0, loss: 3.3580286502838135\n",
      "Epoch: 416, step: 5, loss: 3.360945145289103\n",
      "Epoch: 416, step: 10, loss: 3.3849333199587734\n",
      "Epoch: 416, step: 15, loss: 3.382854789495468\n",
      "Epoch: 416, step: 20, loss: 3.395882175082252\n",
      "Epoch: 416, step: 25, loss: 3.3949294548768263\n",
      "Epoch: 416, step: 30, loss: 3.398418165022327\n",
      "Epoch: 417, step: 0, loss: 3.302536725997925\n",
      "Epoch: 417, step: 5, loss: 3.381912112236023\n",
      "Epoch: 417, step: 10, loss: 3.39999812299555\n",
      "Epoch: 417, step: 15, loss: 3.3962459564208984\n",
      "Epoch: 417, step: 20, loss: 3.393648238409133\n",
      "Epoch: 417, step: 25, loss: 3.4051455626120934\n",
      "Epoch: 417, step: 30, loss: 3.399390712861092\n",
      "Epoch: 418, step: 0, loss: 3.507688522338867\n",
      "Epoch: 418, step: 5, loss: 3.432860573132833\n",
      "Epoch: 418, step: 10, loss: 3.418497657775879\n",
      "Epoch: 418, step: 15, loss: 3.4080712954203287\n",
      "Epoch: 418, step: 20, loss: 3.407531976699829\n",
      "Epoch: 418, step: 25, loss: 3.413662452697754\n",
      "Epoch: 418, step: 30, loss: 3.406798020998637\n",
      "Epoch: 419, step: 0, loss: 3.3805694580078125\n",
      "Epoch: 419, step: 5, loss: 3.377870718638102\n",
      "Epoch: 419, step: 10, loss: 3.373646519400857\n",
      "Epoch: 419, step: 15, loss: 3.395695149898529\n",
      "Epoch: 419, step: 20, loss: 3.398157630647932\n",
      "Epoch: 419, step: 25, loss: 3.398068070411682\n",
      "Epoch: 419, step: 30, loss: 3.39538045852415\n",
      "Epoch: 420, step: 0, loss: 3.4346070289611816\n",
      "Epoch: 420, step: 5, loss: 3.392683744430542\n",
      "Epoch: 420, step: 10, loss: 3.3909521102905273\n",
      "Epoch: 420, step: 15, loss: 3.395148366689682\n",
      "Epoch: 420, step: 20, loss: 3.4068992478506908\n",
      "Epoch: 420, step: 25, loss: 3.4033358005376964\n",
      "Epoch: 420, step: 30, loss: 3.3964510502353793\n",
      "Epoch: 421, step: 0, loss: 3.3104782104492188\n",
      "Epoch: 421, step: 5, loss: 3.382723053296407\n",
      "Epoch: 421, step: 10, loss: 3.3869967027144\n",
      "Epoch: 421, step: 15, loss: 3.3909339904785156\n",
      "Epoch: 421, step: 20, loss: 3.3931889874594554\n",
      "Epoch: 421, step: 25, loss: 3.4043821830015917\n",
      "Epoch: 421, step: 30, loss: 3.4029019109664427\n",
      "Epoch: 422, step: 0, loss: 3.4460396766662598\n",
      "Epoch: 422, step: 5, loss: 3.432734568913778\n",
      "Epoch: 422, step: 10, loss: 3.3897842060435903\n",
      "Epoch: 422, step: 15, loss: 3.4008720368146896\n",
      "Epoch: 422, step: 20, loss: 3.400428601673671\n",
      "Epoch: 422, step: 25, loss: 3.3986649696643534\n",
      "Epoch: 422, step: 30, loss: 3.3977728120742308\n",
      "Epoch: 423, step: 0, loss: 3.4248528480529785\n",
      "Epoch: 423, step: 5, loss: 3.4126646518707275\n",
      "Epoch: 423, step: 10, loss: 3.4160972725261343\n",
      "Epoch: 423, step: 15, loss: 3.4129962027072906\n",
      "Epoch: 423, step: 20, loss: 3.4084455285753523\n",
      "Epoch: 423, step: 25, loss: 3.401512650343088\n",
      "Epoch: 423, step: 30, loss: 3.3979971024297897\n",
      "Epoch: 424, step: 0, loss: 3.3442440032958984\n",
      "Epoch: 424, step: 5, loss: 3.339354236920675\n",
      "Epoch: 424, step: 10, loss: 3.368291984904896\n",
      "Epoch: 424, step: 15, loss: 3.3839875906705856\n",
      "Epoch: 424, step: 20, loss: 3.3904887040456138\n",
      "Epoch: 424, step: 25, loss: 3.392132584865277\n",
      "Epoch: 424, step: 30, loss: 3.395422116915385\n",
      "Epoch: 425, step: 0, loss: 3.2661726474761963\n",
      "Epoch: 425, step: 5, loss: 3.3449326356252036\n",
      "Epoch: 425, step: 10, loss: 3.3818727406588467\n",
      "Epoch: 425, step: 15, loss: 3.394930437207222\n",
      "Epoch: 425, step: 20, loss: 3.3950527849651517\n",
      "Epoch: 425, step: 25, loss: 3.3894311556449304\n",
      "Epoch: 425, step: 30, loss: 3.3948640131181285\n",
      "Epoch: 426, step: 0, loss: 3.479170083999634\n",
      "Epoch: 426, step: 5, loss: 3.3652371168136597\n",
      "Epoch: 426, step: 10, loss: 3.380109656940807\n",
      "Epoch: 426, step: 15, loss: 3.405946671962738\n",
      "Epoch: 426, step: 20, loss: 3.39765765553429\n",
      "Epoch: 426, step: 25, loss: 3.3973938685197096\n",
      "Epoch: 426, step: 30, loss: 3.4011693716049196\n",
      "Epoch: 427, step: 0, loss: 3.4675512313842773\n",
      "Epoch: 427, step: 5, loss: 3.3884329795837402\n",
      "Epoch: 427, step: 10, loss: 3.3916878050023858\n",
      "Epoch: 427, step: 15, loss: 3.3954312950372696\n",
      "Epoch: 427, step: 20, loss: 3.398278168269566\n",
      "Epoch: 427, step: 25, loss: 3.3898106263234067\n",
      "Epoch: 427, step: 30, loss: 3.3939125153326217\n",
      "Epoch: 428, step: 0, loss: 3.478029489517212\n",
      "Epoch: 428, step: 5, loss: 3.434646964073181\n",
      "Epoch: 428, step: 10, loss: 3.4180660247802734\n",
      "Epoch: 428, step: 15, loss: 3.4011075496673584\n",
      "Epoch: 428, step: 20, loss: 3.3954888820648192\n",
      "Epoch: 428, step: 25, loss: 3.404575757980347\n",
      "Epoch: 428, step: 30, loss: 3.3941503604253134\n",
      "Epoch: 429, step: 0, loss: 3.523709297180176\n",
      "Epoch: 429, step: 5, loss: 3.417486071586609\n",
      "Epoch: 429, step: 10, loss: 3.4086941372264516\n",
      "Epoch: 429, step: 15, loss: 3.4102223366498947\n",
      "Epoch: 429, step: 20, loss: 3.407458816255842\n",
      "Epoch: 429, step: 25, loss: 3.3922176636182346\n",
      "Epoch: 429, step: 30, loss: 3.3921036412639003\n",
      "Epoch: 430, step: 0, loss: 3.4134507179260254\n",
      "Epoch: 430, step: 5, loss: 3.404440681139628\n",
      "Epoch: 430, step: 10, loss: 3.3906478014859287\n",
      "Epoch: 430, step: 15, loss: 3.3942898213863373\n",
      "Epoch: 430, step: 20, loss: 3.39502443586077\n",
      "Epoch: 430, step: 25, loss: 3.3985852736693163\n",
      "Epoch: 430, step: 30, loss: 3.3985278760233233\n",
      "Epoch: 431, step: 0, loss: 3.2827277183532715\n",
      "Epoch: 431, step: 5, loss: 3.368687073389689\n",
      "Epoch: 431, step: 10, loss: 3.3626402724872935\n",
      "Epoch: 431, step: 15, loss: 3.3623422384262085\n",
      "Epoch: 431, step: 20, loss: 3.386525960195632\n",
      "Epoch: 431, step: 25, loss: 3.3990278335718007\n",
      "Epoch: 431, step: 30, loss: 3.3928130211368686\n",
      "Epoch: 432, step: 0, loss: 3.3346667289733887\n",
      "Epoch: 432, step: 5, loss: 3.4109603563944497\n",
      "Epoch: 432, step: 10, loss: 3.408815860748291\n",
      "Epoch: 432, step: 15, loss: 3.398914948105812\n",
      "Epoch: 432, step: 20, loss: 3.406977278845651\n",
      "Epoch: 432, step: 25, loss: 3.410205061619098\n",
      "Epoch: 432, step: 30, loss: 3.397891721417827\n",
      "Epoch: 433, step: 0, loss: 3.2659950256347656\n",
      "Epoch: 433, step: 5, loss: 3.3243221282958983\n",
      "Epoch: 433, step: 10, loss: 3.338512659072876\n",
      "Epoch: 433, step: 15, loss: 3.35845890045166\n",
      "Epoch: 433, step: 20, loss: 3.377304434776306\n",
      "Epoch: 433, step: 25, loss: 3.3734358501434327\n",
      "Epoch: 433, step: 30, loss: 3.3896460697568696\n",
      "Epoch: 434, step: 0, loss: 3.4430015087127686\n",
      "Epoch: 434, step: 5, loss: 3.3991334040959678\n",
      "Epoch: 434, step: 10, loss: 3.4096993533047764\n",
      "Epoch: 434, step: 15, loss: 3.402470037341118\n",
      "Epoch: 434, step: 20, loss: 3.3874202569325766\n",
      "Epoch: 434, step: 25, loss: 3.3936274968660793\n",
      "Epoch: 434, step: 30, loss: 3.3921826039591143\n",
      "Epoch: 435, step: 0, loss: 3.4392144680023193\n",
      "Epoch: 435, step: 5, loss: 3.374421795209249\n",
      "Epoch: 435, step: 10, loss: 3.3822671500119297\n",
      "Epoch: 435, step: 15, loss: 3.393373504281044\n",
      "Epoch: 435, step: 20, loss: 3.387766372589838\n",
      "Epoch: 435, step: 25, loss: 3.3945134786459117\n",
      "Epoch: 435, step: 30, loss: 3.3912203235010945\n",
      "Epoch: 436, step: 0, loss: 3.4360756874084473\n",
      "Epoch: 436, step: 5, loss: 3.3936264912287393\n",
      "Epoch: 436, step: 10, loss: 3.3998642401261763\n",
      "Epoch: 436, step: 15, loss: 3.408228799700737\n",
      "Epoch: 436, step: 20, loss: 3.405610368365333\n",
      "Epoch: 436, step: 25, loss: 3.3957229485878577\n",
      "Epoch: 436, step: 30, loss: 3.389020712144913\n",
      "Epoch: 437, step: 0, loss: 3.4591147899627686\n",
      "Epoch: 437, step: 5, loss: 3.355159640312195\n",
      "Epoch: 437, step: 10, loss: 3.3766551451249556\n",
      "Epoch: 437, step: 15, loss: 3.384551167488098\n",
      "Epoch: 437, step: 20, loss: 3.3994704541705905\n",
      "Epoch: 437, step: 25, loss: 3.3949932868664083\n",
      "Epoch: 437, step: 30, loss: 3.393921390656502\n",
      "Epoch: 438, step: 0, loss: 3.4821648597717285\n",
      "Epoch: 438, step: 5, loss: 3.378225485483805\n",
      "Epoch: 438, step: 10, loss: 3.404659726402976\n",
      "Epoch: 438, step: 15, loss: 3.398161068558693\n",
      "Epoch: 438, step: 20, loss: 3.386904727844965\n",
      "Epoch: 438, step: 25, loss: 3.389966240295997\n",
      "Epoch: 438, step: 30, loss: 3.3889645068876204\n",
      "Epoch: 439, step: 0, loss: 3.3755970001220703\n",
      "Epoch: 439, step: 5, loss: 3.3931272824605307\n",
      "Epoch: 439, step: 10, loss: 3.3975837880914863\n",
      "Epoch: 439, step: 15, loss: 3.3998384177684784\n",
      "Epoch: 439, step: 20, loss: 3.4008847872416177\n",
      "Epoch: 439, step: 25, loss: 3.398996664927556\n",
      "Epoch: 439, step: 30, loss: 3.3899795624517624\n",
      "Epoch: 440, step: 0, loss: 3.3763065338134766\n",
      "Epoch: 440, step: 5, loss: 3.3269511063893638\n",
      "Epoch: 440, step: 10, loss: 3.360665440559387\n",
      "Epoch: 440, step: 15, loss: 3.3546960512797037\n",
      "Epoch: 440, step: 20, loss: 3.362659549713135\n",
      "Epoch: 440, step: 25, loss: 3.3847070789337157\n",
      "Epoch: 440, step: 30, loss: 3.390668177604675\n",
      "Epoch: 441, step: 0, loss: 3.420424699783325\n",
      "Epoch: 441, step: 5, loss: 3.381555676460266\n",
      "Epoch: 441, step: 10, loss: 3.4140277342362837\n",
      "Epoch: 441, step: 15, loss: 3.397312045097351\n",
      "Epoch: 441, step: 20, loss: 3.4003645465487526\n",
      "Epoch: 441, step: 25, loss: 3.3949636771128726\n",
      "Epoch: 441, step: 30, loss: 3.3894405518808672\n",
      "Epoch: 442, step: 0, loss: 3.428041458129883\n",
      "Epoch: 442, step: 5, loss: 3.3903099298477173\n",
      "Epoch: 442, step: 10, loss: 3.3989467404105445\n",
      "Epoch: 442, step: 15, loss: 3.4131543040275574\n",
      "Epoch: 442, step: 20, loss: 3.4035124892280217\n",
      "Epoch: 442, step: 25, loss: 3.3919893411489634\n",
      "Epoch: 442, step: 30, loss: 3.386902678397394\n",
      "Epoch: 443, step: 0, loss: 3.4063563346862793\n",
      "Epoch: 443, step: 5, loss: 3.4054625829060874\n",
      "Epoch: 443, step: 10, loss: 3.426474766297774\n",
      "Epoch: 443, step: 15, loss: 3.3946266025304794\n",
      "Epoch: 443, step: 20, loss: 3.395548763729277\n",
      "Epoch: 443, step: 25, loss: 3.404554082797124\n",
      "Epoch: 443, step: 30, loss: 3.386958691381639\n",
      "Epoch: 444, step: 0, loss: 3.395052433013916\n",
      "Epoch: 444, step: 5, loss: 3.3695265452067056\n",
      "Epoch: 444, step: 10, loss: 3.3662004470825195\n",
      "Epoch: 444, step: 15, loss: 3.378294453024864\n",
      "Epoch: 444, step: 20, loss: 3.3752884978339788\n",
      "Epoch: 444, step: 25, loss: 3.383137959700364\n",
      "Epoch: 444, step: 30, loss: 3.3868345983566774\n",
      "Epoch: 445, step: 0, loss: 3.3043808937072754\n",
      "Epoch: 445, step: 5, loss: 3.361203193664551\n",
      "Epoch: 445, step: 10, loss: 3.4013748602433638\n",
      "Epoch: 445, step: 15, loss: 3.395301192998886\n",
      "Epoch: 445, step: 20, loss: 3.3873608452933177\n",
      "Epoch: 445, step: 25, loss: 3.390011640695425\n",
      "Epoch: 445, step: 30, loss: 3.3879624489815003\n",
      "Epoch: 446, step: 0, loss: 3.4266960620880127\n",
      "Epoch: 446, step: 5, loss: 3.4254666805267333\n",
      "Epoch: 446, step: 10, loss: 3.4295215129852297\n",
      "Epoch: 446, step: 15, loss: 3.416216580073039\n",
      "Epoch: 446, step: 20, loss: 3.4092565298080446\n",
      "Epoch: 446, step: 25, loss: 3.3980465730031333\n",
      "Epoch: 446, step: 30, loss: 3.3868353942344926\n",
      "Epoch: 447, step: 0, loss: 3.4596128463745117\n",
      "Epoch: 447, step: 5, loss: 3.414983073870341\n",
      "Epoch: 447, step: 10, loss: 3.399317567998713\n",
      "Epoch: 447, step: 15, loss: 3.397136405110359\n",
      "Epoch: 447, step: 20, loss: 3.4081606524331227\n",
      "Epoch: 447, step: 25, loss: 3.393366446861854\n",
      "Epoch: 447, step: 30, loss: 3.390295228650493\n",
      "Epoch: 448, step: 0, loss: 3.3536033630371094\n",
      "Epoch: 448, step: 5, loss: 3.3156394163767495\n",
      "Epoch: 448, step: 10, loss: 3.360036394812844\n",
      "Epoch: 448, step: 15, loss: 3.3569007515907288\n",
      "Epoch: 448, step: 20, loss: 3.375125692004249\n",
      "Epoch: 448, step: 25, loss: 3.37540079997136\n",
      "Epoch: 448, step: 30, loss: 3.390073291717037\n",
      "Epoch: 449, step: 0, loss: 3.3340096473693848\n",
      "Epoch: 449, step: 5, loss: 3.393252452214559\n",
      "Epoch: 449, step: 10, loss: 3.402363582090898\n",
      "Epoch: 449, step: 15, loss: 3.415384218096733\n",
      "Epoch: 449, step: 25, loss: 3.3972799491882326\n",
      "Epoch: 449, step: 30, loss: 3.381737812360128\n",
      "Epoch: 450, step: 5, loss: 3.3077963829040526\n",
      "Epoch: 450, step: 10, loss: 3.3668689489364625\n",
      "Epoch: 450, step: 15, loss: 3.3885412216186523\n",
      "Epoch: 450, step: 20, loss: 3.3909817337989807\n",
      "Epoch: 450, step: 25, loss: 3.396325263977051\n",
      "Epoch: 450, step: 30, loss: 3.3853317817052204\n",
      "Epoch: 451, step: 0, loss: 3.5141520500183105\n",
      "Epoch: 451, step: 5, loss: 3.4183733065923056\n",
      "Epoch: 451, step: 10, loss: 3.403596054423939\n",
      "Epoch: 451, step: 15, loss: 3.399971157312393\n",
      "Epoch: 451, step: 20, loss: 3.3886110214960006\n",
      "Epoch: 451, step: 25, loss: 3.3892295268865733\n",
      "Epoch: 451, step: 30, loss: 3.392023709512526\n",
      "Epoch: 452, step: 0, loss: 3.5230798721313477\n",
      "Epoch: 452, step: 5, loss: 3.466442108154297\n",
      "Epoch: 452, step: 10, loss: 3.4468155123970727\n",
      "Epoch: 452, step: 15, loss: 3.3989475816488266\n",
      "Epoch: 452, step: 20, loss: 3.3944802511306036\n",
      "Epoch: 452, step: 25, loss: 3.3877905882321873\n",
      "Epoch: 452, step: 30, loss: 3.388523993953582\n",
      "Epoch: 453, step: 0, loss: 3.4656200408935547\n",
      "Epoch: 453, step: 5, loss: 3.4111651182174683\n",
      "Epoch: 453, step: 10, loss: 3.3801659020510586\n",
      "Epoch: 453, step: 15, loss: 3.37707856297493\n",
      "Epoch: 453, step: 20, loss: 3.3836121332077753\n",
      "Epoch: 453, step: 25, loss: 3.384523657652048\n",
      "Epoch: 453, step: 30, loss: 3.3876153961304696\n",
      "Epoch: 454, step: 0, loss: 3.2348361015319824\n",
      "Epoch: 454, step: 5, loss: 3.3974236647288003\n",
      "Epoch: 454, step: 10, loss: 3.426488746296276\n",
      "Epoch: 454, step: 15, loss: 3.408563807606697\n",
      "Epoch: 454, step: 20, loss: 3.400806574594407\n",
      "Epoch: 454, step: 25, loss: 3.396314162474412\n",
      "Epoch: 454, step: 30, loss: 3.390702247619629\n",
      "Epoch: 455, step: 0, loss: 3.341768264770508\n",
      "Epoch: 455, step: 5, loss: 3.3657729228337607\n",
      "Epoch: 455, step: 10, loss: 3.3452417850494385\n",
      "Epoch: 455, step: 15, loss: 3.372974157333374\n",
      "Epoch: 455, step: 20, loss: 3.3741294088817777\n",
      "Epoch: 455, step: 25, loss: 3.381088458574735\n",
      "Epoch: 455, step: 30, loss: 3.3874055493262505\n",
      "Epoch: 456, step: 0, loss: 3.31594181060791\n",
      "Epoch: 456, step: 5, loss: 3.3732899030049643\n",
      "Epoch: 456, step: 10, loss: 3.382692402059382\n",
      "Epoch: 456, step: 15, loss: 3.3801804929971695\n",
      "Epoch: 456, step: 20, loss: 3.3821689514886764\n",
      "Epoch: 456, step: 25, loss: 3.3708802369924693\n",
      "Epoch: 456, step: 30, loss: 3.383850412984048\n",
      "Epoch: 457, step: 0, loss: 3.505335807800293\n",
      "Epoch: 457, step: 5, loss: 3.4428749481836953\n",
      "Epoch: 457, step: 10, loss: 3.424081390554255\n",
      "Epoch: 457, step: 15, loss: 3.404281720519066\n",
      "Epoch: 457, step: 20, loss: 3.3857518831888833\n",
      "Epoch: 457, step: 25, loss: 3.38956448665032\n",
      "Epoch: 457, step: 30, loss: 3.383949864295221\n",
      "Epoch: 458, step: 0, loss: 3.262087345123291\n",
      "Epoch: 458, step: 5, loss: 3.3660382827123008\n",
      "Epoch: 458, step: 10, loss: 3.387711698358709\n",
      "Epoch: 458, step: 15, loss: 3.3901847451925278\n",
      "Epoch: 458, step: 20, loss: 3.3801214808509465\n",
      "Epoch: 458, step: 25, loss: 3.3867484697928796\n",
      "Epoch: 458, step: 30, loss: 3.3883015109646704\n",
      "Epoch: 459, step: 0, loss: 3.289902448654175\n",
      "Epoch: 459, step: 5, loss: 3.3892719745635986\n",
      "Epoch: 459, step: 10, loss: 3.355035521767356\n",
      "Epoch: 459, step: 15, loss: 3.3643148690462112\n",
      "Epoch: 459, step: 20, loss: 3.3632629485357377\n",
      "Epoch: 459, step: 25, loss: 3.3745635197712827\n",
      "Epoch: 459, step: 30, loss: 3.380860874729772\n",
      "Epoch: 460, step: 0, loss: 3.4475209712982178\n",
      "Epoch: 460, step: 5, loss: 3.376120924949646\n",
      "Epoch: 460, step: 15, loss: 3.384845813115438\n",
      "Epoch: 460, step: 20, loss: 3.3744524121284485\n",
      "Epoch: 460, step: 25, loss: 3.3637432157993317\n",
      "Epoch: 460, step: 30, loss: 3.3808731703922668\n",
      "Epoch: 461, step: 0, loss: 3.3271734714508057\n",
      "Epoch: 461, step: 5, loss: 3.371446371078491\n",
      "Epoch: 461, step: 10, loss: 3.379135326905684\n",
      "Epoch: 461, step: 15, loss: 3.376808151602745\n",
      "Epoch: 461, step: 20, loss: 3.3779822758265903\n",
      "Epoch: 461, step: 25, loss: 3.375471729498643\n",
      "Epoch: 461, step: 30, loss: 3.386324913271012\n",
      "Epoch: 462, step: 0, loss: 3.433789014816284\n",
      "Epoch: 462, step: 5, loss: 3.421985149383545\n",
      "Epoch: 462, step: 10, loss: 3.4311799786307593\n",
      "Epoch: 462, step: 15, loss: 3.404893860220909\n",
      "Epoch: 462, step: 20, loss: 3.380586476553054\n",
      "Epoch: 462, step: 25, loss: 3.384861936936012\n",
      "Epoch: 462, step: 30, loss: 3.3840391789713213\n",
      "Epoch: 463, step: 0, loss: 3.5091638565063477\n",
      "Epoch: 463, step: 5, loss: 3.3658155600229898\n",
      "Epoch: 463, step: 10, loss: 3.381586855108088\n",
      "Epoch: 463, step: 15, loss: 3.3814698855082193\n",
      "Epoch: 463, step: 20, loss: 3.3750723719596865\n",
      "Epoch: 463, step: 25, loss: 3.389808292388916\n",
      "Epoch: 463, step: 30, loss: 3.386772457758586\n",
      "Epoch: 464, step: 0, loss: 3.223409652709961\n",
      "Epoch: 464, step: 5, loss: 3.3909960190455117\n",
      "Epoch: 464, step: 10, loss: 3.381068056279963\n",
      "Epoch: 464, step: 15, loss: 3.373554676771164\n",
      "Epoch: 464, step: 20, loss: 3.3754724888574508\n",
      "Epoch: 464, step: 25, loss: 3.3757205192859354\n",
      "Epoch: 464, step: 30, loss: 3.3804272990072928\n",
      "Epoch: 465, step: 0, loss: 3.401407241821289\n",
      "Epoch: 465, step: 5, loss: 3.42850653330485\n",
      "Epoch: 465, step: 10, loss: 3.394870497963645\n",
      "Epoch: 465, step: 15, loss: 3.389181450009346\n",
      "Epoch: 465, step: 20, loss: 3.38101433572315\n",
      "Epoch: 465, step: 25, loss: 3.3855946350097654\n",
      "Epoch: 465, step: 30, loss: 3.3832024176915487\n",
      "Epoch: 466, step: 0, loss: 3.436453342437744\n",
      "Epoch: 466, step: 5, loss: 3.4238260984420776\n",
      "Epoch: 466, step: 10, loss: 3.3820792761715976\n",
      "Epoch: 466, step: 15, loss: 3.390167474746704\n",
      "Epoch: 466, step: 20, loss: 3.388227485475086\n",
      "Epoch: 466, step: 25, loss: 3.375326844362112\n",
      "Epoch: 466, step: 30, loss: 3.380098658223306\n",
      "Epoch: 467, step: 0, loss: 3.4640791416168213\n",
      "Epoch: 467, step: 5, loss: 3.4289843241373696\n",
      "Epoch: 467, step: 10, loss: 3.372338229959661\n",
      "Epoch: 467, step: 15, loss: 3.373963013291359\n",
      "Epoch: 467, step: 20, loss: 3.3842078277042935\n",
      "Epoch: 467, step: 25, loss: 3.37994554409614\n",
      "Epoch: 467, step: 30, loss: 3.377361036116077\n",
      "Epoch: 468, step: 0, loss: 3.3318209648132324\n",
      "Epoch: 468, step: 10, loss: 3.3804568529129027\n",
      "Epoch: 468, step: 15, loss: 3.3786216894785563\n",
      "Epoch: 468, step: 20, loss: 3.3831711530685427\n",
      "Epoch: 468, step: 25, loss: 3.387319164276123\n",
      "Epoch: 468, step: 30, loss: 3.3823218901952106\n",
      "Epoch: 469, step: 0, loss: 3.366889476776123\n",
      "Epoch: 469, step: 5, loss: 3.429172476132711\n",
      "Epoch: 469, step: 10, loss: 3.4324154420332476\n",
      "Epoch: 469, step: 15, loss: 3.412570759654045\n",
      "Epoch: 469, step: 20, loss: 3.393728460584368\n",
      "Epoch: 469, step: 25, loss: 3.390719248698308\n",
      "Epoch: 469, step: 30, loss: 3.3843434933693177\n",
      "Epoch: 470, step: 0, loss: 3.3690600395202637\n",
      "Epoch: 470, step: 5, loss: 3.4314318100611367\n",
      "Epoch: 470, step: 10, loss: 3.4097635746002197\n",
      "Epoch: 470, step: 15, loss: 3.4191885888576508\n",
      "Epoch: 470, step: 20, loss: 3.394978943325224\n",
      "Epoch: 470, step: 25, loss: 3.3848139689518857\n",
      "Epoch: 470, step: 30, loss: 3.3816057558982604\n",
      "Epoch: 471, step: 0, loss: 3.40885066986084\n",
      "Epoch: 471, step: 5, loss: 3.3471980492273965\n",
      "Epoch: 471, step: 10, loss: 3.372962301427668\n",
      "Epoch: 471, step: 15, loss: 3.387225568294525\n",
      "Epoch: 471, step: 20, loss: 3.39344961302621\n",
      "Epoch: 471, step: 25, loss: 3.387616680218623\n",
      "Epoch: 471, step: 30, loss: 3.3803885829064155\n",
      "Epoch: 472, step: 0, loss: 3.404480457305908\n",
      "Epoch: 472, step: 5, loss: 3.3970483938852944\n",
      "Epoch: 472, step: 10, loss: 3.3911560448733242\n",
      "Epoch: 472, step: 15, loss: 3.376606836915016\n",
      "Epoch: 472, step: 20, loss: 3.375516562234788\n",
      "Epoch: 472, step: 25, loss: 3.3780877590179443\n",
      "Epoch: 472, step: 30, loss: 3.377226052745696\n",
      "Epoch: 473, step: 0, loss: 3.2592854499816895\n",
      "Epoch: 473, step: 5, loss: 3.3535420099894204\n",
      "Epoch: 473, step: 10, loss: 3.3889372132041236\n",
      "Epoch: 473, step: 15, loss: 3.3836030662059784\n",
      "Epoch: 473, step: 20, loss: 3.377886102313087\n",
      "Epoch: 473, step: 25, loss: 3.374771549151494\n",
      "Epoch: 473, step: 30, loss: 3.3774034515503915\n",
      "Epoch: 474, step: 0, loss: 3.338073968887329\n",
      "Epoch: 474, step: 5, loss: 3.3621532917022705\n",
      "Epoch: 474, step: 10, loss: 3.355023969303478\n",
      "Epoch: 474, step: 15, loss: 3.3820987045764923\n",
      "Epoch: 474, step: 20, loss: 3.381526163646153\n",
      "Epoch: 474, step: 25, loss: 3.3798775672912598\n",
      "Epoch: 474, step: 30, loss: 3.387617739041646\n",
      "Epoch: 475, step: 0, loss: 3.292239189147949\n",
      "Epoch: 475, step: 5, loss: 3.3939316272735596\n",
      "Epoch: 475, step: 10, loss: 3.36649827523665\n",
      "Epoch: 475, step: 15, loss: 3.3764829337596893\n",
      "Epoch: 475, step: 20, loss: 3.3876449380602156\n",
      "Epoch: 475, step: 25, loss: 3.3758673667907715\n",
      "Epoch: 475, step: 30, loss: 3.3800651181128716\n",
      "Epoch: 476, step: 0, loss: 3.3056588172912598\n",
      "Epoch: 476, step: 5, loss: 3.3296707471211753\n",
      "Epoch: 476, step: 10, loss: 3.336605527184226\n",
      "Epoch: 476, step: 15, loss: 3.3648338317871094\n",
      "Epoch: 476, step: 20, loss: 3.3748828229450045\n",
      "Epoch: 476, step: 25, loss: 3.379795927267808\n",
      "Epoch: 476, step: 30, loss: 3.3770108540852863\n",
      "Epoch: 477, step: 0, loss: 3.3604419231414795\n",
      "Epoch: 477, step: 5, loss: 3.379326025644938\n",
      "Epoch: 477, step: 10, loss: 3.379892739382657\n",
      "Epoch: 477, step: 15, loss: 3.387472227215767\n",
      "Epoch: 477, step: 20, loss: 3.3840514591761996\n",
      "Epoch: 477, step: 25, loss: 3.3793228956369252\n",
      "Epoch: 477, step: 30, loss: 3.377077933280699\n",
      "Epoch: 478, step: 0, loss: 3.293245792388916\n",
      "Epoch: 478, step: 5, loss: 3.3518980344136557\n",
      "Epoch: 478, step: 10, loss: 3.376761024648493\n",
      "Epoch: 478, step: 15, loss: 3.3841462582349777\n",
      "Epoch: 478, step: 20, loss: 3.382824943179176\n",
      "Epoch: 478, step: 25, loss: 3.3828794222611647\n",
      "Epoch: 478, step: 30, loss: 3.3779631122466056\n",
      "Epoch: 479, step: 0, loss: 3.2623000144958496\n",
      "Epoch: 479, step: 5, loss: 3.3527622620264688\n",
      "Epoch: 479, step: 10, loss: 3.380336891521107\n",
      "Epoch: 479, step: 15, loss: 3.385482966899872\n",
      "Epoch: 479, step: 20, loss: 3.3817231882186163\n",
      "Epoch: 479, step: 25, loss: 3.3867777035786557\n",
      "Epoch: 479, step: 30, loss: 3.3831699355956046\n",
      "Epoch: 480, step: 0, loss: 3.3968029022216797\n",
      "Epoch: 480, step: 5, loss: 3.375138521194458\n",
      "Epoch: 480, step: 10, loss: 3.3717286586761475\n",
      "Epoch: 480, step: 15, loss: 3.382910206913948\n",
      "Epoch: 480, step: 20, loss: 3.3815104365348816\n",
      "Epoch: 480, step: 25, loss: 3.381137447357178\n",
      "Epoch: 480, step: 30, loss: 3.3795467615127563\n",
      "Epoch: 481, step: 0, loss: 3.280785322189331\n",
      "Epoch: 481, step: 5, loss: 3.4058106342951455\n",
      "Epoch: 481, step: 10, loss: 3.374489090659402\n",
      "Epoch: 481, step: 15, loss: 3.3845558166503906\n",
      "Epoch: 481, step: 20, loss: 3.388778800056094\n",
      "Epoch: 481, step: 25, loss: 3.3820283321233897\n",
      "Epoch: 481, step: 30, loss: 3.383232332045032\n",
      "Epoch: 482, step: 0, loss: 3.399641513824463\n",
      "Epoch: 482, step: 5, loss: 3.3915963570276895\n",
      "Epoch: 482, step: 10, loss: 3.3353270183910024\n",
      "Epoch: 482, step: 15, loss: 3.3542955070734024\n",
      "Epoch: 482, step: 20, loss: 3.3769154775710333\n",
      "Epoch: 482, step: 25, loss: 3.3686366172937245\n",
      "Epoch: 482, step: 30, loss: 3.3731488796972458\n",
      "Epoch: 483, step: 0, loss: 3.344186782836914\n",
      "Epoch: 483, step: 5, loss: 3.3899054527282715\n",
      "Epoch: 483, step: 10, loss: 3.388215736909346\n",
      "Epoch: 483, step: 15, loss: 3.3913282305002213\n",
      "Epoch: 483, step: 20, loss: 3.3864338397979736\n",
      "Epoch: 483, step: 25, loss: 3.3763145208358765\n",
      "Epoch: 483, step: 30, loss: 3.3779270956593175\n",
      "Epoch: 484, step: 0, loss: 3.3114829063415527\n",
      "Epoch: 484, step: 5, loss: 3.375637173652649\n",
      "Epoch: 484, step: 10, loss: 3.365300568667325\n",
      "Epoch: 484, step: 15, loss: 3.3741898387670517\n",
      "Epoch: 484, step: 20, loss: 3.380186364764259\n",
      "Epoch: 484, step: 25, loss: 3.3750366339316735\n",
      "Epoch: 484, step: 30, loss: 3.3774536348158315\n",
      "Epoch: 485, step: 0, loss: 3.3122267723083496\n",
      "Epoch: 485, step: 5, loss: 3.3608331282933555\n",
      "Epoch: 485, step: 10, loss: 3.3837766647338867\n",
      "Epoch: 485, step: 15, loss: 3.388188436627388\n",
      "Epoch: 485, step: 20, loss: 3.3807758944375172\n",
      "Epoch: 485, step: 25, loss: 3.372225816433246\n",
      "Epoch: 485, step: 30, loss: 3.37431925342929\n",
      "Epoch: 486, step: 0, loss: 3.312074661254883\n",
      "Epoch: 486, step: 5, loss: 3.3919410705566406\n",
      "Epoch: 486, step: 10, loss: 3.3805001865733755\n",
      "Epoch: 486, step: 15, loss: 3.365685537457466\n",
      "Epoch: 486, step: 20, loss: 3.3708365190596807\n",
      "Epoch: 486, step: 25, loss: 3.3815827186291036\n",
      "Epoch: 486, step: 30, loss: 3.374382180552329\n",
      "Epoch: 487, step: 0, loss: 3.308274269104004\n",
      "Epoch: 487, step: 5, loss: 3.394293030103048\n",
      "Epoch: 487, step: 10, loss: 3.3718335845253686\n",
      "Epoch: 487, step: 15, loss: 3.38703915476799\n",
      "Epoch: 487, step: 20, loss: 3.3910490104130337\n",
      "Epoch: 487, step: 25, loss: 3.3787910296366763\n",
      "Epoch: 487, step: 30, loss: 3.3729362641611407\n",
      "Epoch: 488, step: 0, loss: 3.348909378051758\n",
      "Epoch: 488, step: 5, loss: 3.4050912062327066\n",
      "Epoch: 488, step: 10, loss: 3.400400096719915\n",
      "Epoch: 488, step: 15, loss: 3.3873724788427353\n",
      "Epoch: 488, step: 20, loss: 3.3758339200701033\n",
      "Epoch: 488, step: 25, loss: 3.366409971163823\n",
      "Epoch: 488, step: 30, loss: 3.374182170437228\n",
      "Epoch: 489, step: 0, loss: 3.379507064819336\n",
      "Epoch: 489, step: 5, loss: 3.386579672495524\n",
      "Epoch: 489, step: 10, loss: 3.399203372001648\n",
      "Epoch: 489, step: 15, loss: 3.381836748123169\n",
      "Epoch: 489, step: 20, loss: 3.376360464096069\n",
      "Epoch: 489, step: 25, loss: 3.3675440406799315\n",
      "Epoch: 489, step: 30, loss: 3.368002351125081\n",
      "Epoch: 490, step: 0, loss: 3.2398464679718018\n",
      "Epoch: 490, step: 5, loss: 3.36680014928182\n",
      "Epoch: 490, step: 10, loss: 3.3563345779072153\n",
      "Epoch: 490, step: 15, loss: 3.3740383684635162\n",
      "Epoch: 490, step: 20, loss: 3.3648262364523753\n",
      "Epoch: 490, step: 25, loss: 3.3745036033483653\n",
      "Epoch: 490, step: 30, loss: 3.3771204179333103\n",
      "Epoch: 491, step: 0, loss: 3.5133588314056396\n",
      "Epoch: 491, step: 5, loss: 3.362363417943319\n",
      "Epoch: 491, step: 10, loss: 3.3759629509665747\n",
      "Epoch: 491, step: 15, loss: 3.3895452320575714\n",
      "Epoch: 491, step: 20, loss: 3.383135046277727\n",
      "Epoch: 491, step: 25, loss: 3.3802599906921387\n",
      "Epoch: 491, step: 30, loss: 3.3742973343018563\n",
      "Epoch: 492, step: 0, loss: 3.3376502990722656\n",
      "Epoch: 492, step: 5, loss: 3.3443268537521362\n",
      "Epoch: 492, step: 10, loss: 3.3693772012537178\n",
      "Epoch: 492, step: 15, loss: 3.3708760291337967\n",
      "Epoch: 492, step: 20, loss: 3.3733461697896323\n",
      "Epoch: 492, step: 25, loss: 3.3757427747433004\n",
      "Epoch: 492, step: 30, loss: 3.3755001867971113\n",
      "Epoch: 493, step: 0, loss: 3.4812045097351074\n",
      "Epoch: 493, step: 5, loss: 3.3805749813715615\n",
      "Epoch: 493, step: 10, loss: 3.387723771008578\n",
      "Epoch: 493, step: 15, loss: 3.3963624835014343\n",
      "Epoch: 493, step: 20, loss: 3.3883050282796225\n",
      "Epoch: 493, step: 25, loss: 3.383856470768268\n",
      "Epoch: 493, step: 30, loss: 3.3737350356194282\n",
      "Epoch: 494, step: 0, loss: 3.421090602874756\n",
      "Epoch: 494, step: 5, loss: 3.3989526430765786\n",
      "Epoch: 494, step: 10, loss: 3.377907861362804\n",
      "Epoch: 494, step: 15, loss: 3.372872893015544\n",
      "Epoch: 494, step: 20, loss: 3.367894542217255\n",
      "Epoch: 494, step: 25, loss: 3.37244215965271\n",
      "Epoch: 494, step: 30, loss: 3.370112172762553\n",
      "Epoch: 495, step: 0, loss: 3.3324742317199707\n",
      "Epoch: 495, step: 5, loss: 3.356425484021505\n",
      "Epoch: 495, step: 10, loss: 3.3674029220234263\n",
      "Epoch: 495, step: 15, loss: 3.364116996526718\n",
      "Epoch: 495, step: 20, loss: 3.373920508793422\n",
      "Epoch: 495, step: 30, loss: 3.3755844751993815\n",
      "Epoch: 496, step: 0, loss: 3.261967897415161\n",
      "Epoch: 496, step: 5, loss: 3.3500617345174155\n",
      "Epoch: 496, step: 10, loss: 3.363423715938221\n",
      "Epoch: 496, step: 15, loss: 3.3641426265239716\n",
      "Epoch: 496, step: 20, loss: 3.3681122916085378\n",
      "Epoch: 496, step: 25, loss: 3.3733648428550134\n",
      "Epoch: 496, step: 30, loss: 3.3719673464375157\n",
      "Epoch: 497, step: 0, loss: 3.4730472564697266\n",
      "Epoch: 497, step: 5, loss: 3.41938050587972\n",
      "Epoch: 497, step: 10, loss: 3.3964469649574975\n",
      "Epoch: 497, step: 15, loss: 3.393010064959526\n",
      "Epoch: 497, step: 20, loss: 3.3758680593399775\n",
      "Epoch: 497, step: 25, loss: 3.3767703496492825\n",
      "Epoch: 497, step: 30, loss: 3.3707654245438112\n",
      "Epoch: 498, step: 0, loss: 3.409299850463867\n",
      "Epoch: 498, step: 5, loss: 3.315390189488729\n",
      "Epoch: 498, step: 10, loss: 3.347058404575695\n",
      "Epoch: 498, step: 15, loss: 3.3917101472616196\n",
      "Epoch: 498, step: 20, loss: 3.392393044063023\n",
      "Epoch: 498, step: 25, loss: 3.386580613943247\n",
      "Epoch: 498, step: 30, loss: 3.371707393277076\n",
      "Epoch: 499, step: 0, loss: 3.420760154724121\n",
      "Epoch: 499, step: 5, loss: 3.3435637950897217\n",
      "Epoch: 499, step: 10, loss: 3.3501575860110195\n",
      "Epoch: 499, step: 15, loss: 3.368739068508148\n",
      "Epoch: 499, step: 20, loss: 3.371023723057338\n",
      "Epoch: 499, step: 25, loss: 3.36021233521975\n",
      "Epoch: 499, step: 30, loss: 3.369387557429652\n",
      "Epoch: 500, step: 0, loss: 3.3623297214508057\n",
      "Epoch: 500, step: 5, loss: 3.3529401222864785\n",
      "Epoch: 500, step: 10, loss: 3.3415934172543613\n",
      "Epoch: 500, step: 15, loss: 3.3678662478923798\n",
      "Epoch: 500, step: 20, loss: 3.3614717438107444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 50\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, loss_vals \u001b[38;5;129;01min\u001b[39;00m losses\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_losses\u001b[38;5;241m.\u001b[39msetdefault(k, [])\u001b[38;5;241m.\u001b[39mextend(loss_vals)\n",
      "Cell \u001b[0;32mIn[40], line 30\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, epoch, loader)\u001b[0m\n\u001b[1;32m     27\u001b[0m losses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m10.\u001b[39m)\n",
      "File \u001b[0;32m/env_dl/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/env_dl/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(val_loader))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 64, 64])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out, z_list, log_p, log_det = model(img.cuda())\n",
    "    z = z_list[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.2008, 1.0382, 1.1664, 1.0596, 1.0973, 1.1453, 1.0821, 1.0738, 1.0757,\n",
       "         1.0963, 1.0168, 1.0781, 1.0468, 1.0969, 1.0911, 1.1341],\n",
       "        device='cuda:0'),\n",
       " tensor([-0.0200, -0.0210,  0.0226, -0.0412, -0.0024,  0.0133, -0.0577, -0.0219,\n",
       "         -0.0239, -0.0068, -0.0283, -0.0183, -0.0336,  0.0168,  0.0112, -0.0207],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std_mean(z, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = model.glow.reverse(z)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = x + u => p(v) = p(x + u)\n",
    "# z = log(v) - log(1-v)\n",
    "# det(z/v) = 1 / v(1-v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
