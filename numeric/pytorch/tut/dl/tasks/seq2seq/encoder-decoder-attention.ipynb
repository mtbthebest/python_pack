{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/env_nlp/lib/python3.9/site-packages/fairseq']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairseq.__path__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fairseq.tasks' from '/env_nlp/lib/python3.9/site-packages/fairseq/tasks/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairseq.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multilingual_masked_lm': fairseq.tasks.multilingual_masked_lm.MultiLingualMaskedLMTask,\n",
       " 'translation': fairseq.tasks.translation.TranslationTask,\n",
       " 'translation_lev': fairseq.tasks.translation_lev.TranslationLevenshteinTask,\n",
       " 'translation_multi_simple_epoch': fairseq.tasks.translation_multi_simple_epoch.TranslationMultiSimpleEpochTask,\n",
       " 'speech_unit_modeling': fairseq.tasks.speech_ulm_task.SpeechUnitLanguageModelingTask,\n",
       " 'hubert_pretraining': fairseq.tasks.hubert_pretraining.HubertPretrainingTask,\n",
       " 'multilingual_translation': fairseq.tasks.multilingual_translation.MultilingualTranslationTask,\n",
       " 'language_modeling': fairseq.tasks.language_modeling.LanguageModelingTask,\n",
       " 'masked_lm': fairseq.tasks.masked_lm.MaskedLMTask,\n",
       " 'audio_pretraining': fairseq.tasks.audio_pretraining.AudioPretrainingTask,\n",
       " 'audio_finetuning': fairseq.tasks.audio_finetuning.AudioFinetuningTask,\n",
       " 'multilingual_language_modeling': fairseq.tasks.multilingual_language_modeling.MultilingualLanguageModelingTask,\n",
       " 'speech_to_text': fairseq.tasks.speech_to_text.SpeechToTextTask,\n",
       " 'simul_speech_to_text': fairseq.tasks.simultaneous_translation.SimulSpeechToTextTask,\n",
       " 'simul_text_to_text': fairseq.tasks.simultaneous_translation.SimulTextToTextTask,\n",
       " 'legacy_masked_lm': fairseq.tasks.legacy_masked_lm.LegacyMaskedLMTask,\n",
       " 'sentence_prediction': fairseq.tasks.sentence_prediction.SentencePredictionTask,\n",
       " 'translation_from_pretrained_xlm': fairseq.tasks.translation_from_pretrained_xlm.TranslationFromPretrainedXLMTask,\n",
       " 'text_to_speech': fairseq.tasks.text_to_speech.TextToSpeechTask,\n",
       " 'translation_from_pretrained_bart': fairseq.tasks.translation_from_pretrained_bart.TranslationFromPretrainedBARTTask,\n",
       " 'denoising': fairseq.tasks.denoising.DenoisingTask,\n",
       " 'multilingual_denoising': fairseq.tasks.multilingual_denoising.MultilingualDenoisingTask,\n",
       " 'frm_text_to_speech': fairseq.tasks.frm_text_to_speech.FrmTextToSpeechTask,\n",
       " 'sentence_prediction_adapters': fairseq.tasks.sentence_prediction_adapters.SentencePredictionAdapterTask,\n",
       " 'online_backtranslation': fairseq.tasks.online_backtranslation.OnlineBackTranslationTask,\n",
       " 'cross_lingual_lm': fairseq.tasks.cross_lingual_lm.CrossLingualLMTask,\n",
       " 'sentence_ranking': fairseq.tasks.sentence_ranking.SentenceRankingTask,\n",
       " 'semisupervised_translation': fairseq.tasks.semisupervised_translation.SemisupervisedTranslationTask,\n",
       " 'speech_to_speech': fairseq.tasks.speech_to_speech.SpeechToSpeechTask,\n",
       " 'dummy_lm': fairseq.benchmark.dummy_lm.DummyLMTask,\n",
       " 'dummy_masked_lm': fairseq.benchmark.dummy_masked_lm.DummyMaskedLMTask,\n",
       " 'dummy_mt': fairseq.benchmark.dummy_mt.DummyMTTask}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairseq.tasks.TASK_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16044,  8239, 34933,  ...,  2374, 13973, 36703],\n",
       "        [23757, 31943, 33848,  ..., 22762, 16629, 23216],\n",
       "        [ 8606,   763, 25616,  ...,  8786, 32391, 13916],\n",
       "        [ 7937, 25449, 14018,  ...,  3122, 12978, 14438]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "VOCAB_SIZE = 40000\n",
    "SRC_SIZE = 256\n",
    "BATCH_SIZE = 4\n",
    "PAD_IDX = 0\n",
    "src = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SRC_SIZE))\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16044,  8239, 34933,  ...,     0,     0,     0],\n",
       "        [23757, 31943, 33848,  ...,     0,     0,     0],\n",
       "        [ 8606,   763, 25616,  ...,     0,     0,     0],\n",
       "        [ 7937, 25449, 14018,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.randint(8, SRC_SIZE, ( BATCH_SIZE, 1))\n",
    "cond = torch.arange(SRC_SIZE, dtype=mask.dtype)\n",
    "src[cond > mask] = 0\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16044,  8239, 34933, 13760,  8963,  8379, 15427, 38503, 23497, 25683,\n",
       "        14101, 26866, 22756, 21399, 15878, 20376, 20056,  9868, 28794, 36033,\n",
       "        33126, 38119, 26391,  6254, 12824, 37841,  7269, 10969, 17549, 25480,\n",
       "        17481, 37012, 37363, 25360, 34995, 30085, 37822,  1999, 33711, 36288,\n",
       "        13778, 11043, 39896,  5489, 38771, 33757, 27283,  5395, 24782, 13671,\n",
       "        39840, 35969, 17673,   441,  2111, 29280,  1503, 29706, 39476, 18327,\n",
       "        17699, 11326, 28363, 39474, 30775, 36700,  5218, 37632, 21568,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([68])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21568,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[0][mask[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX, UNK_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 68],\n",
       "        [187],\n",
       "        [ 70],\n",
       "        [232]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21568,     0,     0,     0],\n",
       "        [ 5210, 29448,  3756,     0],\n",
       "        [18816,     0, 22200,     0],\n",
       "        [ 1261, 20180, 32506, 35495]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[ ..., mask.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1261, 20180, 32506, 35495])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[ -1, mask.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[21568],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0]],\n",
       "\n",
       "        [[ 5210],\n",
       "         [29448],\n",
       "         [ 3756],\n",
       "         [    0]],\n",
       "\n",
       "        [[18816],\n",
       "         [    0],\n",
       "         [22200],\n",
       "         [    0]],\n",
       "\n",
       "        [[ 1261],\n",
       "         [20180],\n",
       "         [32506],\n",
       "         [35495]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[..., mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[21568],\n",
       "         [    0],\n",
       "         [    0],\n",
       "         [    0]],\n",
       "\n",
       "        [[ 5210],\n",
       "         [29448],\n",
       "         [ 3756],\n",
       "         [    0]],\n",
       "\n",
       "        [[18816],\n",
       "         [    0],\n",
       "         [22200],\n",
       "         [    0]],\n",
       "\n",
       "        [[ 1261],\n",
       "         [20180],\n",
       "         [32506],\n",
       "         [35495]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21568, 29448, 22200, 35495])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[ torch.arange(BATCH_SIZE), mask.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_input(batch_size=BATCH_SIZE):\n",
    "    src = torch.randint(0, VOCAB_SIZE, (batch_size, SRC_SIZE))\n",
    "    mask_idx = torch.randint(5, SRC_SIZE, (batch_size, 1))\n",
    "    mask = torch.arange(SRC_SIZE, dtype=mask_idx.dtype) > mask_idx\n",
    "    src[mask] = PAD_IDX\n",
    "    src[torch.arange(batch_size), mask_idx.flatten()] = EOS_IDX \n",
    "    src[torch.arange(batch_size), [0] * batch_size] = BOS_IDX \n",
    "    return src\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_src = get_dummy_input()\n",
    "inp_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2, 15972, 15186, 33687,  7189, 11198, 33410, 26120, 11018, 17925,\n",
       "        11110, 23014, 10660, 39489, 39925, 27707, 15524,  9861,  7877,  7718,\n",
       "        39587, 14424, 38577, 16335, 31572, 27186,  5577, 23159, 21380, 15793,\n",
       "        23256,  7198, 12418,    83, 26115, 27085,  4939, 38541,  3492, 17425,\n",
       "        26522, 23430, 18799,  7458, 36629,  6228, 20074,  5946, 10105, 27729,\n",
       "        33061, 15769,  5364, 38293,  6839,  8292, 24068, 16047, 35618, 30603,\n",
       "        16354, 21665, 28910, 13011, 10032,  2379, 22497, 14553, 17121,  4575,\n",
       "        26733, 34502, 15577, 24152, 14701,  9446, 26291, 22800, 29143, 13978,\n",
       "        35651, 36058,  2862,  6714,   669, 26882, 33605, 33209, 16135, 24079,\n",
       "        26510,   547,  4596, 19943, 22198, 38599, 14579,  5990, 11129, 26483,\n",
       "        10397,  4215, 13105, 18763, 32515,  5837, 12026,  4727, 38680,   635,\n",
       "        12220, 33208, 36484, 21749, 19059,  2134, 30329, 15965, 32806, 10379,\n",
       "         2318, 35858,  7641, 10860, 10499,  2917, 37127, 16797, 16846, 24075,\n",
       "        29769, 21259, 10768, 28540, 38203, 26151, 21247,  3963, 17601,  2002,\n",
       "         6261, 39452, 24128, 15701, 34328, 29001, 18888, 37522, 13003, 27996,\n",
       "        28149, 39283, 34641, 12215, 29327,  8387, 13418, 19992, 24405,  5020,\n",
       "        30266,  8725,  4814,  4271, 30081, 36876, 21131, 11417, 18023, 37387,\n",
       "        30618, 14893, 13282, 30465, 27262, 28880,  9921, 39760, 19105, 30049,\n",
       "         4396, 26918, 16296,   136, 16205,  9706, 10771,  7154,  7392, 10903,\n",
       "        34116,  3807, 24534,  7808, 10277, 21663, 31759, 11603, 19791, 37811,\n",
       "        20089, 26188, 35117, 34186,  2676, 12393, 39760, 22534, 36629, 11806,\n",
       "        25027,  8336, 13312, 16833, 38066, 16480, 23979, 36155,  6994, 13778,\n",
       "        35961, 23894, 13327, 19855,  5468, 36743, 39131, 30168, 18873, 23930,\n",
       "        21464, 33676, 39053, 29714,  2510,     3,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_src[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,  7725, 16233, 14146, 20513, 24204, 22223,  1718, 39329, 34007,\n",
       "         4556,  5061, 39462,  8961, 25738, 33286, 19453, 28166, 33446, 39014,\n",
       "        21963, 28594,  4437, 24382, 39986, 31346, 38579, 13954, 16140,  1053,\n",
       "        29001, 25267, 24392, 21381, 22964, 18421, 22059, 36993, 27691, 39631,\n",
       "        29182,  5964, 21615, 30156, 38485, 17912,  1938, 23257, 33933, 31326,\n",
       "         7068, 12908, 10217, 38229,  8454,  8292, 25008, 22568, 17370, 22046,\n",
       "        27013, 35983, 23295,  6565,  9253,  6459,  3185,  7711,  5216, 28528,\n",
       "         8208, 23584, 32638, 37450, 15180, 35101,  8576,   989, 18458, 14375,\n",
       "           86, 18387, 31237,  9252, 19603, 22975, 31780, 15173, 30236, 13452,\n",
       "        27546, 30193, 38163, 11980, 12646, 31879, 39604,  2576, 11229, 30656,\n",
       "         9239,  1935, 14410, 28284, 12803, 24701,  9347, 12368, 22064,  7817,\n",
       "        38484, 22260, 37093, 38591,  4088, 30846,    56, 25264,   721, 18869,\n",
       "        21753, 15113, 14380,  1790,  3279, 20198,  9550, 38689,  9412, 29136,\n",
       "        15972, 11847, 24638, 27543, 38107,  4078, 38407, 22205, 28182, 36066,\n",
       "         6430,  9398,  9552, 17115, 27375,  4015, 18802,  7051, 10327,  3470,\n",
       "         3367, 31326, 17242,  2722, 28470,  9425, 39011, 34247, 19693, 15355,\n",
       "        27184, 17573, 15236,     3,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_src[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 2048\n",
    "EMBED_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Embedding(voc_size, embed_dim):\n",
    "    embedding = nn.Embedding(num_embeddings=voc_size, embedding_dim=embed_dim,\n",
    "                             padding_idx=PAD_IDX, )\n",
    "    nn.init.uniform_(embedding.weight, -0.1, 0.1)\n",
    "    nn.init.constant_(embedding.weight[PAD_IDX], 0.)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_net = Embedding(VOCAB_SIZE, embed_dim=EMBED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0256,  0.0326,  0.0972,  ..., -0.0654, -0.0206, -0.0127],\n",
       "        [-0.0246, -0.0414,  0.0920,  ...,  0.0560,  0.0799, -0.0465],\n",
       "        ...,\n",
       "        [ 0.0165,  0.0210,  0.0956,  ..., -0.0821,  0.0432, -0.0001],\n",
       "        [-0.0118, -0.0252, -0.0988,  ...,  0.0664, -0.0533,  0.0143],\n",
       "        [ 0.0847,  0.0929,  0.0016,  ..., -0.0736, -0.0177, -0.0841]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_net.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40000, 1024])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_net.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = embedding_net(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 1024])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size, seq_size, hidden_size\n",
    "em.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_net.weight[1].data.copy_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
       "          1.0000e+00,  1.0000e+00],\n",
       "        [-2.4572e-02, -4.1433e-02,  9.2018e-02,  ...,  5.6014e-02,\n",
       "          7.9891e-02, -4.6524e-02],\n",
       "        ...,\n",
       "        [ 1.6473e-02,  2.0970e-02,  9.5622e-02,  ..., -8.2123e-02,\n",
       "          4.3167e-02, -1.1914e-04],\n",
       "        [-1.1751e-02, -2.5235e-02, -9.8790e-02,  ...,  6.6410e-02,\n",
       "         -5.3273e-02,  1.4340e-02],\n",
       "        [ 8.4664e-02,  9.2933e-02,  1.5503e-03,  ..., -7.3641e-02,\n",
       "         -1.7708e-02, -8.4070e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_net.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([40000, 1024]), torch.Size([4, 256]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_net.weight.shape, src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 1024])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_net.weight.index_select(0, src.flatten()).view(BATCH_SIZE, SRC_SIZE, EMBED_SIZE).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16044,  8239, 34933,  ...,     0,     0,     0],\n",
       "        [23757, 31943, 33848,  ...,     0,     0,     0],\n",
       "        [ 8606,   763, 25616,  ...,     0,     0,     0],\n",
       "        [ 7937, 25449, 14018,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embed = Embedding(VOCAB_SIZE, EMBED_SIZE)\n",
    "decoder_embed = Embedding(VOCAB_SIZE, EMBED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_embed.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREEZE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FREEZE:\n",
    "    encoder_embed.weight.requires_grad = False\n",
    "    decoder_embed.weight.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(embed_size, hidden_size, num_layers, bidirectional=False, dropout=0., batch_first=True, bias=True):\n",
    "    lstm = nn.LSTM(input_size=embed_size,\n",
    "                   hidden_size=hidden_size,\n",
    "                   num_layers=num_layers,\n",
    "                   bidirectional=bidirectional,\n",
    "                   dropout=dropout,\n",
    "                   batch_first=batch_first,\n",
    "                   bias=bias\n",
    "                   )\n",
    "    for name, param in lstm.named_parameters():\n",
    "        if \"weight\" in name or \"bias\" in name:\n",
    "            nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "            \n",
    "    return lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(1024, 2048, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0839, -0.0312,  0.0732,  ..., -0.0152, -0.0167,  0.0414],\n",
       "        [-0.0888, -0.0119,  0.0562,  ...,  0.0124,  0.0246, -0.0816],\n",
       "        [-0.0145, -0.0381, -0.0945,  ...,  0.0341,  0.0032,  0.0858],\n",
       "        ...,\n",
       "        [-0.0844,  0.0543, -0.0329,  ..., -0.0752,  0.0890,  0.0523],\n",
       "        [-0.0571,  0.0174, -0.0177,  ..., -0.0368,  0.0885,  0.0769],\n",
       "        [-0.0156,  0.0214,  0.0489,  ..., -0.0976,  0.0896, -0.0419]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.weight_hh_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 2048]), torch.Size([8192, 2048]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.weight_hh_l0.shape, lstm.weight_hh_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 1024]), torch.Size([8192, 2048]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.weight_ih_l0.shape, lstm.weight_ih_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192]), torch.Size([8192]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.bias_ih_l0.shape, lstm.bias_ih_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 1024]), torch.Size([8192, 2048]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.weight_ih_l0.shape, lstm.weight_ih_l1.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 2048])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm(embedding_net(src))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, (hn, cn) = lstm(embedding_net(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 2048])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2048])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 2048])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(1024, 2048, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 256, 2048]),\n",
       " torch.Size([4, 4, 2048]),\n",
       " torch.Size([4, 4, 2048]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, (hn, cn) = lstm(embedding_net(src))\n",
    "out.shape, hn.shape, cn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 2048]), torch.Size([8192, 2048]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.weight_ih_l3.shape, lstm.weight_ih_l3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(1024, 2048, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2048])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0 = torch.randn(2048)\n",
    "h0 = h0.expand(3, 4, -1) #   dim 0 -> num_layers * bidirectional, dim 1 -> batch_size,\n",
    "h0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2048])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c0 = torch.randn(2048)\n",
    "c0 = c0.expand( 3, 4, -1) \n",
    "c0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, (hn, cn) = lstm(embedding_net(src), (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 2048])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 2048]), torch.Size([3, 4, 2048]), torch.Size([3, 4, 2048]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape, hn.shape, cn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional\n",
    "lstm = LSTM(1024, 2048, 3, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0164,  0.0693,  0.0390,  ..., -0.0925,  0.0564,  0.0884],\n",
       "        [-0.0491,  0.0902,  0.0821,  ..., -0.0888,  0.0981, -0.0451],\n",
       "        [-0.0084, -0.0034, -0.0749,  ...,  0.0713,  0.0714,  0.0286],\n",
       "        ...,\n",
       "        [-0.0234,  0.1000, -0.0939,  ..., -0.0311,  0.0420, -0.0901],\n",
       "        [ 0.0302, -0.0847,  0.0112,  ..., -0.0763,  0.0141,  0.0681],\n",
       "        [-0.0454,  0.0429, -0.0436,  ..., -0.0012, -0.0645, -0.0658]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.weight_hh_l0_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 2048])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.weight_hh_l0_reverse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 1024]), torch.Size([8192, 4096]), torch.Size([8192, 4096]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.weight_ih_l0.shape, lstm.weight_ih_l1.shape, lstm.weight_ih_l2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 1024]), torch.Size([8192, 4096]), torch.Size([8192, 4096]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.weight_ih_l0_reverse.shape, lstm.weight_ih_l1_reverse.shape, lstm.weight_ih_l2_reverse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm.all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, list)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm.all_weights[0]), type(lstm.all_weights[0]) # Wih, Wif, Wig, Wio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.0270, -0.0652, -0.0140,  ..., -0.0091,  0.0693,  0.0468],\n",
       "         [ 0.0362,  0.0413, -0.0744,  ...,  0.0002,  0.0516, -0.0539],\n",
       "         [ 0.0288, -0.0080, -0.0532,  ..., -0.0147,  0.0828,  0.0289],\n",
       "         ...,\n",
       "         [-0.0231,  0.0375, -0.0368,  ..., -0.0428, -0.0488, -0.0843],\n",
       "         [ 0.0663,  0.0398, -0.0229,  ...,  0.0997, -0.0129, -0.0330],\n",
       "         [ 0.0962, -0.0518, -0.0279,  ..., -0.0304,  0.0987,  0.0815]],\n",
       "        requires_grad=True),\n",
       " tensor([[ 0.0270, -0.0652, -0.0140,  ..., -0.0091,  0.0693,  0.0468],\n",
       "         [ 0.0362,  0.0413, -0.0744,  ...,  0.0002,  0.0516, -0.0539],\n",
       "         [ 0.0288, -0.0080, -0.0532,  ..., -0.0147,  0.0828,  0.0289],\n",
       "         ...,\n",
       "         [-0.0231,  0.0375, -0.0368,  ..., -0.0428, -0.0488, -0.0843],\n",
       "         [ 0.0663,  0.0398, -0.0229,  ...,  0.0997, -0.0129, -0.0330],\n",
       "         [ 0.0962, -0.0518, -0.0279,  ..., -0.0304,  0.0987,  0.0815]],\n",
       "        grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.all_weights[0][0], lstm.weight_ih_l0[:, :1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lstm.all_weights[0][0]  - lstm.weight_ih_l0[:, :1024]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm.all_weights[2]), len(lstm.all_weights[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 1024]), torch.Size([8192, 1024]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.all_weights[1][0].shape, lstm.weight_ih_l0_reverse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.0386, -0.0226,  0.0679,  ..., -0.0732,  0.0101, -0.0297],\n",
       "         [ 0.0937,  0.0623,  0.0302,  ...,  0.0047, -0.0335,  0.0184],\n",
       "         [ 0.0924, -0.0607, -0.0399,  ...,  0.0347, -0.0368,  0.0720],\n",
       "         ...,\n",
       "         [-0.0131,  0.0256,  0.0554,  ..., -0.0180, -0.0110,  0.0932],\n",
       "         [ 0.0628, -0.0708,  0.0190,  ..., -0.0954,  0.0142,  0.0919],\n",
       "         [ 0.0825,  0.0975, -0.0044,  ..., -0.0327,  0.0937,  0.0865]],\n",
       "        requires_grad=True),\n",
       " tensor([[ 0.0386, -0.0226,  0.0679,  ..., -0.0732,  0.0101, -0.0297],\n",
       "         [ 0.0937,  0.0623,  0.0302,  ...,  0.0047, -0.0335,  0.0184],\n",
       "         [ 0.0924, -0.0607, -0.0399,  ...,  0.0347, -0.0368,  0.0720],\n",
       "         ...,\n",
       "         [-0.0131,  0.0256,  0.0554,  ..., -0.0180, -0.0110,  0.0932],\n",
       "         [ 0.0628, -0.0708,  0.0190,  ..., -0.0954,  0.0142,  0.0919],\n",
       "         [ 0.0825,  0.0975, -0.0044,  ..., -0.0327,  0.0937,  0.0865]],\n",
       "        grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.all_weights[1][0], lstm.weight_ih_l0_reverse[:, :1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 4096]), torch.Size([8192, 4096]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.all_weights[2][0].shape, lstm.weight_ih_l1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 0.0657, -0.0548,  0.0271,  ...,  0.0324, -0.0260, -0.0590],\n",
       "         [ 0.0863,  0.0830,  0.0494,  ...,  0.0477, -0.0521, -0.0444],\n",
       "         [ 0.0781,  0.0236, -0.0523,  ...,  0.0738,  0.0493, -0.0803],\n",
       "         ...,\n",
       "         [-0.0906, -0.0942,  0.0503,  ..., -0.0435,  0.0104,  0.0342],\n",
       "         [-0.0487, -0.0549, -0.0973,  ..., -0.0448, -0.0126, -0.0669],\n",
       "         [-0.0916, -0.0943,  0.0814,  ...,  0.0107, -0.0140, -0.0919]],\n",
       "        requires_grad=True),\n",
       " tensor([[ 0.0657, -0.0548,  0.0271,  ..., -0.0245,  0.0995,  0.0231],\n",
       "         [ 0.0863,  0.0830,  0.0494,  ...,  0.0550, -0.0011,  0.0104],\n",
       "         [ 0.0781,  0.0236, -0.0523,  ..., -0.0888, -0.0014,  0.0096],\n",
       "         ...,\n",
       "         [-0.0906, -0.0942,  0.0503,  ...,  0.0483, -0.0793, -0.0889],\n",
       "         [-0.0487, -0.0549, -0.0973,  ..., -0.0006,  0.0926, -0.0764],\n",
       "         [-0.0916, -0.0943,  0.0814,  ..., -0.0793, -0.0456, -0.0537]],\n",
       "        grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.all_weights[2][0], lstm.weight_ih_l1[:, :1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 1024]),\n",
       " torch.Size([8192, 2048]),\n",
       " torch.Size([8192]),\n",
       " torch.Size([8192]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.all_weights[0][0].shape,lstm.all_weights[0][1].shape,  lstm.all_weights[0][2].shape, lstm.all_weights[0][3].shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8192, 4096]),\n",
       " torch.Size([8192, 2048]),\n",
       " torch.Size([8192]),\n",
       " torch.Size([8192]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.all_weights[2][0].shape,lstm.all_weights[2][1].shape,  lstm.all_weights[2][2].shape, lstm.all_weights[2][3].shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([-0.0824,  0.0584, -0.0415,  ..., -0.0184,  0.0727, -0.0223],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0824,  0.0584, -0.0415,  ..., -0.0184,  0.0727, -0.0223],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.all_weights[4][3], lstm.bias_hh_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([ 0.0708,  0.0131, -0.0339,  ..., -0.0379,  0.0161,  0.0285],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0708,  0.0131, -0.0339,  ..., -0.0379,  0.0161,  0.0285],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.all_weights[5][3], lstm.bias_hh_l2_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, (hn, cn) = lstm(embedding_net(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 4096])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4, 2048])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4, 2048])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0887, -0.2716, -0.0035,  ..., -0.1872, -0.2095,  0.2743],\n",
       "        [ 0.2145,  0.4700, -0.0473,  ...,  0.0824, -0.2775,  0.1891],\n",
       "        [-0.1518,  0.3974, -0.0902,  ...,  0.0360, -0.4124,  0.1423],\n",
       "        [-0.0665, -0.2272, -0.2140,  ..., -0.1708, -0.3700,  0.1284]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2423,  0.4435,  0.3883,  ...,  0.0613, -0.2588,  0.1261],\n",
       "        [-0.1673, -0.0230,  0.2761,  ..., -0.1330, -0.0902, -0.2122],\n",
       "        [ 0.3518,  0.5100,  0.3094,  ...,  0.0697, -0.0110, -0.0222],\n",
       "        [ 0.3315, -0.0788,  0.2140,  ...,  0.0008, -0.0609,  0.1283]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_config = {\n",
    "    'vocab_size': SRC_SIZE,\n",
    "    'embed_dim': EMBED_SIZE, \n",
    "    'embed_net': encoder_embed,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'bidirectional': True, \n",
    "    'model_type': 'lstm',\n",
    "    'batch_first': True,\n",
    "    'num_layers': 2,\n",
    "    'dropout_in': 0.1, \n",
    "    'dropout_out': 0.1,\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40000, 1024])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_embed.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_input(batch_size=BATCH_SIZE):\n",
    "    src = torch.randint(0, VOCAB_SIZE, (batch_size, SRC_SIZE))\n",
    "    mask_idx = torch.randint(5, SRC_SIZE, (batch_size, 1))\n",
    "    mask = torch.arange(SRC_SIZE, dtype=mask_idx.dtype) > mask_idx\n",
    "    src[mask] = PAD_IDX\n",
    "    src[torch.arange(batch_size), mask_idx.flatten()] = EOS_IDX \n",
    "    src[torch.arange(batch_size), [0] * batch_size] = BOS_IDX \n",
    "    src_lengths = (src !=0).sum(axis=1)\n",
    "    return src, src_lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.0):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    \n",
    "    def forward(self, x, inplace=False):\n",
    "        if self.p > 0 and self.training:\n",
    "            return F.dropout(x, self.p, training=True, inplace=inplace)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(5, 4, 8)\n",
    "drop_net = Dropout(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_net.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout()"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000e+00,  1.0222e+00,  1.2342e+00,  1.4332e+00, -1.6424e+00,\n",
       "          2.8525e+00, -5.2569e-01,  3.7283e-01],\n",
       "        [-1.8104e+00, -6.1083e-01, -5.3315e-01, -0.0000e+00, -1.1855e+00,\n",
       "          1.2388e+00, -1.5630e-01,  8.9528e-01],\n",
       "        [-1.0372e-01,  7.6339e-01, -9.3146e-01,  9.9091e-04,  9.3544e-01,\n",
       "         -4.4448e-01,  1.1550e+00,  3.9795e-01],\n",
       "        [-2.7333e-01,  2.5584e+00, -2.0908e+00, -5.5252e-02, -1.1611e+00,\n",
       "         -1.0628e+00,  0.0000e+00,  7.8899e-01]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = drop_net(x)\n",
    "y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_net.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_net.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.6925e-01,  9.1997e-01,  1.1108e+00,  1.2899e+00, -1.4782e+00,\n",
       "          2.5672e+00, -4.7312e-01,  3.3555e-01],\n",
       "        [-1.6293e+00, -5.4974e-01, -4.7983e-01, -4.9968e-01, -1.0670e+00,\n",
       "          1.1149e+00, -1.4067e-01,  8.0575e-01],\n",
       "        [-9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04,  8.4189e-01,\n",
       "         -4.0003e-01,  1.0395e+00,  3.5815e-01],\n",
       "        [-2.4600e-01,  2.3025e+00, -1.8817e+00, -4.9727e-02, -1.0450e+00,\n",
       "         -9.5650e-01,  3.3532e-02,  7.1009e-01]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = drop_net(x)\n",
    "y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoderModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_config):\n",
    "        super(SeqEncoderModel, self).__init__()\n",
    "        self.vocab_size = encoder_config['vocab_size']\n",
    "        self.embed_net = encoder_config['embed_net']\n",
    "        self.hidden_size = encoder_config['hidden_size']\n",
    "        self.num_layers = encoder_config['num_layers']\n",
    "        self.batch_first = encoder_config['batch_first']\n",
    "        self.bidirectional = encoder_config['bidirectional']\n",
    "        self.dropout_in = encoder_config['dropout_in']\n",
    "        self.dropout_out = encoder_config['dropout_out']\n",
    "        \n",
    "        self.model = self.get_model(encoder_config['model_type'])\n",
    "        self.dropout_in_model = Dropout(self.dropout_in)\n",
    "        self.dropout_out_model = Dropout(self.dropout_out)\n",
    "        \n",
    "        self.num_direction = 2 if self.bidirectional else 1\n",
    "        self.units = self.hidden_size * 2 if self.bidirectional else 1\n",
    "    \n",
    "    def get_model(self, model_type, **kwargs):\n",
    "        if model_type == 'lstm':\n",
    "            return LSTM(embed_size=self.embed_net.weight.shape[1],\n",
    "                        hidden_size=self.hidden_size,\n",
    "                        num_layers=self.num_layers,\n",
    "                        bidirectional=self.bidirectional,\n",
    "                        dropout=self.dropout_out if self.num_layers > 1 else 0.0,\n",
    "                        batch_first=self.batch_first,\n",
    "                        )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    def forward(self, src_tokens, src_lengths, ):\n",
    "        bs, ts = src_tokens.size()\n",
    "        x = self.embed_net(src_tokens)\n",
    "        x = self.dropout_in_model(x)\n",
    "        h0, c0 = torch.zeros(self.num_layers * self.num_direction, bs, self.hidden_size).to(device=src_tokens.device), \\\n",
    "            torch.zeros(self.num_layers * self.num_direction, bs, self.hidden_size).to(device=src_tokens.device)\n",
    "        out, (hn, cn) = self.model(x, (h0, c0))\n",
    "        x = self.dropout_out_model(out)\n",
    "        \n",
    "        hn = self.reshape_hidden(hn, bs)\n",
    "        cn = self.reshape_hidden(cn, bs)\n",
    "        \n",
    "        return x, (hn, cn)\n",
    "    \n",
    "    def reshape_hidden(self, h, batch_size):\n",
    "        if not self.bidirectional:\n",
    "            return h\n",
    "        h = h.view(self.num_layers, 2, batch_size, -1).transpose(1, 2).contiguous()\n",
    "        h = h.view(self.num_layers, batch_size, -1)\n",
    "        return h\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SeqEncoderModel(encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(1024, 2048, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, src_lengths = get_dummy_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(59)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_lengths[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,  7468, 19430, 18143,  5529,  2244,  8060, 15349, 26755, 15458,\n",
       "        27830,  2026, 26136,  8813, 25890,  5216, 16913,  6806,  3128, 37882,\n",
       "        15613,  1872,  3989, 37173, 31764, 30318, 12432, 38457,  5387, 27609,\n",
       "         6411, 17284, 24931, 23437, 24264,  2216, 35462, 13931, 32271, 19423,\n",
       "        27902, 15291,  2932, 27558,  5229, 31215, 38406, 12103, 30495,  7789,\n",
       "         2491, 16002, 31367, 25722, 39821, 17545, 33764, 36206,     3,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, (hn, cn) = encoder(src, src_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 4096])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4096])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4096])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear(in_units, out_units, bias=True):\n",
    "    lin = nn.Linear(in_features=in_units, out_features=out_units, bias=bias)\n",
    "    lin.weight.data.uniform_(-0.1, 0.1)\n",
    "    if bias:\n",
    "        lin.bias.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    return lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Identity():\n",
    "    return lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMCell(input_size, hidden_size, bias=True):\n",
    "    cell = nn.LSTMCell(input_size=input_size, hidden_size=hidden_size, bias=bias)\n",
    "    for name, param in cell.named_parameters():\n",
    "        if \"weight\" in name or \"bias\" in name:\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, src_hidden_size, out_size, bias=False):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.inp_proj = Linear(input_size + src_hidden_size, src_hidden_size, bias=bias)\n",
    "        self.out_proj = Linear(input_size + src_hidden_size, out_size, bias=bias)\n",
    "        self.scale = nn.Parameter(torch.empty(src_hidden_size, dtype=torch.float32).uniform_(-0.1, 0.1), requires_grad=True)\n",
    "        # self.score_net = Linear(src_hidden_size + src_hidden_size)\n",
    "    def forward(self, input_hidden, src_hiddens, src_lengths=None):\n",
    "        bs, ts, hs = src_hiddens.size()\n",
    "        # bs ts hs\n",
    "        x = input_hidden.expand(ts, -1, -1).transpose(0, 1)\n",
    "        x = torch.cat([src_hiddens, x], dim=2)\n",
    "        # bs, ts, hs\n",
    "        score_proj = self.inp_proj(x)\n",
    "        score_proj = F.tanh(score_proj)\n",
    "        # bs, ts\n",
    "        score_proj = score_proj @ self.scale\n",
    "        assert tuple(score_proj.size()) == tuple((bs, ts))\n",
    "        \n",
    "        if src_lengths is not None:\n",
    "            mask_idx = torch.arange(1, ts + 1).to(device=src_lengths.device)\n",
    "            mask_idx = mask_idx.tile((bs, 1))\n",
    "            mask = mask_idx < src_lengths.unsqueeze(dim=1)\n",
    "            score_proj.float().masked_fill_(mask, -torch.inf)\n",
    "\n",
    "        # bs, ts\n",
    "        attn_scores = F.softmax(score_proj, dim=1)\n",
    "        context_vector = src_hiddens * attn_scores.unsqueeze(dim=2)\n",
    "        # bs, hs\n",
    "        context_vector = context_vector.sum(axis=1)\n",
    "        \n",
    "        out = F.tanh(self.out_proj(\n",
    "            torch.cat([input_hidden, context_vector], dim=1)\n",
    "        ))\n",
    "        \n",
    "        return out, attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_config = {\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'embed_dim': decoder_embed.weight.size(1), \n",
    "    'embed_net': decoder_embed,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'decoder_only': True,\n",
    "    'encoder_units': encoder.units, \n",
    "    'batch_first': True,\n",
    "    'num_layers': 2,\n",
    "    'dropout_in': 0.1, \n",
    "    'dropout_out': 0.1,\n",
    "    'attention': True,\n",
    "    'residual': True\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDecoderModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, decoder_config):\n",
    "        super(SeqDecoderModel, self).__init__()\n",
    "        self.vocab_size = decoder_config['vocab_size']\n",
    "        self.embed_net = decoder_config['embed_net']\n",
    "        self.embed_dim = decoder_config['embed_dim']\n",
    "        self.hidden_size = decoder_config['hidden_size']\n",
    "        self.num_layers = decoder_config['num_layers']\n",
    "        self.batch_first = decoder_config['batch_first']\n",
    "        self.encoder_units = decoder_config['encoder_units']\n",
    "        self.dropout_in = decoder_config['dropout_in']\n",
    "        self.dropout_out = decoder_config['dropout_out']\n",
    "        self.decoder_only = decoder_config['decoder_only']\n",
    "        self.residual = decoder_config['residual']\n",
    "        \n",
    "        self.dropout_in_model = Dropout(self.dropout_in)\n",
    "        self.dropout_out_model = Dropout(self.dropout_out)\n",
    "        \n",
    "        # input_feed_size = 0 if self.encoder_units > 0 else self.hidden_size\n",
    "        input_feed_size = 0 if decoder_config['decoder_only'] else self.hidden_size\n",
    "        \n",
    "        if self.encoder_units != self.hidden_size:\n",
    "            self.hn_proj = Linear(self.encoder_units, self.hidden_size)\n",
    "            self.cn_proj = Linear(self.encoder_units, self.hidden_size)\n",
    "        else:\n",
    "            self.hn_proj = Identity()\n",
    "            self.cn_proj = Identity()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            LSTMCell(input_size=input_feed_size + self.embed_dim if layer == 0 else self.hidden_size,\n",
    "                     hidden_size=self.hidden_size)\n",
    "            for layer in range(self.num_layers)\n",
    "            \n",
    "        ])\n",
    "        \n",
    "        if decoder_config['attention']:\n",
    "            self.attention = AttentionLayer(self.hidden_size, self.encoder_units, self.hidden_size, bias=False)\n",
    "        else:\n",
    "            self.attention = None\n",
    "        \n",
    "        self.out_embed_net = Linear(self.hidden_size, self.embed_dim)\n",
    "        \n",
    "        self.classifier = Linear(self.embed_dim, self.vocab_size)\n",
    "        \n",
    "    def forward(self, tgt_tokens, context, ):\n",
    "        bs, ts = tgt_tokens.size()\n",
    "        encoder_out, (encoder_hn, encoder_cn), src_lengths = context\n",
    "        \n",
    "        # bs, ts, emb_size\n",
    "        x = self.embed_net(tgt_tokens)\n",
    "        x = self.dropout_in_model(x)\n",
    "        # ts, bs, emb_size\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        if self.decoder_only:\n",
    "            previous_hn = x.new_zeros(self.num_layers, bs, self.hidden_size)\n",
    "            previous_cn = x.new_zeros(self.num_layers, bs, self.hidden_size)\n",
    "            input_feed = None\n",
    "        else:\n",
    "            previous_hn = [self.hn_proj(encoder_hn[i]) for i in range(self.num_layers)]\n",
    "            previous_cn = [self.cn_proj(encoder_cn[i]) for i in range(self.num_layers)]\n",
    "            input_feed = x.new_zeros(bs, self.hidden_size)\n",
    "        \n",
    "        outs = []\n",
    "        attn_scores = x.new_zeros(bs, ts, encoder_out.size(1)) if self.attention else None\n",
    "        for seq in range(ts):\n",
    "            if input_feed is None:\n",
    "                input = x[seq, ...]\n",
    "            else:\n",
    "                input = torch.cat((x[seq, ...], input_feed), dim=1)\n",
    "            \n",
    "            for i in range(self.num_layers):\n",
    "                hn, cn = self.layers[i](input, (previous_hn[i], previous_cn[i]))\n",
    "                input = self.dropout_out_model(hn)\n",
    "                if self.residual:\n",
    "                    input = input + previous_hn[i]\n",
    "                previous_hn[i] = hn\n",
    "                previous_cn[i] = cn\n",
    "            \n",
    "            if self.attention:\n",
    "                out, scores = self.attention(hn, encoder_out, src_lengths)\n",
    "                attn_scores[:, seq, :] = scores\n",
    "            else:\n",
    "                out = hn\n",
    "            \n",
    "            out = self.dropout_out_model(out)\n",
    "            outs.append(out)\n",
    "            if input_feed is not None:\n",
    "                input_feed = out\n",
    "        \n",
    "\n",
    "        x = torch.stack(outs, dim=0).view(ts, bs, -1)\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        x = self.out_embed_net(x)\n",
    "        x = self.dropout_out_model(x)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x, attn_scores\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SeqEncoderModel(encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = SeqDecoderModel(decoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, src_lengths = get_dummy_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt, tgt_lengths = get_dummy_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_out, (enc_hn, enc_cn) = encoder(src, src_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 4096])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4, 2048])\n"
     ]
    }
   ],
   "source": [
    "context = (enc_out, (enc_hn, enc_cn), src_lengths)\n",
    "out, attn = decoder(tgt, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 40000])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.1071, 0.0727], grad_fn=<TopkBackward0>),\n",
       "indices=tensor([190, 191]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn[0][0].topk(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
