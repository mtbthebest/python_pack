{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx: autograd.function.FunctionCtx, x: torch.Tensor):\n",
    "        res = x.exp()\n",
    "        print('x: ', x)\n",
    "        print('res: ', res)\n",
    "        print('ctx: ', ctx)\n",
    "        ctx.save_for_backward(res)\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        print(\"backward: ----------\")\n",
    "        print('Grad outputs: ', grad_outputs)\n",
    "        out,  = ctx.saved_tensors\n",
    "        print('out:  ', out)\n",
    "        \n",
    "        return grad_outputs[0] * out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2.], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3).float().to(float).requires_grad_()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([0., 1., 2.], dtype=torch.float64, requires_grad=True)\n",
      "res:  tensor([1.0000, 2.7183, 7.3891], dtype=torch.float64)\n",
      "ctx:  <torch.autograd.function.ExpBackward object at 0x7f26747ba9a0>\n"
     ]
    }
   ],
   "source": [
    "exp = Exp.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.1073, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = exp.sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward: ----------\n",
      "Grad outputs:  (tensor([1., 1., 1.], dtype=torch.float64),)\n",
      "out:   tensor([1.0000, 2.7183, 7.3891], dtype=torch.float64, grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 2.7183, 7.3891], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias) -> torch.Tensor:\n",
    "        # out = w @ x  + b\n",
    "        out = weights @ input + bias\n",
    "        ctx.save_for_backward(input, weights, bias, out)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        print(\"Linear backward \", grad_outputs)\n",
    "        # print(ctx.saved_tensors)\n",
    "        input, weights, bias, out = ctx.saved_tensors\n",
    "        # grad_ouputs: dl / dout\n",
    "        # grad_w = dl / dw =  dout/dw * dl/dout = x\n",
    "        grad_w = grad_outputs[0].unsqueeze(1) * input.expand(out.size(0), -1)\n",
    "        grad_b =  bias.new_ones(bias.size()) * grad_outputs[0]\n",
    "        print(\"grad w shape \", grad_w.shape)\n",
    "        print(\"grad b shape \", grad_b.shape)\n",
    "        inp_grad = None\n",
    "        \n",
    "        if ctx.needs_input_grad[0]:\n",
    "            print(\"Need inp grad \")\n",
    "            inp_grad = weights.t() @ grad_outputs[0]\n",
    "        return  inp_grad, grad_w, grad_b,\n",
    "    \n",
    "class Sum(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        out = x.sum()\n",
    "        ctx.save_for_backward(x)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        # J * v\n",
    "        # Jacobian dy.T/dx\n",
    "        # grad_outputs: v\n",
    "        \n",
    "        print(\"Sum backward: \", grad_outputs)\n",
    "        x,  = ctx.saved_tensors\n",
    "        grad = grad_outputs[0] * torch.ones_like(x)\n",
    "        print(\"Returning sum grad: \", grad)\n",
    "        return grad\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum backward:  (tensor(2.),)\n",
      "Returning sum grad:  tensor([2., 2., 2.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2.])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.arange(3).float().requires_grad_()\n",
    "t = 2 * Sum.apply(s)\n",
    "t.backward()\n",
    "s.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]),\n",
       " tensor([[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.],\n",
       "         [ 9., 10., 11.]], requires_grad=True),\n",
       " tensor([0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.arange(4*3, dtype=torch.float).view(4, 3).requires_grad_()\n",
    "b = torch.zeros(4).type_as(w).requires_grad_()\n",
    "x = torch.arange(3).float()\n",
    "x, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 14., 23., 32.], grad_fn=<LinearBackward>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = Linear.apply(x, w, b)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(148., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 2 * Sum.apply(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum backward:  (tensor(2.),)\n",
      "Returning sum grad:  tensor([2., 2., 2., 2.])\n",
      "Linear backward  (tensor([2., 2., 2., 2.]),)\n",
      "grad w shape  torch.Size([4, 3])\n",
      "grad b shape  torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 4.],\n",
       "        [0., 2., 4.],\n",
       "        [0., 2., 4.],\n",
       "        [0., 2., 4.]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad -= 0.1 * w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.8000, 3.6000],\n",
       "        [0.0000, 1.8000, 3.6000],\n",
       "        [0.0000, 1.8000, 3.6000],\n",
       "        [0.0000, 1.8000, 3.6000]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.],\n",
       "         [ 9., 10., 11.]], requires_grad=True),\n",
       " tensor([5., 7., 9.]),\n",
       " tensor([ 25.,  88., 151., 214.], grad_fn=<LinearBackward>),\n",
       " tensor(478., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.arange(4*3, dtype=torch.float).view(4, 3).requires_grad_()\n",
    "b = torch.zeros(4).type_as(w).requires_grad_()\n",
    "x = torch.arange(3).float()\n",
    "q = torch.ones(3)\n",
    "u =  2 * x * q  + 5\n",
    "y = Linear.apply(u, w, b)\n",
    "w, u, y,y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear backward  (tensor([1., 1., 1., 1.]),)\n",
      "grad w shape  torch.Size([4, 3])\n",
      "grad b shape  torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 7., 9.],\n",
       "        [5., 7., 9.],\n",
       "        [5., 7., 9.],\n",
       "        [5., 7., 9.]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hadamard(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, scale) -> torch.Tensor:\n",
    "        print(\"forward hadamard: \", x)\n",
    "        out = x * scale\n",
    "        ctx.save_for_backward(x)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        print(\"Hadamard backward \", grad_outputs)\n",
    "        print(ctx.saved_tensors)\n",
    "        input, = ctx.saved_tensors\n",
    "        print(\"grad hadamard \")\n",
    "        return  None, input * grad_outputs[0]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward hadamard:  tensor([0., 1., 2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(3).float()\n",
    "q = torch.empty_like(x).copy_(x).requires_grad_()\n",
    "h = Hadamard.apply(x, q)\n",
    "h.retain_grad() # because not leaf this is needed for getting the gradients\n",
    "w = torch.arange(4*3, dtype=torch.float).view(4, 3).requires_grad_()\n",
    "b = torch.zeros(4).type_as(w).requires_grad_()\n",
    "y = Linear.apply(h, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 24., 39., 54.], grad_fn=<LinearBackward>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(126., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear backward  (tensor([1., 1., 1., 1.]),)\n",
      "grad w shape  torch.Size([4, 3])\n",
      "grad b shape  torch.Size([4])\n",
      "Need inp grad \n",
      "Hadamard backward  (tensor([18., 22., 26.]),)\n",
      "(tensor([0., 1., 2.]),)\n",
      "grad hadamard \n"
     ]
    }
   ],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.],\n",
       "         [ 9., 10., 11.]], requires_grad=True),\n",
       " tensor([[0., 1., 4.],\n",
       "         [0., 1., 4.],\n",
       "         [0., 1., 4.],\n",
       "         [0., 1., 4.]]))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 4.], grad_fn=<HadamardBackward>), tensor([18., 22., 26.]))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, h.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., 22., 52.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward hadamard:  tensor([0., 1., 2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(3).float()\n",
    "q = torch.arange(3).float().requires_grad_()\n",
    "h = Hadamard.apply(x, q)\n",
    "h.retain_grad()\n",
    "w = torch.arange(4*3, dtype=torch.float).view(4, 3).requires_grad_()\n",
    "b = torch.zeros(4).type_as(w).requires_grad_()\n",
    "# y = w @ h + b\n",
    "y = Linear.apply(h, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear backward  (tensor([1., 1., 1., 1.]),)\n",
      "grad w shape  torch.Size([4, 3])\n",
      "grad b shape  torch.Size([4])\n",
      "Need inp grad \n",
      "Hadamard backward  (tensor([18., 22., 26.]),)\n",
      "(tensor([0., 1., 2.]),)\n",
      "grad hadamard \n"
     ]
    }
   ],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 4.],\n",
       "        [0., 1., 4.],\n",
       "        [0., 1., 4.],\n",
       "        [0., 1., 4.]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., 22., 52.])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18., 22., 26.])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7f2560527c70>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MvBackward0 at 0x7f2560527d90>, 0), (<AccumulateGrad at 0x7f2560527dc0>, 0))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.autograd.function.HadamardBackward at 0x7f2537b71130>, 0)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((None, 0), (<AccumulateGrad at 0x7f2537b5a850>, 0))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0].next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.function.HadamardBackward at 0x7f25604cd400>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0].next_functions[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((None, 0), (<AccumulateGrad at 0x7f25604ccc40>, 0))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0].next_functions[1][0].next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AccumulateGrad at 0x7f25604ccc40>, 0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn.next_functions[0][0].next_functions[1][0].next_functions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 2, 3]).requires_grad_()\n",
    "y = x\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 2, 3]).requires_grad_()\n",
    "y = x\n",
    "x.requires_grad_(False)\n",
    "z = x\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tensor1(x, y):\n",
    "    return x + y\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_tensor2(x, y):\n",
    "    return x + y\n",
    "\n",
    "x = torch.tensor([1., 2, 3]).requires_grad_()\n",
    "y = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_tensor1(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_tensor2(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4446,  3.3463, -3.2221], grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(3, 4).requires_grad_()\n",
    "x = torch.arange(4).float()\n",
    "y = W @ x\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(y), retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3.],\n",
       "        [0., 1., 2., 3.],\n",
       "        [0., 1., 2., 3.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(y),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 4., 6.],\n",
       "        [0., 2., 4., 6.],\n",
       "        [0., 2., 4., 6.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    z = x**2 \n",
    "    u = 2*x\n",
    "    y = z  + u + 1\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = autograd.functional.jacobian(func, torch.tensor(1.0))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = autograd.functional.jacobian(func, torch.tensor(0.))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 0.],\n",
       "        [0., 4.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = autograd.functional.jacobian(func, torch.tensor([0., 1.]))\n",
    "y # [dy1/dx, dy2/dx].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = autograd.functional.jacobian(lambda x: torch.dot(x, x), \n",
    "                                torch.tensor([0., 1.]))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hess_fn(x):\n",
    "    return x.t() @ torch.eye(x.size(0)) @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 0.],\n",
       "        [0., 2.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.functional.hessian(hess_fn, torch.tensor([1.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.functional.jacobian(hess_fn, torch.tensor([1.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(10.))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (output, J*v)\n",
    "autograd.functional.jvp(hess_fn, torch.tensor([1.0, 2.0]), v=torch.tensor([1.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor([4., 8.]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.functional.vjp(hess_fn, torch.tensor([1.0, 2.0]), v=torch.tensor(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adder(x, y):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    return 2 * x + 3 *y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([4., 5., 6.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 0., 0.],\n",
       "         [0., 2., 0.],\n",
       "         [0., 0., 2.]]),\n",
       " tensor([[3., 0., 0.],\n",
       "         [0., 3., 0.],\n",
       "         [0., 0., 3.]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.functional.jacobian(adder, (torch.tensor([1.0, 2.0, 3.0]), torch.tensor([4.0, 5.0, 6.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([4., 5., 6.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([14., 19., 24.]), tensor([3., 3., 3.]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.functional.jvp(adder, (torch.tensor([1.0, 2.0, 3.0]), torch.tensor([4.0, 5.0, 6.0])), v=(torch.tensor([0.0, 0.0, 0.0]), torch.tensor([1.0, 1.0, 1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([4., 5., 6.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([14., 19., 24.]), tensor([5., 5., 5.]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.functional.jvp(adder, (torch.tensor([1.0, 2.0, 3.0]), torch.tensor([4.0, 5.0, 6.0])), v=(torch.tensor([1.0] * 3), torch.tensor([1.0, 1.0, 1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  1.],\n",
       "        [ 1., 14.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return x ** 3 + x + sum(x)\n",
    "jac = autograd.functional.jacobian(square, torch.tensor([1.0, 2.0]))\n",
    "jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5., 13.]), tensor([ 6., 15.]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# J = dy/dx.T\n",
    "# dy_i = 3 *x**2 + 1 + I.roll(1)\n",
    "# dy_1 = [5  1]\n",
    "# dy_2 = [1  14]\n",
    "# jvp = inner_prod(J, v)\n",
    "out, jvp = autograd.functional.jvp(square, torch.tensor([1.0, 2.0]), v=torch.ones(2))\n",
    "out, jvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  1.],\n",
       "        [ 1., 14.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jac\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 15.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 15.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac.inner(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183, 7.3891])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exp_func(x):\n",
    "    return x.exp().sum()\n",
    "autograd.functional.jacobian(exp_func, torch.tensor([1.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.1073)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_func(torch.tensor([1.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7183,  7.3891],\n",
       "        [20.0855, 54.5981]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.functional.jacobian(exp_func, torch.tensor([[1.0, 2.0], [3.0, 4.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(84.7910),\n",
       " tensor([[ 2.7183,  7.3891],\n",
       "         [20.0855, 54.5981]]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.functional.vjp(exp_func, torch.tensor([[1.0, 2.0], [3.0, 4.0]]), torch.tensor(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.7183,  7.3891],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [20.0855, 54.5981]]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exp_func(x):\n",
    "    return x.exp().sum(dim=1)\n",
    "# J = block(dy / dX)\n",
    "autograd.functional.jacobian(exp_func, torch.tensor([[1.0, 2.0], [3.0, 4.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.7183,  7.3891],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000],\n",
       "         [20.0855, 54.5981]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.functional.jacobian(exp_func, torch.tensor([[1.0, 2.0], [3.0, 4.0]]), create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10.1073, 74.6837]),\n",
       " tensor([[ 2.7183,  7.3891],\n",
       "         [20.0855, 54.5981]]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  v = dl / dy\n",
    "# vjp_i = v[0] * dy1 / dx1 + v[1] * dy2 / dx1\n",
    "autograd.functional.vjp(exp_func, torch.tensor([[1.0, 2.0], [3.0, 4.0]]), v=torch.tensor([1.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10.1073, 74.6837]),\n",
       " tensor([[ 0.0000,  0.0000],\n",
       "         [20.0855, 54.5981]]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  v = dl / dy\n",
    "# vjp_i = v[0] * dy1 / dx1 + v[1] * dy2 / dx1\n",
    "autograd.functional.vjp(exp_func, torch.tensor([[1.0, 2.0], [3.0, 4.0]]), v=torch.tensor([0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9642,  0.4815, -1.2837, -1.4077,  0.7736, -3.2275,  1.5103,  4.7469,\n",
       "        -0.0962, -1.6041])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "N = 10\n",
    "D = 5\n",
    "x = torch.randn(N, D)\n",
    "y = torch.randn(N, D)\n",
    "res = (x * y).sum(dim=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchedTensor(lvl=1, bdim=0, value=\n",
      "    tensor([[-1.0408,  0.9166, -1.3042, -1.1097, -1.2188],\n",
      "            [ 1.1676, -1.0574, -0.1188, -0.9078,  0.3452],\n",
      "            [-0.5713, -0.2351,  1.0076, -0.7529, -0.2250],\n",
      "            [-0.4327, -1.5071, -0.4586, -0.8480,  0.5266],\n",
      "            [ 0.0299, -0.0498,  1.0651,  0.8860,  0.4640],\n",
      "            [-0.4986,  0.1289,  2.7631,  0.1405,  1.1191],\n",
      "            [ 0.3152,  1.7528, -0.7650,  1.8299, -1.6036],\n",
      "            [ 1.8493,  0.0447,  1.5853, -0.5912,  1.1312],\n",
      "            [ 0.9466, -1.7669, -0.5833, -0.4407, -1.9791],\n",
      "            [ 0.7787, -0.7749, -0.1398, -0.3467,  0.0873]])\n",
      ") BatchedTensor(lvl=1, bdim=0, value=\n",
      "    tensor([[-1.4702, -0.2134, -0.8707,  1.6159, -0.2356],\n",
      "            [ 0.9444,  0.5461, -1.3575,  0.1757, -0.1319],\n",
      "            [-0.2735,  0.3355,  0.1885,  2.1432, -0.2779],\n",
      "            [ 0.5511, -0.0625,  0.8269,  0.5599, -0.7776],\n",
      "            [ 0.3339,  0.1759,  0.4863,  0.2769,  0.0195],\n",
      "            [ 1.1213, -1.4873, -0.2043, -1.0466, -1.5772],\n",
      "            [ 0.4269,  0.6789,  0.2421,  0.6463,  0.5061],\n",
      "            [ 1.5192, -0.4897,  0.9231,  1.9978,  1.4827],\n",
      "            [ 0.1945, -1.1372, -0.9244,  1.8737,  1.0122],\n",
      "            [-1.4482,  0.1755,  0.6062,  0.3114, -1.6916]])\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9642,  0.4815, -1.2837, -1.4077,  0.7736, -3.2275,  1.5103,  4.7469,\n",
       "        -0.0962, -1.6041])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_dot(x, y):\n",
    "    print(x, y)\n",
    "    return (x * y).sum()\n",
    "\n",
    "torch.manual_seed(2)\n",
    "N = 10\n",
    "D = 5\n",
    "x = torch.randn(N, D)\n",
    "y = torch.randn(N, D)\n",
    "res = torch.vmap(batched_dot)(x, y)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.0021,  2.0832,  2.7936, -4.8749, -3.1466])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def batched_dot(x, y):\n",
    "    return (x * y).sum()\n",
    "\n",
    "torch.manual_seed(2)\n",
    "N = 10\n",
    "D = 5\n",
    "x = torch.randn(N, D)\n",
    "y = torch.randn(N, D)\n",
    "res = torch.vmap(batched_dot, in_dims=1)(x, y,)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def batched_dot(x, y):\n",
    "    return (x * y).sum()\n",
    "\n",
    "def mult_dot(x, y):\n",
    "    return (x * y).sum(dim=1)\n",
    "torch.manual_seed(2)\n",
    "N = 40000\n",
    "D = 5000\n",
    "x = torch.randn(N, D)\n",
    "y = torch.randn(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360 ms ± 14.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mult_dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmap = torch.vmap(batched_dot, in_dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393 ms ± 23.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  vmap(x, y,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 ms ± 13.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  vmap(x, y,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369 ms ± 14.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mult_dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398 ms ± 16.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  vmap(x, y,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1.).requires_grad_()\n",
    "y = x ** 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.),)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.grad(y, x, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.),)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.grad(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.),)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1.).requires_grad_()\n",
    "u = torch.tensor(2.0).requires_grad_()\n",
    "y = x ** 2 + u\n",
    "z = 3 * y  + 1\n",
    "autograd.grad(z, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.), tensor(3.))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1.).requires_grad_()\n",
    "u = torch.tensor(2.0).requires_grad_()\n",
    "y = x ** 2 + u\n",
    "z = 3 * y  + 1\n",
    "autograd.grad(z, (x, u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(18.), tensor(9.))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1.).requires_grad_()\n",
    "u = torch.tensor(2.0).requires_grad_()\n",
    "y = x ** 2 + u\n",
    "z = 3 * y  + 1\n",
    "autograd.grad(z, (x, u), grad_outputs=(torch.tensor(3.0), torch.tensor(4.0), torch.tensor(4.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([18., 36.]), tensor(18.))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "u = torch.tensor(2.0).requires_grad_()\n",
    "y = x ** 2 + u  # [x1 ** 2 + u   x2 ** 2 + u]\n",
    "z = (3 * y  + 1).sum()\n",
    "# dz / du = 3.0 * [3 3] * [1, 1].T\n",
    "autograd.grad(z, (x, u), grad_outputs=(torch.tensor(3.0), torch.tensor(4.0), torch.tensor(4.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 4.]), tensor(2.))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "u = torch.tensor(2.0).requires_grad_()\n",
    "y = x ** 2 + u  \n",
    "z = y.sum()\n",
    "autograd.grad(z, (x, u), grad_outputs=(torch.tensor(1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([20., 40.]), tensor(20.))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "u = torch.tensor(2.0).requires_grad_()\n",
    "y = x ** 2 + u  # [x1 + u   x2 + u]\n",
    "z1 = (3 * y  + 1).sum() # dz1/dx = [6*x1 6*x2], [6]\n",
    "z2 = y.sum() # dz1/dx = [2*x1 2*x2], [2]\n",
    "autograd.grad((z1, z2), (x, u), grad_outputs=(torch.tensor(3.0), torch.tensor(1.0),))\n",
    "# aggeration of grad wrt to inputs: x.grad = dz1 / dx + dz2 / dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([20., 40.], grad_fn=<MulBackward0>), tensor(20.))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 2.]).requires_grad_()\n",
    "u = torch.tensor(2.0).requires_grad_()\n",
    "y = x ** 2 + u  # [x1 + u   x2 + u]\n",
    "z1 = (3 * y  + 1).sum()\n",
    "z2 = y.sum()\n",
    "autograd.grad((z1, z2), (x, u), grad_outputs=(torch.tensor(3.0), torch.tensor(1.0),), create_graph=True)\n",
    "# aggeration of grad wrt to inputs: x.grad = dz1 / dx + dz2 / dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate jacobian from Rn -> R\n",
    "D = 10\n",
    "x = torch.arange(D).float().requires_grad_()\n",
    "y = sum(x)\n",
    "autograd.grad(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate jacobian from Rn -> Rn\n",
    "D = 10\n",
    "x = torch.arange(D).float().requires_grad_()\n",
    "y = x\n",
    "v = torch.eye(D)\n",
    "def get_grad(v):\n",
    "    return autograd.grad(x, x, v)\n",
    "torch.vmap(get_grad)(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 2., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 2., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 2., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 2., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 2., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 2., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 2., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 2., 1.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 2.]]),)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate jacobian from Rn -> Rn\n",
    "D = 10\n",
    "x = torch.arange(D).float().requires_grad_()\n",
    "y = x + x.sum()\n",
    "v = torch.eye(D)\n",
    "def get_grad(v):\n",
    "    print(v.size(), y.size())\n",
    "    return autograd.grad(y, x, v) # vjp\n",
    "                         \n",
    "torch.vmap(get_grad)(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.8423,  0.5189, -1.7119, -1.7014,  2.0194],\n",
       "          [-0.2686, -0.1307, -1.4374,  0.3908, -0.0190]],\n",
       " \n",
       "         [[-1.3527, -0.7308,  0.9879, -0.4194, -0.5849],\n",
       "          [-0.7823,  2.7799,  1.2220, -0.3364, -0.9651]],\n",
       " \n",
       "         [[-0.1297, -0.6018,  0.1450, -0.1498,  0.8183],\n",
       "          [-0.6633,  0.2653, -1.5660, -1.6407, -0.0197]],\n",
       " \n",
       "         [[ 0.2278, -0.3985, -1.0365,  0.6705, -0.1777],\n",
       "          [ 0.4314,  1.2417,  2.1503, -2.2281, -1.2897]]]),\n",
       " tensor([[[ 0.0000,  0.0000, -0.0000, -0.0000,  2.0194],\n",
       "          [-0.0000, -0.0000, -0.0000,  0.0000, -0.0190]],\n",
       " \n",
       "         [[-0.0000, -0.0000,  0.0000, -0.0000, -0.5849],\n",
       "          [-0.0000,  0.0000,  0.0000, -0.0000, -0.9651]],\n",
       " \n",
       "         [[-0.0000, -0.0000,  0.0000, -0.0000,  0.8183],\n",
       "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0197]],\n",
       " \n",
       "         [[ 0.0000, -0.0000, -0.0000,  0.0000, -0.1777],\n",
       "          [ 0.0000,  0.0000,  0.0000, -0.0000, -1.2897]]]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y# calculate jacobian from Rn -> Rn\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "torch.manual_seed(5)\n",
    "x = torch.randn(4, 2, 5)\n",
    "y = torch.zeros(2, 5)\n",
    "y[:, -1] = 1.\n",
    "x, torch.vmap(multiply, in_dims=(0, None))(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.8423,  0.5189, -1.7119, -1.7014,  2.0194],\n",
       "          [-0.2686, -0.1307, -1.4374,  0.3908, -0.0190]],\n",
       " \n",
       "         [[-1.3527, -0.7308,  0.9879, -0.4194, -0.5849],\n",
       "          [-0.7823,  2.7799,  1.2220, -0.3364, -0.9651]],\n",
       " \n",
       "         [[-0.1297, -0.6018,  0.1450, -0.1498,  0.8183],\n",
       "          [-0.6633,  0.2653, -1.5660, -1.6407, -0.0197]],\n",
       " \n",
       "         [[ 0.2278, -0.3985, -1.0365,  0.6705, -0.1777],\n",
       "          [ 0.4314,  1.2417,  2.1503, -2.2281, -1.2897]]]),\n",
       " tensor([[[ 0.0000, -0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000,  0.0000],\n",
       "          [ 2.0194, -0.5849,  0.8183, -0.1777]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0190, -0.9651, -0.0197, -1.2897]]]),\n",
       " torch.Size([2, 5, 4]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate jacobian from Rn -> Rn\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "torch.manual_seed(5)\n",
    "x = torch.randn(4, 2, 5)\n",
    "y = torch.zeros(2, 5)\n",
    "y[:, -1] = 1.\n",
    "z = torch.vmap(multiply, in_dims=(0, None), out_dims=-1)(x, y)\n",
    "x, z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.8423,  0.5189, -1.7119, -1.7014,  2.0194],\n",
       "          [-0.2686, -0.1307, -1.4374,  0.3908, -0.0190]],\n",
       " \n",
       "         [[-1.3527, -0.7308,  0.9879, -0.4194, -0.5849],\n",
       "          [-0.7823,  2.7799,  1.2220, -0.3364, -0.9651]],\n",
       " \n",
       "         [[-0.1297, -0.6018,  0.1450, -0.1498,  0.8183],\n",
       "          [-0.6633,  0.2653, -1.5660, -1.6407, -0.0197]],\n",
       " \n",
       "         [[ 0.2278, -0.3985, -1.0365,  0.6705, -0.1777],\n",
       "          [ 0.4314,  1.2417,  2.1503, -2.2281, -1.2897]]]),\n",
       " tensor([[[ 0.0000, -0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000, -0.0000],\n",
       "          [-0.0000, -0.0000, -0.0000,  0.0000],\n",
       "          [ 2.0194, -0.5849,  0.8183, -0.1777]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [-0.0190, -0.9651, -0.0197, -1.2897]]]),\n",
       " torch.Size([2, 5, 4]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate jacobian from Rn -> Rn\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "torch.manual_seed(5)\n",
    "x = torch.randn(4, 2, 5)\n",
    "y = torch.zeros(2, 5)\n",
    "y[:, -1] = 1.\n",
    "z = torch.vmap(multiply, in_dims=(0, None), out_dims=2)(x, y)\n",
    "x, z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.8423,  0.5189, -1.7119, -1.7014,  2.0194],\n",
       "          [-0.2686, -0.1307, -1.4374,  0.3908, -0.0190]],\n",
       " \n",
       "         [[-1.3527, -0.7308,  0.9879, -0.4194, -0.5849],\n",
       "          [-0.7823,  2.7799,  1.2220, -0.3364, -0.9651]],\n",
       " \n",
       "         [[-0.1297, -0.6018,  0.1450, -0.1498,  0.8183],\n",
       "          [-0.6633,  0.2653, -1.5660, -1.6407, -0.0197]],\n",
       " \n",
       "         [[ 0.2278, -0.3985, -1.0365,  0.6705, -0.1777],\n",
       "          [ 0.4314,  1.2417,  2.1503, -2.2281, -1.2897]]]),\n",
       " tensor([[[ 0.0000,  0.0000, -0.0000, -0.0000,  2.0194],\n",
       "          [-0.0000, -0.0000,  0.0000, -0.0000, -0.5849],\n",
       "          [-0.0000, -0.0000,  0.0000, -0.0000,  0.8183],\n",
       "          [ 0.0000, -0.0000, -0.0000,  0.0000, -0.1777]],\n",
       " \n",
       "         [[-0.0000, -0.0000, -0.0000,  0.0000, -0.0190],\n",
       "          [-0.0000,  0.0000,  0.0000, -0.0000, -0.9651],\n",
       "          [-0.0000,  0.0000, -0.0000, -0.0000, -0.0197],\n",
       "          [ 0.0000,  0.0000,  0.0000, -0.0000, -1.2897]]]),\n",
       " torch.Size([2, 4, 5]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate jacobian from Rn -> Rn\n",
    "def multiply(x, y):\n",
    "    return x * y\n",
    "torch.manual_seed(5)\n",
    "x = torch.randn(4, 2, 5)\n",
    "y = torch.zeros(2, 5)\n",
    "y[:, -1] = 1.\n",
    "z = torch.vmap(multiply, in_dims=(0, None), out_dims=1)(x, y)\n",
    "x, z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCube(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        print(\"Forward \")\n",
    "        \n",
    "        # We wish to save dx for backward. In order to do so, it must\n",
    "        # be returned as an output.\n",
    "        dx = 3 * x ** 2\n",
    "        result = x ** 3\n",
    "        return result, dx\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        print(\"Setup context: \")\n",
    "        x, = inputs\n",
    "        result, dx = output\n",
    "        ctx.save_for_backward(x, dx)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, grad_dx):\n",
    "        x, dx = ctx.saved_tensors\n",
    "        # In order for the autograd.Function to work with higher-order\n",
    "        # gradients, we must add the gradient contribution of `dx`,\n",
    "        # which is grad_dx * 6 * x.\n",
    "        result = grad_output * dx + grad_dx * 6 * x\n",
    "        print(\"grad: \", result)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward \n",
      "Setup context: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-7.4607, -0.1744,  0.0792], grad_fn=<MyCubeBackward>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrap MyCube in a function so that it is clearer what the output is\n",
    "def my_cube(x):\n",
    "    result, dx = MyCube.apply(x)\n",
    "    return result\n",
    "\n",
    "my_cube(torch.randn(3, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward \n",
      "Setup context: \n",
      "grad:  tensor([15.3814,  0.1221,  3.4200])\n"
     ]
    }
   ],
   "source": [
    "my_cube(torch.randn(3, requires_grad=True)).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFunction(Function):\n",
    "    @staticmethod\n",
    "    # ctx is the first argument to forward\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        # The forward pass can use ctx.\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.cpu().numpy()\n",
    "\n",
    "class NumpySort(torch.autograd.Function):\n",
    "    # Note that forward does not take ctx\n",
    "    @staticmethod\n",
    "    def forward(x, dim):\n",
    "        device = x.device\n",
    "        x = to_numpy(x)\n",
    "        ind = np.argsort(x, axis=dim)\n",
    "        ind_inv = np.argsort(ind, axis=dim)\n",
    "        result = np.take_along_axis(x, ind, axis=dim)\n",
    "        print(\"forward \", result, ind)\n",
    "        \n",
    "        # Any intermediates to be saved in backward must be returned as\n",
    "        # outputs.\n",
    "        return (\n",
    "            # The desired output\n",
    "            torch.tensor(result, device=device),\n",
    "            # intermediate to save for backward\n",
    "            torch.tensor(ind, device=device),\n",
    "            # intermediate to save for backward\n",
    "            torch.tensor(ind_inv, device=device),\n",
    "        )\n",
    "\n",
    "    # setup_context is responsible for calling methods and/or assigning to\n",
    "    # the ctx object. Please do not do additional compute (e.g. add\n",
    "    # Tensors together) in setup_context.\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x, dim = inputs\n",
    "        # Note that output is whatever you returned from forward.\n",
    "        # If you returned multiple values, then output is a Tuple of multiple values.\n",
    "        # If you returned a single Tensor, then output is a Tensor.\n",
    "        # If you returned a Tuple with a single Tensor, then output is a\n",
    "        # Tuple with a single Tensor.\n",
    "        _, ind, ind_inv = output\n",
    "        ctx.mark_non_differentiable(ind, ind_inv)\n",
    "        print('setup context sort ', ind)\n",
    "        # Tensors must be saved via ctx.save_for_backward. Please do not\n",
    "        # assign them directly onto the ctx object.\n",
    "        ctx.save_for_backward(ind, ind_inv)\n",
    "        # Non-tensors may be saved by assigning them as attributes on the ctx object.\n",
    "        ctx.dim = dim\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, _0, _1):\n",
    "        # For the autograd.Function to be arbitrarily composable with function\n",
    "        # transforms, all staticmethod other than forward and setup_context\n",
    "        # must be implemented in a \"transformable\" way; that is, they must\n",
    "        # only consist of PyTorch operations or autograd.Function.\n",
    "        #\n",
    "        # For example, this allows us to do double backwards and/or compute\n",
    "        # second order gradients.\n",
    "        #\n",
    "        # We've written the backward pass of NumpySort in terms of another\n",
    "        # autograd.Function, NumpyTake.\n",
    "        ind, ind_inv = ctx.saved_tensors\n",
    "        print(\"backward sort \", grad_output)\n",
    "        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n",
    "\n",
    "class NumpyTake(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x, ind, ind_inv, dim):\n",
    "        print(\"forward take \")\n",
    "        device = x.device\n",
    "        x = to_numpy(x)\n",
    "        ind = to_numpy(ind)\n",
    "        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x, ind, ind_inv, dim = inputs\n",
    "        ctx.save_for_backward(ind, ind_inv)\n",
    "        print('setup context take ')\n",
    "        \n",
    "        ctx.dim = dim\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        print('backward take ')\n",
    "        ind, ind_inv = ctx.saved_tensors\n",
    "        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n",
    "        return result, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_sort(x, dim=-1):\n",
    "    result, _, _ = NumpySort.apply(x, dim)\n",
    "    print(\"done forward\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6014, -1.0122, -0.3023],\n",
       "        [-1.2277,  0.9198, -0.3485]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "x = torch.randn(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  [[-1.0122098  -0.6013928  -0.30226925]\n",
      " [-1.2276864  -0.3484694   0.91982824]] [[1 0 2]\n",
      " [0 2 1]]\n",
      "setup context sort  tensor([[1, 0, 2],\n",
      "        [0, 2, 1]])\n",
      "done forward\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0122, -0.6014, -0.3023],\n",
       "        [-1.2277, -0.3485,  0.9198]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = numpy_sort(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward  [[-1.0122098  -0.6013928  -0.30226925]\n",
      " [-1.2276864  -0.3484694   0.91982824]] [[1 0 2]\n",
      " [0 2 1]]\n",
      "setup context sort  tensor([[1, 0, 2],\n",
      "        [0, 2, 1]])\n",
      "setup context sort  GradTrackingTensor(lvl=1, value=\n",
      "    tensor([[1, 0, 2],\n",
      "            [0, 2, 1]])\n",
      ")\n",
      "done forward\n",
      "backward sort  GradTrackingTensor(lvl=1, value=\n",
      "    tensor([[1., 1., 1.],\n",
      "            [1., 1., 1.]])\n",
      ")\n",
      "forward take \n",
      "setup context take \n",
      "setup context take \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "x = torch.randn(2, 3)\n",
    "x\n",
    "grad_x = torch.func.grad(lambda x: numpy_sort(x).sum())(x)\n",
    "grad_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1920, -0.9582, -0.8692],\n",
       "        [-0.9373, -0.8465,  1.9050]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.cpu().numpy()\n",
    "\n",
    "class NumpySort(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x, dim):\n",
    "        device = x.device\n",
    "        x = to_numpy(x)\n",
    "        ind = np.argsort(x, axis=dim)\n",
    "        ind_inv = np.argsort(ind, axis=dim)\n",
    "        result = np.take_along_axis(x, ind, axis=dim)\n",
    "        return (\n",
    "            torch.tensor(result, device=device),\n",
    "            torch.tensor(ind, device=device),\n",
    "            torch.tensor(ind_inv, device=device),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x, dim = inputs\n",
    "        _, ind, ind_inv = output\n",
    "        ctx.mark_non_differentiable(ind, ind_inv)\n",
    "        ctx.save_for_backward(ind, ind_inv)\n",
    "        ctx.dim = dim\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output, _0, _1):\n",
    "        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n",
    "\n",
    "    # The signature of the vmap staticmethod is:\n",
    "    # vmap(info, in_dims: Tuple[Optional[int]], *args)\n",
    "    # where *args is the same as the arguments to `forward`.\n",
    "    @staticmethod\n",
    "    def vmap(info, in_dims, x, dim):\n",
    "        # For every input (x and dim), in_dims stores an Optional[int]\n",
    "        # that is:\n",
    "        # - None if the input is not being vmapped over or if the input\n",
    "        #   is not a Tensor\n",
    "        # - an integer if the input is being vmapped over that represents\n",
    "        #   the index of the dimension being vmapped over.\n",
    "        x_bdim, _ = in_dims\n",
    "\n",
    "        # A \"vmap rule\" is the logic of how to perform the operation given\n",
    "        # inputs with one additional dimension. In NumpySort, x has an\n",
    "        # additional dimension (x_bdim). The vmap rule is simply\n",
    "        # to call NumpySort again but pass it a different `dim`.\n",
    "        x = x.movedim(x_bdim, 0)\n",
    "        # Handle negative dims correctly\n",
    "        dim = dim if dim >= 0 else dim + x.dim() - 1\n",
    "        result = NumpySort.apply(x, dim + 1)\n",
    "\n",
    "        # The vmap rule must return a tuple of two things\n",
    "        # 1. the output. Should be the same amount of things\n",
    "        #    as returned by the forward().\n",
    "        # 2. one Optional[int] for each output specifying if each output\n",
    "        # is being vmapped over, and if so, the index of the\n",
    "        # dimension being vmapped over.\n",
    "        #\n",
    "        # NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the\n",
    "        # dimension being vmapped over to the front of `x`, that appears at\n",
    "        # dimension 0 of all outputs.\n",
    "        # The return is (output, out_dims) -- output is a tuple of 3 Tensors\n",
    "        # and out_dims is a Tuple of 3 Optional[int]\n",
    "        return NumpySort.apply(x, dim + 1), (0, 0, 0)\n",
    "\n",
    "class NumpyTake(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x, ind, ind_inv, dim):\n",
    "        device = x.device\n",
    "        x = to_numpy(x)\n",
    "        ind = to_numpy(ind)\n",
    "        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x, ind, ind_inv, dim = inputs\n",
    "        ctx.save_for_backward(ind, ind_inv)\n",
    "        ctx.dim = dim\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ind, ind_inv = ctx.saved_tensors\n",
    "        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n",
    "        return result, None, None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def vmap(info, in_dims, x, ind, ind_inv, dim):\n",
    "        x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims\n",
    "\n",
    "        # The strategy is: expand {x, ind, ind_inv} to all have the dimension\n",
    "        # being vmapped over.\n",
    "        # Then, call back into NumpyTake(expanded_x, expanded_ind, expanded_ind_inv, new_dim).\n",
    "\n",
    "        # Handle negative dims by wrapping them to be positive\n",
    "        logical_dim = x.dim() if x_bdim is None else x_bdim - 1\n",
    "        dim = dim if dim >= 0 else dim + logical_dim\n",
    "\n",
    "        def maybe_expand_bdim_at_front(x, x_bdim):\n",
    "            if x_bdim is None:\n",
    "                return x.expand(info.batch_size, *x.shape)\n",
    "            return x.movedim(x_bdim, 0)\n",
    "\n",
    "        # If the Tensor doesn't have the dimension being vmapped over,\n",
    "        # expand it out. Otherwise, move it to the front of the Tensor\n",
    "        x = maybe_expand_bdim_at_front(x, x_bdim)\n",
    "        ind = maybe_expand_bdim_at_front(ind, ind_bdim)\n",
    "        ind_inv = maybe_expand_bdim_at_front(ind_inv, ind_inv_bdim)\n",
    "\n",
    "        # The return is a tuple (output, out_dims). Since output is a Tensor,\n",
    "        # then out_dims is an Optional[int] (instead of being a Tuple).\n",
    "        return NumpyTake.apply(x, ind, ind_inv, dim + 1), 0\n",
    "\n",
    "def numpy_sort(x, dim=-1):\n",
    "    result, _, _ = NumpySort.apply(x, dim)\n",
    "    return result\n",
    "\n",
    "x = torch.randn(2, 3)\n",
    "result = torch.vmap(numpy_sort)(x)\n",
    "assert torch.allclose(result, numpy_sort(result, 1))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradTrackingTensor(lvl=1, value=\n",
      "    tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      ")\n",
      "y :  GradTrackingTensor(lvl=1, value=\n",
      "    tensor([ 2.,  3.,  6., 11., 18., 27., 38., 51., 66., 83.])\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18.]), tensor(10.))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10).float()\n",
    "u = torch.tensor(2.0)\n",
    "def grad_fn(x, u):\n",
    "    print(x)\n",
    "    y = x**2 + u\n",
    "    print(\"y : \", y)\n",
    "    return y.sum()\n",
    "    \n",
    "    \n",
    "grad_x = torch.func.grad(grad_fn, (0,1))(x, u)\n",
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], requires_grad=True)\n",
      "y :  tensor([ 2.,  3.,  6., 11., 18., 27., 38., 51., 66., 83.],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18.]), tensor(10.))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autograd.functional.jacobian(grad_fn, (x, u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradTrackingTensor(lvl=1, value=\n",
      "    tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      ")\n",
      "y :  GradTrackingTensor(lvl=1, value=\n",
      "    tensor([ 2.,  3.,  6., 11., 18., 27., 38., 51., 66., 83.])\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(10.),)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10).float()\n",
    "u = torch.tensor(2.0)\n",
    "def grad_fn(x, u):\n",
    "    print(x)\n",
    "    y = x**2 + u\n",
    "    print(\"y : \", y)\n",
    "    return y.sum()\n",
    "    \n",
    "    \n",
    "grad_x = torch.func.grad(grad_fn, (1, ))(x, u)\n",
    "grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6802, -2.3130, -1.7118,  1.0709,  1.1529],\n",
       "        [-0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
       "        [36.6034, 41.5923,  3.5197, 13.4378,  9.3961]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.func import grad, vmap\n",
    "batch_size, feature_size = 3, 5\n",
    "def model(weights, feature_vec):\n",
    "    # Very simple linear model with activation\n",
    "    assert feature_vec.dim() == 1\n",
    "    return feature_vec.dot(weights).relu()\n",
    "def compute_loss(weights, example, target):\n",
    "    y = model(weights, example)\n",
    "    return ((y - target) ** 2).mean()  # MSELoss\n",
    "weights = torch.randn(feature_size, requires_grad=True)\n",
    "examples = torch.randn(batch_size, feature_size)\n",
    "targets = torch.randn(batch_size)\n",
    "inputs = (weights, examples, targets)\n",
    "grad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)\n",
    "grad_weight_per_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([ 0.1713,  0.2256, -0.0783, -0.1581], grad_fn=<NegBackward0>),\n",
       "  tensor([-0.0856, -0.1128,  0.0391,  0.0790], grad_fn=<MulBackward0>)),\n",
       " (tensor([0.3074, 0.6341, 0.4901, 0.8964], requires_grad=True),\n",
       "  tensor([0.1173, 0.2036, 0.0245, 0.1000], grad_fn=<PowBackward0>),\n",
       "  tensor(0.1113, grad_fn=<MeanBackward0>)))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.func import grad\n",
    "def my_loss_func(y, y_pred):\n",
    "   loss_per_sample = (0.5 * y_pred - y) ** 2\n",
    "   loss = loss_per_sample.mean()\n",
    "   return loss, (y_pred, loss_per_sample, loss)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "fn = grad(my_loss_func, argnums=(0, 1), has_aux=True)\n",
    "y_true = torch.rand(4)\n",
    "y_preds = torch.rand(4, requires_grad=True)\n",
    "out = fn(y_true, y_preds)\n",
    "# > output is ((grads w.r.t y_true, grads w.r.t y_preds), (y_pred, loss_per_sample, loss))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    with torch.no_grad():\n",
    "        c = x ** 2\n",
    "    return x - c\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.tensor(2.0, requires_grad=True)\n",
    "    print(grad(f)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
