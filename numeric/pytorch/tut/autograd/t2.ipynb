{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5., dtype=torch.float32)\n",
    "w = torch.tensor(2., dtype=torch.float32, requires_grad=True)\n",
    "y = w * x\n",
    "z = y.detach()\n",
    "print(z.requires_grad)\n",
    "u = torch.tensor(2., dtype=torch.float32, requires_grad=True)\n",
    "v = z * u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(10.), tensor(10.), tensor(True))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.grad, z, u.grad == z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinFn(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, w):\n",
    "        # with torch.no_grad():\n",
    "        o = x * w\n",
    "        print(x, w)\n",
    "        ctx.save_for_backward(x, w)\n",
    "        print(o, o.data_ptr(), o.requires_grad)\n",
    "        return o.detach()\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_o):\n",
    "        x, w = ctx.saved_tensors\n",
    "        print(\"backward x: , w: \", x, w)\n",
    "        grad_x, grad_w = None, None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_x = w\n",
    "            grad_x = grad_x * grad_o\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_w = x\n",
    "            grad_w = grad_w * grad_o\n",
    "            \n",
    "        \n",
    "        return grad_x , grad_w \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, w):\n",
    "    return LinFn.apply(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.) tensor(2., requires_grad=True)\n",
      "tensor(10.) 94420417797760 False\n",
      "tensor(10., grad_fn=<LinFnBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5., dtype=torch.float32)\n",
    "w = torch.tensor(2., dtype=torch.float32, requires_grad=True)\n",
    "y = linear(x, w)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94420417797760"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.) tensor(2., requires_grad=True)\n",
      "tensor(10.) 94420417801216 False\n",
      "tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5., dtype=torch.float32)\n",
    "w = torch.tensor(2., dtype=torch.float32, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = linear(x, w)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinFn1(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, w):\n",
    "        # with torch.no_grad():\n",
    "        o = x * w\n",
    "        ctx.save_for_backward(x, w)\n",
    "        return o.detach()\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_o):\n",
    "        x, w = ctx.saved_tensors\n",
    "        print(\"backward x: , w: \", x, w)\n",
    "        grad_x, grad_w = None, None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_x = w\n",
    "            grad_x = grad_x * grad_o\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_w = x\n",
    "            grad_w = grad_w * grad_o\n",
    "            \n",
    "        \n",
    "        return grad_x , grad_w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(x, w):\n",
    "    return LinFn1.apply(x, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5., dtype=torch.float32)\n",
    "w = torch.tensor(2., dtype=torch.float32, requires_grad=True)\n",
    "y = linear1(x, w)\n",
    "z = y.detach()\n",
    "print(z.requires_grad)\n",
    "u = torch.tensor(2., dtype=torch.float32, requires_grad=True)\n",
    "v = linear1(z, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward x: , w:  tensor(10.) tensor(2., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "v.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5., dtype=torch.float32)\n",
    "w = torch.tensor(2., dtype=torch.float32, requires_grad=True)\n",
    "y = linear1(x, w)\n",
    "z = y\n",
    "print(z.requires_grad)\n",
    "u = torch.tensor(2., dtype=torch.float32, requires_grad=True)\n",
    "v = linear1(z, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward x: , w:  tensor(10., grad_fn=<LinFn1Backward>) tensor(2., requires_grad=True)\n",
      "backward x: , w:  tensor(5.) tensor(2., requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "v.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad # v = z * u = y * u = x * w * u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryLinear(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, weights):\n",
    "        q = inputs.view(-1, inputs.size(2),) @ weights\n",
    "        ctx.save_for_backward(inputs, weights)\n",
    "        return q.view(inputs.size(0), inputs.size(1), -1).detach()\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_q):\n",
    "        inputs, weights = ctx.saved_tensors\n",
    "        grad_input, grad_weight = None, None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_q @  weights.t()\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_q.unsqueeze(-1) * inputs.unsqueeze(inputs.dim() - 1)\n",
    "            grad_weight = grad_weight.sum((0, 1)).T\n",
    "                        \n",
    "        return grad_input,  grad_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 64])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "bz, sz, hz = 2, 6, 32\n",
    "fdim = 64\n",
    "x = torch.randn(bz, sz, hz, requires_grad=True)\n",
    "w = nn.Parameter(torch.Tensor(hz, fdim))\n",
    "with torch.no_grad():\n",
    "    w.data.copy_(torch.randn_like(w.data))\n",
    "y = QueryLinear.apply(x, w)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad q shape:  torch.Size([2, 6, 64])\n",
      "1s:  torch.Size([2, 6, 32]) torch.Size([2, 6, 64])\n",
      "1s:  torch.Size([2, 6, 1, 32]) torch.Size([2, 6, 64])\n",
      "grad_weight :  torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "z = y.sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3.3197,   6.0255,  -0.6870,  -8.2403,   3.2846,  11.9100,   9.0050,\n",
       "         -3.3683,  -1.7355,   0.5828,   6.0587,  -5.2056,   4.4373,  -1.7716,\n",
       "          2.5015,  -9.2717,   1.4272,  -2.0466,   6.3974,  -6.8130,   6.5871,\n",
       "         -0.6450, -19.5809,  13.9310,   9.5542,  -0.8859,  -1.9697,  -4.5802,\n",
       "          0.4948,  12.9984,  -4.4692,   1.4250])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3.3197,   6.0255,  -0.6870,  -8.2403,   3.2846,  11.9100,   9.0050,\n",
       "         -3.3683,  -1.7355,   0.5828,   6.0587,  -5.2056,   4.4373,  -1.7716,\n",
       "          2.5015,  -9.2717,   1.4272,  -2.0466,   6.3974,  -6.8130,   6.5871,\n",
       "         -0.6450, -19.5809,  13.9310,   9.5542,  -0.8859,  -1.9697,  -4.5802,\n",
       "          0.4948,  12.9984,  -4.4692,   1.4250])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6579,  0.6579,  0.6579,  ...,  0.6579,  0.6579,  0.6579],\n",
       "        [ 3.7910,  3.7910,  3.7910,  ...,  3.7910,  3.7910,  3.7910],\n",
       "        [ 4.2263,  4.2263,  4.2263,  ...,  4.2263,  4.2263,  4.2263],\n",
       "        ...,\n",
       "        [-1.6648, -1.6648, -1.6648,  ..., -1.6648, -1.6648, -1.6648],\n",
       "        [ 0.0623,  0.0623,  0.0623,  ...,  0.0623,  0.0623,  0.0623],\n",
       "        [ 1.5735,  1.5735,  1.5735,  ...,  1.5735,  1.5735,  1.5735]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(468.4965)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  3.3197,   6.0255,  -0.6870,  -8.2403,   3.2846,  11.9100,   9.0050,\n",
       "          -3.3683,  -1.7355,   0.5828,   6.0587,  -5.2056,   4.4373,  -1.7716,\n",
       "           2.5015,  -9.2717,   1.4272,  -2.0466,   6.3974,  -6.8130,   6.5871,\n",
       "          -0.6450, -19.5809,  13.9310,   9.5542,  -0.8859,  -1.9697,  -4.5802,\n",
       "           0.4948,  12.9984,  -4.4692,   1.4250]),\n",
       " tensor([  3.3197,   6.0255,  -0.6870,  -8.2403,   3.2846,  11.9100,   9.0050,\n",
       "          -3.3683,  -1.7355,   0.5828,   6.0587,  -5.2056,   4.4373,  -1.7716,\n",
       "           2.5015,  -9.2717,   1.4272,  -2.0466,   6.3974,  -6.8130,   6.5871,\n",
       "          -0.6450, -19.5809,  13.9310,   9.5542,  -0.8859,  -1.9697,  -4.5802,\n",
       "           0.4948,  12.9984,  -4.4692,   1.4250]))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "bz, sz, hz = 2, 6, 32\n",
    "fdim = 64\n",
    "x = torch.randn(bz, sz, hz, requires_grad=True)\n",
    "w = nn.Parameter(torch.Tensor(hz, fdim))\n",
    "with torch.no_grad():\n",
    "    w.data.copy_(torch.randn_like(w.data))\n",
    "y = x @ w\n",
    "z = y.sum()\n",
    "z.backward()\n",
    "x.grad[0][0], x.grad[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6579,  0.6579,  0.6579,  ...,  0.6579,  0.6579,  0.6579],\n",
       "        [ 3.7910,  3.7910,  3.7910,  ...,  3.7910,  3.7910,  3.7910],\n",
       "        [ 4.2263,  4.2263,  4.2263,  ...,  4.2263,  4.2263,  4.2263],\n",
       "        ...,\n",
       "        [-1.6648, -1.6648, -1.6648,  ..., -1.6648, -1.6648, -1.6648],\n",
       "        [ 0.0623,  0.0623,  0.0623,  ...,  0.0623,  0.0623,  0.0623],\n",
       "        [ 1.5735,  1.5735,  1.5735,  ...,  1.5735,  1.5735,  1.5735]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(468.4965)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 3, 2]) torch.Size([2, 8, 2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 3, 4])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 8, 3, 2)\n",
    "b =   torch.randn(2, 8, 2, 4) \n",
    "print(a.size(), b.size())\n",
    "torch.matmul(a , b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 64, 32])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(2, 6, 1, 32) * torch.randn(2, 6, 64, 1) ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 64, 32])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(2, 6, 64, 1) @ torch.randn(2, 6, 1, 32) ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 64, 32])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(2, 6, 64, 1) @ torch.randn(2, 6, 1, 32) ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
