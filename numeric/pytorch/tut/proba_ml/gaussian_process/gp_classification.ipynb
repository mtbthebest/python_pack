{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env_dl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pyro\n",
    "from pyro.contrib.gp import Parameterized\n",
    "import pyro.contrib.gp as gp\n",
    "from pyro import distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(\"/mnt/dl/datasets/alphabet_dataset/dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphabetDataset(Dataset):\n",
    "    letters = {chr(ord('A') + i): i for i in range(26)}\n",
    "    \n",
    "    def __init__(self, path, split, n=512, size=64):\n",
    "        self.path = path\n",
    "        self.n = n\n",
    "        self.split = split\n",
    "        self.size = size\n",
    "        self.data = self.load_files()\n",
    "        self.size = size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fname, letter = self.data[index]\n",
    "        label = self.letters[letter]\n",
    "        \n",
    "        img = Image.open(fname).resize((self.size, self.size))\n",
    "        img = np.array(img).astype(np.float32)\n",
    "        img /= 255.0\n",
    "        img = img[:, :, None]\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        return (img, label)        \n",
    "    \n",
    "    def load_files(self):\n",
    "        data = []\n",
    "        for c in self.letters:\n",
    "            path = os.path.join(self.path, self.split, c)\n",
    "            files = os.listdir(path)\n",
    "            files = list( map(lambda x: os.path.join(path, x), sorted(files)))\n",
    "            data.extend(list(zip(files[:self.n], [c] * self.n)))\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n * len(self.letters)\n",
    "    \n",
    "class TrainAlphabetDataset(AlphabetDataset):\n",
    "    \n",
    "    def __init__(self, path, split=\"train\", n=512):\n",
    "        super().__init__(path, split, n)\n",
    "\n",
    "    \n",
    "class TestAlphabetDataset(AlphabetDataset):\n",
    "    \n",
    "    def __init__(self, path, split=\"test\", n=16):\n",
    "        super().__init__(path, split, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "latent_shape = 50\n",
    "num_classes = 26\n",
    "size = 64\n",
    "train_ds = TrainAlphabetDataset(dataset_path)\n",
    "test_ds = TestAlphabetDataset(dataset_path)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, \n",
    "                          num_workers=4, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=16*num_classes,  shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(64*64, 1096)\n",
    "        self.lin2 = nn.Linear(1096, latent_shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"Cnn \", x.size())\n",
    "        bz = x.size(0)\n",
    "        x = x.view(bz, -1)\n",
    "        x = self.lin1(x)\n",
    "        x = F.softplus(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        # print(\"Cnn output \", x.size())\n",
    "        \n",
    "        return x       \n",
    "\n",
    "class Classification(Parameterized):\n",
    "    \n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(latent_shape, num_classes)\n",
    "        \n",
    "    def forward(self, f_loc, f_var, y=None):\n",
    "        # print(f_loc.size(), f_var.size())\n",
    "        f = dist.Normal(f_loc, f_var.sqrt()).to_event(1)()\n",
    "        f = f.permute((1, 0))\n",
    "        # print(f.size())\n",
    "        f = self.lin1(f)\n",
    "        obs = pyro.sample(\"y\", dist.Categorical(logits=f).to_event(1), obs=y)\n",
    "        return obs      \n",
    "\n",
    "def test(model, loader):\n",
    "    with torch.no_grad():\n",
    "        for img, label in loader:\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "            x = model(img)\n",
    "            yhat = model.likelihood(*x)\n",
    "            acc = yhat == label\n",
    "            acc = acc.sum() * 100 / yhat.size(0)\n",
    "    return acc.item()\n",
    "# Classification()(torch.randn((2, 50), requires_grad=True), torch.randn((2, 50),  requires_grad=True).abs()) \n",
    "# Classification()(torch.randn((2, 50)), torch.randn((2, 50)).abs()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 1, 64, 64]), torch.Size([512]), torch.float32, torch.int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, X.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gp.kernels.Warping(gp.kernels.RBF(input_dim=latent_shape, \n",
    "                                        #    lengthscale=torch.ones(latent_shape),\n",
    "                                        #    variance=torch.ones(latent_shape),\n",
    "                                           ),\n",
    "                            iwarping_fn=cnn\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood =  Classification()#gp.likelihoods.MultiClass(num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gp_model = gp.models.VariationalGP(X, y, kernel=kernel, likelihood=likelihood, \n",
    "#                                    latent_shape=torch.Size([latent_shape]),\n",
    "#                                    jitter=1e-3)\n",
    "\n",
    "\n",
    "gp_model = gp.models.VariationalSparseGP(\n",
    "        X=X,\n",
    "        y=None,\n",
    "        kernel=kernel,\n",
    "        Xu=X,\n",
    "        likelihood=likelihood,\n",
    "        latent_shape=torch.Size([latent_shape]),\n",
    "        num_data=batch_size * num_classes,\n",
    "        whiten=True,\n",
    "        jitter=2e-3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xu torch.Size([512, 1, 64, 64])\n",
      "u_loc torch.Size([50, 512])\n",
      "u_scale_tril_unconstrained torch.Size([50, 512, 512])\n",
      "kernel.kern.variance_unconstrained torch.Size([])\n",
      "kernel.kern.lengthscale_unconstrained torch.Size([])\n",
      "kernel.iwarping_fn.lin1.weight torch.Size([1096, 4096])\n",
      "kernel.iwarping_fn.lin1.bias torch.Size([1096])\n",
      "kernel.iwarping_fn.lin2.weight torch.Size([50, 1096])\n",
      "kernel.iwarping_fn.lin2.bias torch.Size([50])\n",
      "likelihood.lin1.weight torch.Size([26, 50])\n",
      "likelihood.lin1.bias torch.Size([26])\n"
     ]
    }
   ],
   "source": [
    "for n, p in gp_model.named_parameters():\n",
    "    print(n, p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(gp_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalSparseGP(\n",
       "  (kernel): Warping(\n",
       "    (kern): RBF()\n",
       "    (iwarping_fn): CNN(\n",
       "      (lin1): Linear(in_features=4096, out_features=1096, bias=True)\n",
       "      (lin2): Linear(in_features=1096, out_features=50, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (likelihood): Classification(\n",
       "    (lin1): Linear(in_features=50, out_features=26, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = pyro.infer.TraceMeanField_ELBO().differentiable_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/15, Step: 0, Loss: 45174.1484375\n",
      "Episode 0/15, Step: 10, Loss: 44027.91122159091\n",
      "Episode 0/15, Step: 20, Loss: 40957.80896577381\n",
      "Episode 0/15, Loss: 38337.902268629805 Acc: 27.163461685180664\n",
      "Episode 1/15, Step: 5, Loss: 34928.965576171875\n",
      "Episode 1/15, Step: 15, Loss: 30227.71507626488\n",
      "Episode 1/15, Step: 25, Loss: 27026.764610877402\n",
      "Episode 1/15, Loss: 27026.764610877402 Acc: 64.42308044433594\n",
      "Episode 2/15, Step: 0, Loss: 26727.841612617925\n",
      "Episode 2/15, Step: 10, Loss: 24390.228097098214\n",
      "Episode 2/15, Step: 20, Loss: 22733.964455800513\n",
      "Episode 2/15, Loss: 22063.681415264422 Acc: 72.59615325927734\n",
      "Episode 3/15, Step: 5, Loss: 21236.463344029016\n",
      "Episode 3/15, Step: 15, Loss: 20050.605821974736\n",
      "Episode 3/15, Step: 25, Loss: 19074.922072190504\n",
      "Episode 3/15, Loss: 19074.922072190504 Acc: 77.40384674072266\n",
      "Episode 4/15, Step: 0, Loss: 18981.044800967262\n",
      "Episode 4/15, Step: 10, Loss: 18090.767238451088\n",
      "Episode 4/15, Step: 20, Loss: 17320.46883203125\n",
      "Episode 4/15, Loss: 16981.076156850962 Acc: 75.96154022216797\n",
      "Episode 5/15, Step: 5, Loss: 16569.451850442325\n",
      "Episode 5/15, Step: 15, Loss: 15955.357023892337\n",
      "Episode 5/15, Step: 25, Loss: 15432.059939653445\n",
      "Episode 5/15, Loss: 15432.059939653445 Acc: 79.3269271850586\n",
      "Episode 6/15, Step: 0, Loss: 15374.746554040605\n",
      "Episode 6/15, Step: 10, Loss: 14871.007563973615\n",
      "Episode 6/15, Step: 20, Loss: 14417.40874382062\n",
      "Episode 6/15, Loss: 14228.16773534083 Acc: 78.36538696289062\n",
      "Episode 7/15, Step: 5, Loss: 13972.058830098902\n",
      "Episode 7/15, Step: 15, Loss: 13598.197677458176\n",
      "Episode 7/15, Step: 25, Loss: 13279.524188701924\n",
      "Episode 7/15, Loss: 13279.524188701924 Acc: 83.65384674072266\n",
      "Episode 8/15, Step: 0, Loss: 13244.167260859002\n",
      "Episode 8/15, Step: 10, Loss: 12923.681821222175\n",
      "Episode 8/15, Step: 20, Loss: 12634.85137699577\n",
      "Episode 8/15, Loss: 12508.578214726896 Acc: 80.52884674072266\n",
      "Episode 9/15, Step: 5, Loss: 12344.280725097657\n",
      "Episode 9/15, Step: 15, Loss: 12091.3097421875\n",
      "Episode 9/15, Step: 25, Loss: 11871.562963867187\n",
      "Episode 9/15, Loss: 11871.562963867187 Acc: 82.21154022216797\n",
      "Episode 10/15, Step: 0, Loss: 11847.080585114343\n",
      "Episode 10/15, Step: 10, Loss: 11620.918437211716\n",
      "Episode 10/15, Step: 20, Loss: 11415.36376953125\n",
      "Episode 10/15, Loss: 11319.505217438811 Acc: 82.21154022216797\n",
      "Episode 11/15, Step: 5, Loss: 11191.385131835938\n",
      "Episode 11/15, Step: 15, Loss: 11009.214872852855\n",
      "Episode 11/15, Step: 25, Loss: 10828.842474521734\n",
      "Episode 11/15, Loss: 10828.842474521734 Acc: 80.28846740722656\n",
      "Episode 12/15, Step: 0, Loss: 10811.302110373403\n",
      "Episode 12/15, Step: 10, Loss: 10637.58291257498\n",
      "Episode 12/15, Step: 20, Loss: 10471.71089937594\n",
      "Episode 12/15, Loss: 10395.098330887111 Acc: 84.375\n",
      "Episode 13/15, Step: 5, Loss: 10293.17750584802\n",
      "Episode 13/15, Step: 15, Loss: 10145.062592414813\n",
      "Episode 13/15, Step: 25, Loss: 10005.768513103108\n",
      "Episode 13/15, Loss: 10005.768513103108 Acc: 83.89423370361328\n",
      "Episode 14/15, Step: 0, Loss: 9993.331960348887\n",
      "Episode 14/15, Step: 10, Loss: 9853.58312109375\n",
      "Episode 14/15, Step: 20, Loss: 9724.526915077111\n",
      "Episode 14/15, Loss: 9663.730650290465 Acc: 79.56731414794922\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "eps = 15\n",
    "val_losses = []\n",
    "for i in range(eps):\n",
    "    for j, (img, label) in enumerate(train_loader):\n",
    "        img = img.cuda()\n",
    "        label = label.cuda()\n",
    "        gp_model.set_data(img, label)      \n",
    "        loss = loss_fn(gp_model.model, gp_model.guide) #/ (batch_size * size * size)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        step = i * eps + j\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Episode {i}/{eps}, Step: {j}, Loss: {np.mean(losses)}\")\n",
    "    acc = test(gp_model, test_loader)\n",
    "    val_losses.append(acc)\n",
    "    print(f\"Episode {i}/{eps}, Loss: {np.mean(losses)} Acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
