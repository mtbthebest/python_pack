{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, autograd, func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLinear(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, bias=True):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"in_feat\", torch.tensor(in_feat).long())\n",
    "        self.register_buffer(\"out_feat\", torch.tensor(out_feat).long())\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.out_feat, self.in_feat))\n",
    "        self.scale = nn.Parameter(torch.ones(self.out_feat))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(self.out_feat))\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn.init.xavier_normal_(self.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x @ self.weight.transpose(0, 1)  + self.bias \n",
    "        print(out)\n",
    "        return out \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"NetLinear(in_feat={self.in_feat}, out_feat={self.out_feat}, bias={self.bias is not None})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.5587,  2.6825],\n",
      "        [ 2.6825,  2.1854],\n",
      "        [ 1.2287,  0.2064],\n",
      "        [-0.5332,  0.1032]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtb/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "def normalize_grad_backward(module, grad_inp, grad_out):\n",
    "    print(\"No grad scaling....\")\n",
    "    print(f\"Grad input {len(grad_inp)}: \", grad_inp, )\n",
    "    print(f\"Grad input : \", grad_inp[0].size(), grad_inp[0].sum(0) )\n",
    "    print()\n",
    "    print(f\"Grad output {len(grad_out)}: \", grad_out, )\n",
    "    return (grad_inp[0] * 1, grad_inp[1])\n",
    "\n",
    "def criterion(inp, target):\n",
    "    # return (inp - target).square().sum()\n",
    "    return (2*inp**2 - target).sum()\n",
    "N = 4\n",
    "din = 3\n",
    "dout = 2\n",
    "torch.manual_seed(0)\n",
    "net_lin = NetLinear(din, dout)\n",
    "net_lin.zero_grad()\n",
    "net_lin.register_backward_hook(normalize_grad_backward)\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(N, din)\n",
    "w_hat = torch.rand(din, dout)\n",
    "y =  x @ w_hat + torch.tensor([1.])\n",
    "\n",
    "yp = net_lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No grad scaling....\n",
      "Grad input 2:  (tensor([[18.2347, 10.7301],\n",
      "        [10.7301,  8.7415],\n",
      "        [ 4.9149,  0.8257],\n",
      "        [-2.1329,  0.4129]]), tensor([31.7467, 20.7100]))\n",
      "Grad input :  torch.Size([4, 2]) tensor([31.7467, 20.7100])\n",
      "\n",
      "Grad output 1:  (tensor([[18.2347, 10.7301],\n",
      "        [10.7301,  8.7415],\n",
      "        [ 4.9149,  0.8257],\n",
      "        [-2.1329,  0.4129]]),)\n",
      "tensor(76.9643, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(yp, y)\n",
    "net_lin.zero_grad()\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 37.0416, -11.5962, -58.6599],\n",
       "         [ 21.6704, -12.1832, -36.1230]]),\n",
       " tensor([31.7467, 20.7100]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_lin.weight.grad, net_lin.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_grad_backward(module, grad_inp, grad_out):\n",
    "    print(\"grad scaling 2x ....\")\n",
    "    print(f\"Grad input {len(grad_inp)}: \", grad_inp, )\n",
    "    print(f\"Grad input : \", grad_inp[0].size(), grad_inp[0].sum(0) )\n",
    "    print()\n",
    "    print(f\"Grad output {len(grad_out)}: \", grad_out, )\n",
    "    return (grad_inp[0] * 2, grad_inp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad scaling 2x ....\n",
      "Grad input 2:  (tensor([[18.2347, 10.7301],\n",
      "        [10.7301,  8.7415],\n",
      "        [ 4.9149,  0.8257],\n",
      "        [-2.1329,  0.4129]]), tensor([119.0857,  48.0999]))\n",
      "Grad input :  torch.Size([4, 2]) tensor([31.7467, 20.7100])\n",
      "\n",
      "Grad output 1:  (tensor([[18.2347, 10.7301],\n",
      "        [10.7301,  8.7415],\n",
      "        [ 4.9149,  0.8257],\n",
      "        [-2.1329,  0.4129]]),)\n",
      "tensor(76.9643, grad_fn=<SumBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.9746, -0.1856, -1.3780],\n",
      "        [ 0.3595, -0.6859, -0.8845]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "net_lin = NetLinear(din, dout)\n",
    "net_lin.zero_grad()\n",
    "net_lin.register_backward_hook(normalize_grad_backward)\n",
    "yp = net_lin(x)\n",
    "loss = criterion(yp, y)\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(net_lin.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  74.0831,  -23.1925, -117.3197],\n",
       "         [  43.3408,  -24.3665,  -72.2460]]),\n",
       " tensor([63.4934, 41.4201]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_lin.weight.grad, net_lin.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.as_tensor(net_lin.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLinear_Noparam(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, bias=True):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(in_feat, out_feat, bias=bias)\n",
    "        with torch.no_grad():\n",
    "            self.net.weight.data.copy_(weight)\n",
    "        if bias:\n",
    "            nn.init.constant_(self.net.bias, 0.)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad scaling 2x ....\n",
      "Grad input 3:  (tensor([31.7467, 20.7100]), None, tensor([[ 37.0416,  21.6704],\n",
      "        [-11.5962, -12.1832],\n",
      "        [-58.6599, -36.1230]]))\n",
      "Grad input :  torch.Size([2]) tensor(52.4567)\n",
      "\n",
      "Grad output 1:  (tensor([[18.2347, 10.7301],\n",
      "        [10.7301,  8.7415],\n",
      "        [ 4.9149,  0.8257],\n",
      "        [-2.1329,  0.4129]]),)\n",
      "tensor(76.9643, grad_fn=<SumBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.9746, -0.1856, -1.3780],\n",
      "        [ 0.3595, -0.6859, -0.8845]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def normalize_grad_backward(module, grad_inp, grad_out):\n",
    "    print(\"grad scaling 2x ....\")\n",
    "    print(f\"Grad input {len(grad_inp)}: \", grad_inp, )\n",
    "    print(f\"Grad input : \", grad_inp[0].size(), grad_inp[0].sum(0) )\n",
    "    print()\n",
    "    print(f\"Grad output {len(grad_out)}: \", grad_out, )\n",
    "    return (grad_inp[0] * 2, grad_inp[1], grad_inp[2] * 2.)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "net_lin = NetLinear_Noparam(din, dout)\n",
    "net_lin.zero_grad()\n",
    "net_lin.register_backward_hook(normalize_grad_backward)\n",
    "yp = net_lin(x)\n",
    "loss = criterion(yp, y)\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(net_lin.net.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  74.0831,  -23.1925, -117.3197],\n",
       "         [  43.3408,  -24.3665,  -72.2460]]),\n",
       " tensor([63.4934, 41.4201]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_lin.net.weight.grad, net_lin.net.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MatMul(autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias) -> torch.Tensor:\n",
    "        out = input @ weights.transpose(0, 1) + bias\n",
    "        ctx.save_for_backward(input, weights, bias, out)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        # print(\"Linear backward \", grad_outputs)\n",
    "        input, weights, bias, out = ctx.saved_tensors\n",
    "        bz, fz = input.size()\n",
    "        out_feat, in_feat = weights.size()\n",
    "        # assert fz == in_feat\n",
    "        # print(grad_outputs[0].size())\n",
    "        grad_w =  input.unsqueeze(1).expand(bz, out_feat, -1) * grad_outputs[0].unsqueeze(-1) \n",
    "        grad_w = grad_w.sum(0)\n",
    "        grad_b =  (bias.new_ones(bias.size()).unsqueeze(0).unsqueeze(0) * grad_outputs[0]).sum(0).sum(0)\n",
    "        # print(grad_b.size(), grad_w.size())\n",
    "        inp_grad = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            print(\"Need inp grad \")\n",
    "            inp_grad =  grad_outputs[0] @ weights.t() \n",
    "        print(\"Grad_w is : \", grad_w )\n",
    "        print(\"Grad_b is : \", grad_b )\n",
    "        print()\n",
    "        return  inp_grad, grad_w, grad_b,\n",
    "\n",
    "class NetLinear(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, bias=True):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"in_feat\", torch.tensor(in_feat).long())\n",
    "        self.register_buffer(\"out_feat\", torch.tensor(out_feat).long())\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.out_feat, self.in_feat))\n",
    "        self.scale = nn.Parameter(torch.ones(self.out_feat))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(self.out_feat))\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn.init.xavier_normal_(self.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = MatMul.apply(x, self.weight, self.bias)\n",
    "        return out \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"NetLinear(in_feat={self.in_feat}, out_feat={self.out_feat}, bias={self.bias is not None})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtb/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "def normalize_grad_backward(module, grad_inp, grad_out):\n",
    "    print(f\"Grad input {len(grad_inp)}: \", grad_inp, )\n",
    "    print()\n",
    "    print(f\"Grad output {len(grad_out)}: \", grad_out, )\n",
    "    return (grad_inp[0], grad_inp[1] * 2, grad_inp[2])\n",
    "\n",
    "def criterion(inp, target):\n",
    "    # return (inp - target).square().sum()\n",
    "    return (2*inp**2 - target).sum()\n",
    "N = 4\n",
    "din = 3\n",
    "dout = 2\n",
    "torch.manual_seed(0)\n",
    "net_lin = NetLinear(din, dout)\n",
    "net_lin.zero_grad()\n",
    "net_lin.register_backward_hook(normalize_grad_backward)\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(N, din)\n",
    "w_hat = torch.rand(din, dout)\n",
    "y =  x @ w_hat + torch.tensor([1.])\n",
    "\n",
    "yp = net_lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad_w is :  tensor([[ 37.0416, -11.5962, -58.6599],\n",
      "        [ 21.6704, -12.1832, -36.1230]])\n",
      "Grad_b is :  tensor([31.7467, 20.7100])\n",
      "\n",
      "Grad input 3:  (None, tensor([[ 37.0416, -11.5962, -58.6599],\n",
      "        [ 21.6704, -12.1832, -36.1230]]), tensor([31.7467, 20.7100]))\n",
      "\n",
      "Grad output 1:  (tensor([[18.2347, 10.7301],\n",
      "        [10.7301,  8.7415],\n",
      "        [ 4.9149,  0.8257],\n",
      "        [-2.1329,  0.4129]]),)\n",
      "tensor(76.9643, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(yp, y)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  74.0831,  -23.1925, -117.3197],\n",
       "         [  43.3408,  -24.3665,  -72.2460]]),\n",
       " tensor([31.7467, 20.7100]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_lin.weight.grad, net_lin.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
