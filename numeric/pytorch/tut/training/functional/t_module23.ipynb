{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, autograd, func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "N = 4\n",
    "din = 3\n",
    "dout = 2\n",
    "x = torch.randn(N, din)\n",
    "w_hat = torch.rand(din, dout)\n",
    "y =  x @ w_hat + torch.tensor([1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MatMul(autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias) -> torch.Tensor:\n",
    "        out = input @ weights.transpose(0, 1) + bias\n",
    "        ctx.save_for_backward(input, weights, bias, out)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        input, weights, bias, out = ctx.saved_tensors\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print(\"received \", grad_outputs)\n",
    "        \n",
    "        bz, fz = input.size()\n",
    "        out_feat, in_feat = weights.size()\n",
    "        grad_w =  input.unsqueeze(1).expand(bz, out_feat, -1) * grad_outputs[0].unsqueeze(-1) \n",
    "        grad_w = grad_w.sum(0)\n",
    "        grad_b =  (bias.new_ones(bias.size()).unsqueeze(0).unsqueeze(0) * grad_outputs[0]).sum(0).sum(0)\n",
    "        inp_grad = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            print(\"Need inp grad \")\n",
    "            inp_grad =  grad_outputs[0] @ weights\n",
    "        print(\"Grad_w is : \", grad_w )\n",
    "        print(\"Grad_b is : \", grad_b )\n",
    "        print()\n",
    "        return  inp_grad, grad_w, grad_b,\n",
    "\n",
    "class NetLinear(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, bias=True):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"in_feat\", torch.tensor(in_feat).long())\n",
    "        self.register_buffer(\"out_feat\", torch.tensor(out_feat).long())\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.out_feat, self.in_feat))\n",
    "        self.scale = nn.Parameter(torch.ones(self.out_feat))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(self.out_feat))\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn.init.xavier_normal_(self.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = MatMul.apply(x, self.weight, self.bias)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward pre hook  (tensor([[ 9.1173,  5.3650],\n",
      "        [ 5.3650,  4.3707],\n",
      "        [ 2.4574,  0.4128],\n",
      "        [-1.0664,  0.2064]]),)\n",
      "Backward hook : (None,)\n",
      "Backward hook : (tensor([[18.2347, 10.7301],\n",
      "        [10.7301,  8.7415],\n",
      "        [ 4.9149,  0.8257],\n",
      "        [-2.1329,  0.4129]]),)\n",
      "\n",
      "\n",
      "\n",
      "received  (tensor([[ 9.1173,  5.3650],\n",
      "        [ 5.3650,  4.3707],\n",
      "        [ 2.4574,  0.4128],\n",
      "        [-1.0664,  0.2064]]),)\n",
      "Grad_w is :  tensor([[ 18.5208,  -5.7981, -29.3299],\n",
      "        [ 10.8352,  -6.0916, -18.0615]])\n",
      "Grad_b is :  tensor([15.8734, 10.3550])\n",
      "\n",
      "tensor(35.1679, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def backward_pre_hook(module,  grad_out):\n",
    "    print(f\"Backward pre hook \", grad_out)\n",
    "    return (2 * grad_out[0], )\n",
    "\n",
    "def backward_hook(module,  grad_inp, grad_out):\n",
    "    print(f\"Backward hook : {grad_inp}\")\n",
    "    print(f\"Backward hook : {grad_out}\")\n",
    "    # return (grad_inp[0], grad_inp[1] * 2, grad_inp[2])\n",
    "\n",
    "def criterion(inp, target):\n",
    "    return (inp**2 - target).sum()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "net_lin = NetLinear(din, dout)\n",
    "net_lin.zero_grad()\n",
    "net_lin.register_full_backward_pre_hook(backward_pre_hook)\n",
    "net_lin.register_full_backward_hook(backward_hook)\n",
    "# net_lin.register_backward_hook(backward_hook)\n",
    "yp = net_lin(x)\n",
    "loss = criterion(yp, y)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAutograd(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, bias=True):\n",
    "        super().__init__()\n",
    "        self.net1 = NetLinear(in_feat, 5, bias=bias)\n",
    "        self.net2 = NetLinear(5, out_feat , bias=bias)\n",
    "        with torch.no_grad():\n",
    "            self.net1.weight.data.copy_(torch.ones_like(self.net1.weight))\n",
    "            self.net2.weight.data.copy_(torch.ones_like(self.net2.weight))\n",
    "        if bias:\n",
    "            nn.init.constant_(self.net1.bias, 0.)\n",
    "            nn.init.constant_(self.net2.bias, 0.)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net2(self.net1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward pre hook  (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "Backward hook grad inp: (None,)\n",
      "Backward hook grad out : (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "\n",
      "Backward pre hook net 2  (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "multiply by 3\n",
      "\n",
      "\n",
      "\n",
      "received  (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "Need inp grad \n",
      "Grad_w is :  tensor([[109.4967, 109.4967, 109.4967, 109.4967, 109.4967],\n",
      "        [109.4967, 109.4967, 109.4967, 109.4967, 109.4967]])\n",
      "Grad_b is :  tensor([-62.8347, -62.8347])\n",
      "\n",
      "++++++++++++++++++++\n",
      "Backward hook 2  grad inp: (tensor([[-37.2489, -37.2489, -37.2489, -37.2489, -37.2489],\n",
      "        [-76.5875, -76.5875, -76.5875, -76.5875, -76.5875],\n",
      "        [ 20.8846,  20.8846,  20.8846,  20.8846,  20.8846],\n",
      "        [-32.7177, -32.7177, -32.7177, -32.7177, -32.7177]]),)\n",
      "Backward hook 2 grad out : (tensor([[ -55.8733,  -55.8733],\n",
      "        [-114.8812, -114.8812],\n",
      "        [  31.3269,   31.3269],\n",
      "        [ -49.0765,  -49.0765]]),)\n",
      "++++++++++++++++++++\n",
      "Backward pre hook net 1 \n",
      "--------------------\n",
      "Backward hook 1  grad inp: (None,)\n",
      "Backward hook 1 grad out : (tensor([[-37.2489, -37.2489, -37.2489, -37.2489, -37.2489],\n",
      "        [-76.5875, -76.5875, -76.5875, -76.5875, -76.5875],\n",
      "        [ 20.8846,  20.8846,  20.8846,  20.8846,  20.8846],\n",
      "        [-32.7177, -32.7177, -32.7177, -32.7177, -32.7177]]),)\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "received  (tensor([[-37.2489, -37.2489, -37.2489, -37.2489, -37.2489],\n",
      "        [-76.5875, -76.5875, -76.5875, -76.5875, -76.5875],\n",
      "        [ 20.8846,  20.8846,  20.8846,  20.8846,  20.8846],\n",
      "        [-32.7177, -32.7177, -32.7177, -32.7177, -32.7177]]),)\n",
      "Grad_w is :  tensor([[-79.3149, 131.0131, 167.2951],\n",
      "        [-79.3149, 131.0131, 167.2951],\n",
      "        [-79.3149, 131.0131, 167.2951],\n",
      "        [-79.3149, 131.0131, 167.2951],\n",
      "        [-79.3149, 131.0131, 167.2951]])\n",
      "Grad_b is :  tensor([-125.6694, -125.6694, -125.6694, -125.6694, -125.6694])\n",
      "\n",
      "tensor(267.1132, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def backward_pre_hook(module,  grad_out):\n",
    "    print(f\"Backward pre hook \", grad_out)\n",
    "    g = grad_out[0]\n",
    "    g *= 2\n",
    "    # return (2 * grad_out[0], )\n",
    "\n",
    "def backward_hook(module,  grad_inp, grad_out):\n",
    "    print(f\"Backward hook grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook grad out : {grad_out}\")\n",
    "    print()\n",
    "    return (None, )\n",
    "\n",
    "def backward_pre_hook1(module,  grad_out):\n",
    "    print(f\"Backward pre hook net 1 \")\n",
    "\n",
    "def backward_pre_hook2(module,  grad_out):\n",
    "    print(f\"Backward pre hook net 2 \", grad_out)\n",
    "    print(\"multiply by 3\")\n",
    "    return (3 * grad_out[0], )\n",
    "\n",
    "def backward_hook2(module,    grad_inp, grad_out):\n",
    "    print(\"+\" * 20)\n",
    "    print(f\"Backward hook 2  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook 2 grad out : {grad_out}\")\n",
    "    print(\"+\" * 20)\n",
    "    \n",
    "def backward_hook1(module, grad_inp, grad_out):\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Backward hook 1  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook 1 grad out : {grad_out}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "def criterion(inp, target):\n",
    "    return (inp**2 - target).sum()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "net_lin = ModelAutograd(din, dout)\n",
    "net_lin.zero_grad()\n",
    "net_lin.register_full_backward_pre_hook(backward_pre_hook)\n",
    "net_lin.net1.register_full_backward_pre_hook(backward_pre_hook1)\n",
    "net_lin.net2.register_full_backward_pre_hook(backward_pre_hook2)\n",
    "net_lin.register_full_backward_hook(backward_hook)\n",
    "net_lin.net2.register_full_backward_hook(backward_hook2)\n",
    "net_lin.net1.register_full_backward_hook(backward_hook1)\n",
    "# net_lin.register_backward_hook(backward_hook)\n",
    "yp = net_lin(x)\n",
    "loss = criterion(yp, y)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward pre hook  (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "Backward hook grad inp: (None,)\n",
      "Backward hook grad out : (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "\n",
      "Backward pre hook net 2  (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "multiply by 3 fixed*************************************\n",
      "\n",
      "\n",
      "\n",
      "received  (tensor([[ -55.8733,  -55.8733],\n",
      "        [-114.8812, -114.8812],\n",
      "        [  31.3269,   31.3269],\n",
      "        [ -49.0765,  -49.0765]]),)\n",
      "Need inp grad \n",
      "Grad_w is :  tensor([[328.4901, 328.4901, 328.4901, 328.4901, 328.4901],\n",
      "        [328.4901, 328.4901, 328.4901, 328.4901, 328.4901]])\n",
      "Grad_b is :  tensor([-188.5041, -188.5041])\n",
      "\n",
      "++++++++++++++++++++\n",
      "Backward hook 2  grad inp: (tensor([[-111.7467, -111.7467, -111.7467, -111.7467, -111.7467],\n",
      "        [-229.7624, -229.7624, -229.7624, -229.7624, -229.7624],\n",
      "        [  62.6539,   62.6539,   62.6539,   62.6539,   62.6539],\n",
      "        [ -98.1531,  -98.1531,  -98.1531,  -98.1531,  -98.1531]]),)\n",
      "Backward hook 2 grad out : (tensor([[-167.6200, -167.6200],\n",
      "        [-344.6436, -344.6436],\n",
      "        [  93.9808,   93.9808],\n",
      "        [-147.2296, -147.2296]]),)\n",
      "++++++++++++++++++++\n",
      "Backward pre hook net 1 \n",
      "--------------------\n",
      "Backward hook 1  grad inp: (None,)\n",
      "Backward hook 1 grad out : (tensor([[-111.7467, -111.7467, -111.7467, -111.7467, -111.7467],\n",
      "        [-229.7624, -229.7624, -229.7624, -229.7624, -229.7624],\n",
      "        [  62.6539,   62.6539,   62.6539,   62.6539,   62.6539],\n",
      "        [ -98.1531,  -98.1531,  -98.1531,  -98.1531,  -98.1531]]),)\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "received  (tensor([[-111.7467, -111.7467, -111.7467, -111.7467, -111.7467],\n",
      "        [-229.7624, -229.7624, -229.7624, -229.7624, -229.7624],\n",
      "        [  62.6539,   62.6539,   62.6539,   62.6539,   62.6539],\n",
      "        [ -98.1531,  -98.1531,  -98.1531,  -98.1531,  -98.1531]]),)\n",
      "Grad_w is :  tensor([[-237.9446,  393.0393,  501.8854],\n",
      "        [-237.9446,  393.0393,  501.8854],\n",
      "        [-237.9446,  393.0393,  501.8854],\n",
      "        [-237.9446,  393.0393,  501.8854],\n",
      "        [-237.9446,  393.0393,  501.8854]])\n",
      "Grad_b is :  tensor([-377.0082, -377.0082, -377.0082, -377.0082, -377.0082])\n",
      "\n",
      "tensor(267.1132, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def backward_pre_hook(module,  grad_out):\n",
    "    print(f\"Backward pre hook \", grad_out)\n",
    "    g = grad_out[0]\n",
    "    g *= 2\n",
    "    # return (2 * grad_out[0], )\n",
    "\n",
    "def backward_hook(module,  grad_inp, grad_out):\n",
    "    print(f\"Backward hook grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook grad out : {grad_out}\")\n",
    "    print()\n",
    "    return (None, )\n",
    "\n",
    "def backward_pre_hook1(module,  grad_out):\n",
    "    print(f\"Backward pre hook net 1 \")\n",
    "\n",
    "def backward_pre_hook2(module,  grad_out):\n",
    "    print(f\"Backward pre hook net 2 \", grad_out)\n",
    "    print(\"multiply by 3 fixed*************************************\")\n",
    "    g = grad_out[0]\n",
    "    g *= 3\n",
    "    return (3 * grad_out[0], )\n",
    "\n",
    "def backward_hook2(module,    grad_inp, grad_out):\n",
    "    print(\"+\" * 20)\n",
    "    print(f\"Backward hook 2  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook 2 grad out : {grad_out}\")\n",
    "    print(\"+\" * 20)\n",
    "    \n",
    "def backward_hook1(module, grad_inp, grad_out):\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Backward hook 1  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook 1 grad out : {grad_out}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "def criterion(inp, target):\n",
    "    return (inp**2 - target).sum()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "net_lin = ModelAutograd(din, dout)\n",
    "net_lin.zero_grad()\n",
    "net_lin.register_full_backward_pre_hook(backward_pre_hook)\n",
    "net_lin.net1.register_full_backward_pre_hook(backward_pre_hook1)\n",
    "net_lin.net2.register_full_backward_pre_hook(backward_pre_hook2)\n",
    "net_lin.register_full_backward_hook(backward_hook)\n",
    "net_lin.net2.register_full_backward_hook(backward_hook2)\n",
    "net_lin.net1.register_full_backward_hook(backward_hook1)\n",
    "# net_lin.register_backward_hook(backward_hook)\n",
    "yp = net_lin(x)\n",
    "loss = criterion(yp, y)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@@\n",
      "Backward pre hook criterion  grad out : (tensor(1.),)\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "Backward hook criterion  grad inp: (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]), tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]))\n",
      "Backward hook criterion grad out : (tensor(1.),)\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "\n",
      "Backward pre hook model (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "\n",
      "Backward hook model grad inp: (None,)\n",
      "Backward hook model grad out : (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "\n",
      "Backward pre hook net 2  (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "Gradient for weight  is:  None\n",
      "multiply by 3 fixed*************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "received  (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "Need inp grad \n",
      "Grad_w is :  tensor([[54.7483, 54.7483, 54.7483, 54.7483, 54.7483],\n",
      "        [54.7483, 54.7483, 54.7483, 54.7483, 54.7483]])\n",
      "Grad_b is :  tensor([-31.4174, -31.4174])\n",
      "\n",
      "++++++++++++++++++++\n",
      "Backward hook 2  grad inp: (tensor([[-18.6244, -18.6244, -18.6244, -18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937, -38.2937, -38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423,  10.4423,  10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588, -16.3588, -16.3588, -16.3588]]),)\n",
      "Backward hook 2 grad out : (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "Gradient for weight  is:  tensor([[54.7483, 54.7483, 54.7483, 54.7483, 54.7483],\n",
      "        [54.7483, 54.7483, 54.7483, 54.7483, 54.7483]])\n",
      "++++++++++++++++++++\n",
      "\n",
      "Backward pre hook net 1  (tensor([[-18.6244, -18.6244, -18.6244, -18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937, -38.2937, -38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423,  10.4423,  10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588, -16.3588, -16.3588, -16.3588]]),)\n",
      "\n",
      "--------------------\n",
      "Backward hook 1  grad inp: (None,)\n",
      "Backward hook 1 grad out : (tensor([[-18.6244, -18.6244, -18.6244, -18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937, -38.2937, -38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423,  10.4423,  10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588, -16.3588, -16.3588, -16.3588]]),)\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "received  (tensor([[-18.6244, -18.6244, -18.6244, -18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937, -38.2937, -38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423,  10.4423,  10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588, -16.3588, -16.3588, -16.3588]]),)\n",
      "Grad_w is :  tensor([[-39.6574,  65.5066,  83.6476],\n",
      "        [-39.6574,  65.5066,  83.6476],\n",
      "        [-39.6574,  65.5066,  83.6476],\n",
      "        [-39.6574,  65.5066,  83.6476],\n",
      "        [-39.6574,  65.5066,  83.6476]])\n",
      "Grad_b is :  tensor([-62.8347, -62.8347, -62.8347, -62.8347, -62.8347])\n",
      "\n",
      "tensor(267.1132, grad_fn=<BackwardHookFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "def backward_pre_hook(module,  grad_out):\n",
    "    print(f\"Backward pre hook model\", grad_out)\n",
    "    print()\n",
    "    g = grad_out[0]\n",
    "\n",
    "def backward_hook(module,  grad_inp, grad_out):\n",
    "    print(f\"Backward hook model grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook model grad out : {grad_out}\")\n",
    "    print()\n",
    "    return (None, )\n",
    "\n",
    "def backward_pre_hook1(module,  grad_out):\n",
    "    print(f\"Backward pre hook net 1 \", grad_out)\n",
    "    print()\n",
    "\n",
    "def backward_pre_hook2(module,  grad_out):\n",
    "    print(f\"Backward pre hook net 2 \", grad_out)\n",
    "    print('Gradient for weight  is: ', module.weight.grad)\n",
    "    print(\"multiply by 3 fixed*************************************\")\n",
    "    print()\n",
    "    g = grad_out[0]\n",
    "\n",
    "def backward_hook2(module,    grad_inp, grad_out):\n",
    "    print(\"+\" * 20)\n",
    "    print(f\"Backward hook 2  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook 2 grad out : {grad_out}\")\n",
    "    print('Gradient for weight  is: ', module.weight.grad)\n",
    "    print(\"+\" * 20)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "def backward_hook1(module, grad_inp, grad_out):\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Backward hook 1  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook 1 grad out : {grad_out}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "def backward_hook_criterion(module, grad_inp, grad_out):\n",
    "    print(\"@\" * 20)\n",
    "    print(f\"Backward hook criterion  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook criterion grad out : {grad_out}\")\n",
    "    print(\"@\" * 20)\n",
    "    print()\n",
    "    \n",
    "def backward_pre_hook_criterion(module, grad_out):\n",
    "    print(\"@\" * 20)\n",
    "    print(f\"Backward pre hook criterion  grad out : {grad_out}\")\n",
    "    print(\"@\" * 20)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "class Loss(nn.Module):\n",
    "    \n",
    "    def forward(self, inp, target):\n",
    "        return (inp**2 - target).sum()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "net_lin = ModelAutograd(din, dout)\n",
    "criterion = Loss()\n",
    "\n",
    "net_lin.zero_grad()\n",
    "criterion.register_full_backward_hook(backward_hook_criterion)\n",
    "criterion.register_full_backward_pre_hook(backward_pre_hook_criterion)\n",
    "\n",
    "net_lin.register_full_backward_pre_hook(backward_pre_hook)\n",
    "net_lin.net1.register_full_backward_pre_hook(backward_pre_hook1)\n",
    "net_lin.net2.register_full_backward_pre_hook(backward_pre_hook2)\n",
    "net_lin.register_full_backward_hook(backward_hook)\n",
    "net_lin.net2.register_full_backward_hook(backward_hook2)\n",
    "net_lin.net1.register_full_backward_hook(backward_hook1)\n",
    "yp = net_lin(x)\n",
    "loss = criterion(yp, y)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.3122,  -9.3122],\n",
       "        [-19.1469, -19.1469],\n",
       "        [  5.2212,   5.2212],\n",
       "        [ -8.1794,  -8.1794]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yp * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[54.7483, 54.7483, 54.7483, 54.7483, 54.7483],\n",
       "        [54.7483, 54.7483, 54.7483, 54.7483, 54.7483]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_lin.net2.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@@\n",
      "Backward pre hook criterion  grad out : (tensor(1.),)\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "Backward hook criterion  grad inp: (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]), tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]))\n",
      "Backward hook criterion grad out : (tensor(2.),)\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "\n",
      "Backward pre hook model (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "\n",
      "Backward hook model grad inp: (None,)\n",
      "Backward hook model grad out : (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "\n",
      "Backward pre hook net 2  (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "Gradient for weight  is:  None\n",
      "multiply by 3 fixed*************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "received  (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "Need inp grad \n",
      "Grad_w is :  tensor([[54.7483, 54.7483, 54.7483, 54.7483, 54.7483],\n",
      "        [54.7483, 54.7483, 54.7483, 54.7483, 54.7483]])\n",
      "Grad_b is :  tensor([-31.4174, -31.4174])\n",
      "\n",
      "++++++++++++++++++++\n",
      "Backward hook 2  grad inp: (tensor([[-18.6244, -18.6244, -18.6244, -18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937, -38.2937, -38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423,  10.4423,  10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588, -16.3588, -16.3588, -16.3588]]),)\n",
      "Backward hook 2 grad out : (tensor([[ -9.3122,  -9.3122],\n",
      "        [-19.1469, -19.1469],\n",
      "        [  5.2212,   5.2212],\n",
      "        [ -8.1794,  -8.1794]]),)\n",
      "Gradient for weight  is:  tensor([[54.7483, 54.7483, 54.7483, 54.7483, 54.7483],\n",
      "        [54.7483, 54.7483, 54.7483, 54.7483, 54.7483]])\n",
      "++++++++++++++++++++\n",
      "\n",
      "Backward pre hook net 1  (tensor([[-18.6244, -18.6244, -18.6244, -18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937, -38.2937, -38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423,  10.4423,  10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588, -16.3588, -16.3588, -16.3588]]),)\n",
      "\n",
      "--------------------\n",
      "Backward hook 1  grad inp: (None,)\n",
      "Backward hook 1 grad out : (tensor([[-18.6244, -18.6244, -18.6244, -18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937, -38.2937, -38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423,  10.4423,  10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588, -16.3588, -16.3588, -16.3588]]),)\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "received  (tensor([[-18.6244, -18.6244, -18.6244, -18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937, -38.2937, -38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423,  10.4423,  10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588, -16.3588, -16.3588, -16.3588]]),)\n",
      "Grad_w is :  tensor([[-39.6574,  65.5066,  83.6476],\n",
      "        [-39.6574,  65.5066,  83.6476],\n",
      "        [-39.6574,  65.5066,  83.6476],\n",
      "        [-39.6574,  65.5066,  83.6476],\n",
      "        [-39.6574,  65.5066,  83.6476]])\n",
      "Grad_b is :  tensor([-62.8347, -62.8347, -62.8347, -62.8347, -62.8347])\n",
      "\n",
      "tensor(267.1132, grad_fn=<BackwardHookFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "def backward_pre_hook(module,  grad_out):\n",
    "    print(f\"Backward pre hook model\", grad_out)\n",
    "    print()\n",
    "    \n",
    "    g = grad_out[0]\n",
    "    # g *= 2\n",
    "    # return (2 * grad_out[0], )\n",
    "\n",
    "def backward_hook(module,  grad_inp, grad_out):\n",
    "    print(f\"Backward hook model grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook model grad out : {grad_out}\")\n",
    "    print()\n",
    "    return (None, )\n",
    "\n",
    "def backward_pre_hook1(module,  grad_out):\n",
    "    print(f\"Backward pre hook net 1 \", grad_out)\n",
    "    print()\n",
    "\n",
    "def backward_pre_hook2(module,  grad_out):\n",
    "    print(f\"Backward pre hook net 2 \", grad_out)\n",
    "    print('Gradient for weight  is: ', module.weight.grad)\n",
    "    print(\"multiply by 3 fixed*************************************\")\n",
    "    print()\n",
    "    \n",
    "    g = grad_out[0]\n",
    "    # g *= 3\n",
    "    # return (3 * grad_out[0], )\n",
    "\n",
    "def backward_hook2(module,    grad_inp, grad_out):\n",
    "    print(\"+\" * 20)\n",
    "    print(f\"Backward hook 2  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook 2 grad out : {grad_out}\")\n",
    "    print('Gradient for weight  is: ', module.weight.grad)\n",
    "    \n",
    "    print(\"+\" * 20)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "def backward_hook1(module, grad_inp, grad_out):\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Backward hook 1  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook 1 grad out : {grad_out}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "def backward_hook_criterion(module, grad_inp, grad_out):\n",
    "    print(\"@\" * 20)\n",
    "    print(f\"Backward hook criterion  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook criterion grad out : {grad_out}\")\n",
    "    print(\"@\" * 20)\n",
    "    print()\n",
    "    return grad_inp\n",
    "    \n",
    "def backward_pre_hook_criterion(module, grad_out):\n",
    "    print(\"@\" * 20)\n",
    "    print(f\"Backward pre hook criterion  grad out : {grad_out}\")\n",
    "    print(\"@\" * 20)\n",
    "    print()\n",
    "    # Does not change gradient\n",
    "    return (2 * grad_out[0], )\n",
    "    \n",
    "    \n",
    "class Loss(nn.Module):\n",
    "    \n",
    "    def forward(self, inp, target):\n",
    "        return (inp**2 - target).sum()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "net_lin = ModelAutograd(din, dout)\n",
    "criterion = Loss()\n",
    "\n",
    "net_lin.zero_grad()\n",
    "criterion.register_full_backward_hook(backward_hook_criterion)\n",
    "criterion.register_full_backward_pre_hook(backward_pre_hook_criterion)\n",
    "\n",
    "net_lin.register_full_backward_pre_hook(backward_pre_hook)\n",
    "net_lin.net1.register_full_backward_pre_hook(backward_pre_hook1)\n",
    "net_lin.net2.register_full_backward_pre_hook(backward_pre_hook2)\n",
    "net_lin.register_full_backward_hook(backward_hook)\n",
    "net_lin.net2.register_full_backward_hook(backward_hook2)\n",
    "net_lin.net1.register_full_backward_hook(backward_hook1)\n",
    "yp = net_lin(x)\n",
    "loss = criterion(yp, y)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@@\n",
      "Backward pre hook criterion  grad out : (tensor(1.),)\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "Backward hook criterion  grad inp: (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]), tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]]))\n",
      "Backward hook criterion grad out : (tensor(2.),)\n",
      "@@@@@@@@@@@@@@@@@@@@\n",
      "\n",
      "Backward pre hook model (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "\n",
      "Backward hook model grad inp: (None,)\n",
      "Backward hook model grad out : (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "\n",
      "Backward pre hook net 2  (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "Gradient for weight  is:  None\n",
      "multiply by 3 fixed*************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "received  (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "Need inp grad \n",
      "Grad_w is :  tensor([[109.4967, 109.4967, 109.4967, 109.4967, 109.4967],\n",
      "        [109.4967, 109.4967, 109.4967, 109.4967, 109.4967]])\n",
      "Grad_b is :  tensor([-62.8347, -62.8347])\n",
      "\n",
      "++++++++++++++++++++\n",
      "Backward hook 2  grad inp: (tensor([[-37.2489, -37.2489, -37.2489, -37.2489, -37.2489],\n",
      "        [-76.5875, -76.5875, -76.5875, -76.5875, -76.5875],\n",
      "        [ 20.8846,  20.8846,  20.8846,  20.8846,  20.8846],\n",
      "        [-32.7177, -32.7177, -32.7177, -32.7177, -32.7177]]),)\n",
      "Backward hook 2 grad out : (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "Gradient for weight  is:  tensor([[109.4967, 109.4967, 109.4967, 109.4967, 109.4967],\n",
      "        [109.4967, 109.4967, 109.4967, 109.4967, 109.4967]])\n",
      "++++++++++++++++++++\n",
      "\n",
      "Backward pre hook net 1  (tensor([[-37.2489, -37.2489, -37.2489, -37.2489, -37.2489],\n",
      "        [-76.5875, -76.5875, -76.5875, -76.5875, -76.5875],\n",
      "        [ 20.8846,  20.8846,  20.8846,  20.8846,  20.8846],\n",
      "        [-32.7177, -32.7177, -32.7177, -32.7177, -32.7177]]),)\n",
      "\n",
      "--------------------\n",
      "Backward hook 1  grad inp: (None,)\n",
      "Backward hook 1 grad out : (tensor([[-37.2489, -37.2489, -37.2489, -37.2489, -37.2489],\n",
      "        [-76.5875, -76.5875, -76.5875, -76.5875, -76.5875],\n",
      "        [ 20.8846,  20.8846,  20.8846,  20.8846,  20.8846],\n",
      "        [-32.7177, -32.7177, -32.7177, -32.7177, -32.7177]]),)\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n",
      "received  (tensor([[-37.2489, -37.2489, -37.2489, -37.2489, -37.2489],\n",
      "        [-76.5875, -76.5875, -76.5875, -76.5875, -76.5875],\n",
      "        [ 20.8846,  20.8846,  20.8846,  20.8846,  20.8846],\n",
      "        [-32.7177, -32.7177, -32.7177, -32.7177, -32.7177]]),)\n",
      "Grad_w is :  tensor([[-79.3149, 131.0131, 167.2951],\n",
      "        [-79.3149, 131.0131, 167.2951],\n",
      "        [-79.3149, 131.0131, 167.2951],\n",
      "        [-79.3149, 131.0131, 167.2951],\n",
      "        [-79.3149, 131.0131, 167.2951]])\n",
      "Grad_b is :  tensor([-125.6694, -125.6694, -125.6694, -125.6694, -125.6694])\n",
      "\n",
      "tensor(267.1132, grad_fn=<BackwardHookFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "def backward_pre_hook(module,  grad_out):\n",
    "    print(f\"Backward pre hook model\", grad_out)\n",
    "    print()\n",
    "    \n",
    "    g = grad_out[0]\n",
    "    # g *= 2\n",
    "    # return (2 * grad_out[0], )\n",
    "\n",
    "def backward_hook(module,  grad_inp, grad_out):\n",
    "    print(f\"Backward hook model grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook model grad out : {grad_out}\")\n",
    "    print()\n",
    "    return (None, )\n",
    "\n",
    "def backward_pre_hook1(module,  grad_out):\n",
    "    print(f\"Backward pre hook net 1 \", grad_out)\n",
    "    print()\n",
    "\n",
    "def backward_pre_hook2(module,  grad_out):\n",
    "    print(f\"Backward pre hook net 2 \", grad_out)\n",
    "    print('Gradient for weight  is: ', module.weight.grad)\n",
    "    print(\"multiply by 3 fixed*************************************\")\n",
    "    print()\n",
    "    \n",
    "    g = grad_out[0]\n",
    "    # g *= 3\n",
    "    # return (3 * grad_out[0], )\n",
    "\n",
    "def backward_hook2(module,    grad_inp, grad_out):\n",
    "    print(\"+\" * 20)\n",
    "    print(f\"Backward hook 2  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook 2 grad out : {grad_out}\")\n",
    "    print('Gradient for weight  is: ', module.weight.grad)\n",
    "    \n",
    "    print(\"+\" * 20)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "def backward_hook1(module, grad_inp, grad_out):\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Backward hook 1  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook 1 grad out : {grad_out}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "def backward_hook_criterion(module, grad_inp, grad_out):\n",
    "    print(\"@\" * 20)\n",
    "    print(f\"Backward hook criterion  grad inp: {grad_inp}\")\n",
    "    print(f\"Backward hook criterion grad out : {grad_out}\")\n",
    "    print(\"@\" * 20)\n",
    "    print()\n",
    "    return grad_inp\n",
    "    \n",
    "def backward_pre_hook_criterion(module, grad_out):\n",
    "    print(\"@\" * 20)\n",
    "    print(f\"Backward pre hook criterion  grad out : {grad_out}\")\n",
    "    print(\"@\" * 20)\n",
    "    print()\n",
    "    #  change gradient by 2\n",
    "    g = grad_out[0]\n",
    "    g *= 2\n",
    "    \n",
    "    \n",
    "class Loss(nn.Module):\n",
    "    \n",
    "    def forward(self, inp, target):\n",
    "        return (inp**2 - target).sum()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "net_lin = ModelAutograd(din, dout)\n",
    "criterion = Loss()\n",
    "\n",
    "net_lin.zero_grad()\n",
    "criterion.register_full_backward_hook(backward_hook_criterion)\n",
    "criterion.register_full_backward_pre_hook(backward_pre_hook_criterion)\n",
    "\n",
    "net_lin.register_full_backward_pre_hook(backward_pre_hook)\n",
    "net_lin.net1.register_full_backward_pre_hook(backward_pre_hook1)\n",
    "net_lin.net2.register_full_backward_pre_hook(backward_pre_hook2)\n",
    "net_lin.register_full_backward_hook(backward_hook)\n",
    "net_lin.net2.register_full_backward_hook(backward_hook2)\n",
    "net_lin.net1.register_full_backward_hook(backward_hook1)\n",
    "yp = net_lin(x)\n",
    "loss = criterion(yp, y)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
