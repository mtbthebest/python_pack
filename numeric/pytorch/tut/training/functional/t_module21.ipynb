{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, autograd, func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N = 4\n",
    "din = 3\n",
    "dout = 2\n",
    "x = torch.randn(N, din)\n",
    "w_hat = torch.rand(din, dout)\n",
    "y =  x @ w_hat + torch.tensor([1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(inp, target):\n",
    "    # return (inp - target).square().sum()\n",
    "    return (2*inp**2 - target).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLinear_Noparam(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, bias=True):\n",
    "        super().__init__()\n",
    "        self.net1 = nn.Linear(in_feat, 5, bias=bias)\n",
    "        self.net2 = nn.Linear(5, out_feat , bias=bias)\n",
    "        with torch.no_grad():\n",
    "            self.net1.weight.data.copy_(torch.ones_like(self.net1.weight))\n",
    "            self.net2.weight.data.copy_(torch.ones_like(self.net2.weight))\n",
    "        if bias:\n",
    "            nn.init.constant_(self.net1.bias, 0.)\n",
    "            nn.init.constant_(self.net2.bias, 0.)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1= self.net1(x)\n",
    "        \n",
    "        out = self.net2(out1)\n",
    "        print(out.size(), out1.size())\n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2]) torch.Size([4, 5])\n",
      "grad scaling 2x .... <class '__main__.NetLinear_Noparam'>\n",
      "Grad input 3:  (tensor([-62.8347, -62.8347]), tensor([[-37.2489, -37.2489, -37.2489, -37.2489, -37.2489],\n",
      "        [-76.5875, -76.5875, -76.5875, -76.5875, -76.5875],\n",
      "        [ 20.8846,  20.8846,  20.8846,  20.8846,  20.8846],\n",
      "        [-32.7177, -32.7177, -32.7177, -32.7177, -32.7177]]), tensor([[109.4967, 109.4967],\n",
      "        [109.4967, 109.4967],\n",
      "        [109.4967, 109.4967],\n",
      "        [109.4967, 109.4967],\n",
      "        [109.4967, 109.4967]]))\n",
      "Grad input :  torch.Size([4, 5])\n",
      "\n",
      "Grad output 1:  (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "tensor(540.8549, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtb/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "def normalize_grad_backward(module, grad_inp, grad_out):\n",
    "    print(\"grad scaling 2x ....\", type(module))\n",
    "    print(f\"Grad input {len(grad_inp)}: \", grad_inp, )\n",
    "    print(f\"Grad input : \", grad_inp[1].size() )\n",
    "    print()\n",
    "    print(f\"Grad output {len(grad_out)}: \", grad_out, )\n",
    "    # return (grad_inp[0] * 2, grad_inp[1], grad_inp[2] * 2.)\n",
    "    # return (grad_inp[0], grad_inp[1] * 2, grad_inp[2] )\n",
    "    return (grad_inp[0], grad_inp[1] * 2, grad_inp[2] )\n",
    "\n",
    "torch.manual_seed(0)\n",
    "net_lin = NetLinear_Noparam(din, dout)\n",
    "net_lin.zero_grad()\n",
    "net_lin.register_backward_hook(normalize_grad_backward)\n",
    "yp = net_lin(x)\n",
    "loss = criterion(yp, y)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-158.6298,  262.0262,  334.5902],\n",
       "         [-158.6298,  262.0262,  334.5902],\n",
       "         [-158.6298,  262.0262,  334.5902],\n",
       "         [-158.6298,  262.0262,  334.5902],\n",
       "         [-158.6298,  262.0262,  334.5902]]),\n",
       " tensor([-251.3388, -251.3388, -251.3388, -251.3388, -251.3388]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_lin.net1.weight.grad, net_lin.net1.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[109.4967, 109.4967, 109.4967, 109.4967, 109.4967],\n",
       "         [109.4967, 109.4967, 109.4967, 109.4967, 109.4967]]),\n",
       " tensor([-62.8347, -62.8347]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_lin.net2.weight.grad, net_lin.net2.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MatMul(autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias) -> torch.Tensor:\n",
    "        out = input @ weights.transpose(0, 1) + bias\n",
    "        ctx.save_for_backward(input, weights, bias, out)\n",
    "        return out\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, *grad_outputs):\n",
    "        print(\"--\"* 100)\n",
    "        print(grad_outputs)\n",
    "        print(\"------\" * 50)\n",
    "        input, weights, bias, out = ctx.saved_tensors\n",
    "        bz, fz = input.size()\n",
    "        out_feat, in_feat = weights.size()\n",
    "        assert fz == in_feat\n",
    "        print(input.size(), grad_outputs[0].size())\n",
    "        grad_w =  input.unsqueeze(1).expand(bz, out_feat, -1) * grad_outputs[0].unsqueeze(-1) \n",
    "        grad_w = grad_w.sum(0)\n",
    "        grad_b =  (bias.new_ones(bias.size()).unsqueeze(0).unsqueeze(0) * grad_outputs[0]).sum(0).sum(0)\n",
    "        inp_grad = None\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            print(\"Need inp grad \", grad_outputs[0].size(), weights.size())\n",
    "            inp_grad =  grad_outputs[0] @ weights\n",
    "            assert inp_grad.size() == input.size()\n",
    "        print(\"Grad_w is : \", grad_w )\n",
    "        print(\"Grad_b is : \", grad_b )\n",
    "        print()\n",
    "        return  inp_grad, grad_w, grad_b,\n",
    "\n",
    "class NetLinear(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, bias=True):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"in_feat\", torch.tensor(in_feat).long())\n",
    "        self.register_buffer(\"out_feat\", torch.tensor(out_feat).long())\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.out_feat, self.in_feat))\n",
    "        self.scale = nn.Parameter(torch.ones(self.out_feat))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(self.out_feat))\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn.init.xavier_normal_(self.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = MatMul.apply(x, self.weight, self.bias)\n",
    "        return out \n",
    "\n",
    "class ModelAutograd(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat, bias=True):\n",
    "        super().__init__()\n",
    "        self.net1 = NetLinear(in_feat, 5, bias=bias)\n",
    "        self.net2 = NetLinear(5, out_feat , bias=bias)\n",
    "        with torch.no_grad():\n",
    "            self.net1.weight.data.copy_(torch.ones_like(self.net1.weight))\n",
    "            self.net2.weight.data.copy_(torch.ones_like(self.net2.weight))\n",
    "        if bias:\n",
    "            nn.init.constant_(self.net1.bias, 0.)\n",
    "            nn.init.constant_(self.net2.bias, 0.)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net2(self.net1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "torch.Size([4, 5]) torch.Size([4, 2])\n",
      "Need inp grad  torch.Size([4, 2]) torch.Size([2, 5])\n",
      "Grad_w is :  tensor([[109.4967, 109.4967, 109.4967, 109.4967, 109.4967],\n",
      "        [109.4967, 109.4967, 109.4967, 109.4967, 109.4967]])\n",
      "Grad_b is :  tensor([-62.8347, -62.8347])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Grad input 3:  (tensor([[-37.2489, -37.2489, -37.2489, -37.2489, -37.2489],\n",
      "        [-76.5875, -76.5875, -76.5875, -76.5875, -76.5875],\n",
      "        [ 20.8846,  20.8846,  20.8846,  20.8846,  20.8846],\n",
      "        [-32.7177, -32.7177, -32.7177, -32.7177, -32.7177]]), tensor([[109.4967, 109.4967, 109.4967, 109.4967, 109.4967],\n",
      "        [109.4967, 109.4967, 109.4967, 109.4967, 109.4967]]), tensor([-62.8347, -62.8347]))\n",
      "\n",
      "Grad output 1:  (tensor([[-18.6244, -18.6244],\n",
      "        [-38.2937, -38.2937],\n",
      "        [ 10.4423,  10.4423],\n",
      "        [-16.3588, -16.3588]]),)\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "(tensor([[ -74.4978,  -74.4978,  -74.4978,  -74.4978,  -74.4978],\n",
      "        [-153.1749, -153.1749, -153.1749, -153.1749, -153.1749],\n",
      "        [  41.7692,   41.7692,   41.7692,   41.7692,   41.7692],\n",
      "        [ -65.4354,  -65.4354,  -65.4354,  -65.4354,  -65.4354]]),)\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "torch.Size([4, 3]) torch.Size([4, 5])\n",
      "Grad_w is :  tensor([[-158.6298,  262.0262,  334.5902],\n",
      "        [-158.6298,  262.0262,  334.5902],\n",
      "        [-158.6298,  262.0262,  334.5902],\n",
      "        [-158.6298,  262.0262,  334.5902],\n",
      "        [-158.6298,  262.0262,  334.5902]])\n",
      "Grad_b is :  tensor([-251.3388, -251.3388, -251.3388, -251.3388, -251.3388])\n",
      "\n",
      "tensor(540.8549, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtb/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "def normalize_grad_backward(module, grad_inp, grad_out):\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(f\"Grad input {len(grad_inp)}: \", grad_inp, )\n",
    "    print()\n",
    "    print(f\"Grad output {len(grad_out)}: \", grad_out, )\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    return (grad_inp[0] *2, grad_inp[1] , grad_inp[2])\n",
    "torch.manual_seed(0)\n",
    "net_lin = ModelAutograd(din, dout)\n",
    "net_lin.zero_grad()\n",
    "net_lin.register_backward_hook(normalize_grad_backward)\n",
    "yp = net_lin(x)\n",
    "loss = criterion(yp, y)\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-158.6298,  262.0262,  334.5902],\n",
       "         [-158.6298,  262.0262,  334.5902],\n",
       "         [-158.6298,  262.0262,  334.5902],\n",
       "         [-158.6298,  262.0262,  334.5902],\n",
       "         [-158.6298,  262.0262,  334.5902]]),\n",
       " tensor([-251.3388, -251.3388, -251.3388, -251.3388, -251.3388]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_lin.net1.weight.grad, net_lin.net1.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[109.4967, 109.4967, 109.4967, 109.4967, 109.4967],\n",
       "         [109.4967, 109.4967, 109.4967, 109.4967, 109.4967]]),\n",
       " tensor([-62.8347, -62.8347]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_lin.net2.weight.grad, net_lin.net2.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
